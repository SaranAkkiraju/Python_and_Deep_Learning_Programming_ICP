For deep versus shallow learning in educational psychology, see Student approaches to learning. For more information, see Artificial neural network.
Machine learning anddata mining
Problems
Classification
Clustering
Regression
Anomaly detection
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction


Supervised learning.mw-parser-output .nobold{font-weight:normal}(classification • regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)


Clustering
BIRCH
CURE
Hierarchical
k-means
Expectation–maximization (EM)
DBSCAN
OPTICS
Mean-shift


Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
t-SNE


Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov


Anomaly detection
k-NN
Local outlier factor


Artificial neural networks
Autoencoder
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net


Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)


Theory
Bias–variance dilemma
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory


Machine-learning venues
NIPS
ICML
ML
JMLR
ArXiv:cs.LG


Glossary of artificial intelligence
Glossary of artificial intelligence


Related articles
List of datasets for machine-learning research
Outline of machine learning


 Machine learning portalvte
Deep learning  (also known as deep structured learning  or hierarchical learning) is part of a broader family of machine learning methods based on artificial neural networks. Learning can be supervised, semi-supervised or unsupervised.[1][2][3]
Deep learning architectures such as deep neural networks, deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases superior to human experts.[4][5][6]
Artificial Neural Networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains.  Specifically, neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.[7][8][9]

.mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}Contents

1 Definition
2 Overview
3 Interpretations
4 History

4.1 Deep learning revolution


5 Neural networks

5.1 Artificial neural networks
5.2 Deep neural networks

5.2.1 Challenges




6 Applications

6.1 Automatic speech recognition
6.2 Image recognition
6.3 Visual art processing
6.4 Natural language processing
6.5 Drug discovery and toxicology
6.6 Customer relationship management
6.7 Recommendation systems
6.8 Bioinformatics
6.9 Medical Image Analysis
6.10 Mobile advertising
6.11 Image restoration
6.12 Financial fraud detection
6.13 Military


7 Relation to human cognitive and brain development
8 Commercial activity
9 Criticism and comment

9.1 Theory
9.2 Errors
9.3 Cyber threat


10 See also
11 References
12 Further reading



Definition[edit]
Deep learning is a class of machine learning algorithms that:[10](pp199–200) use multiple layers to progressively extract higher level features from raw input. For example, in image processing, lower layers may identify edges, while higher layer may identify human-meaningful items such as digits/letters or faces.

Overview[edit]
Most modern deep learning models are based on an artificial neural networks, specifically, Convolutional Neural Networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.[11]
In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place in which level on its own. (Of course, this does not completely obviate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.)[1][12]
The "deep" in "deep learning" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.[2] No universally agreed upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth > 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function.[citation needed] Beyond that more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning features.
Deep learning architectures are often constructed with a greedy layer-by-layer method.[clarification needed][further explanation needed][citation needed] Deep learning helps to disentangle these abstractions and pick out which features improve performance.[1]
For supervised learning tasks, deep learning methods obviate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.
Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than labeled data. Examples of deep structures that can be trained in an unsupervised manner are neural history compressors[13] and deep belief networks.[1][14]

Interpretations[edit]
Deep neural networks are generally interpreted in terms of the universal approximation theorem[15][16][17][18][19][20] or probabilistic inference.[10][11][1][2][14][21][22]
The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.[15][16][17][18][19] In 1989, the first proof was published by George Cybenko for sigmoid activation functions[16] and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.[17]
The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al.[20] proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; If the width is smaller or equal to the input dimension, then deep neural network is not a universal approximator.
The probabilistic interpretation[21] derives from the field of machine learning. It features inference,[10][11][1][2][14][21] as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.[21] The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks.[23] The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.[24]

History[edit]
The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986,[25][13] and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.[26][27]
The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1965.[28] A 1971 paper described a deep network with 8 layers trained by the group method of data handling algorithm.[29]
Other deep learning working architectures, specifically those built for computer vision, began with the Neocognitron introduced by Kunihiko Fukushima in 1980.[30] In 1989, Yann LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970,[31][32][33][34] to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days.[35]
By 1991 such systems were used for recognizing isolated 2-D hand-written digits, while recognizing 3-D objects was done by matching 2-D images with a handcrafted 3-D object model. Weng et al. suggested that a human brain does not use a monolithic 3-D object model and in 1992 they published Cresceptron,[36][37][38] a method for performing 3-D object recognition in cluttered scenes. Because it directly used natural images, Cresceptron started the beginning of general-purpose visual learning for natural 3D worlds. Cresceptron is a cascade of layers similar to Neocognitron. But while Neocognitron required a human programmer to hand-merge features, Cresceptron learned an open number of features in each layer without supervision, where each feature is represented by a convolution kernel. Cresceptron segmented each learned object from a cluttered scene through back-analysis through the network. Max pooling, now often adopted by deep neural networks (e.g. ImageNet tests), was first used in Cresceptron to reduce the position resolution by a factor of (2x2) to 1 through the cascade for better generalization.
In 1994, André de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.[39]
In 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton.[40] Many factors contribute to the slow speed, including the vanishing gradient problem analyzed in 1991 by Sepp Hochreiter.[41][42]
Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of artificial neural network's (ANN) computational cost and a lack of understanding of how the brain wires its biological networks.
Both shallow and deep learning (e.g., recurrent nets) of ANNs have been explored for many years.[43][44][45] These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[46] Key difficulties have been analyzed, including gradient diminishing[41] and weak temporal correlation structure in neural predictive models.[47][48] Additional difficulties were the lack of training data and limited computing power.
Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI studied deep neural networks in speech and speaker recognition. Heck's speaker recognition team achieved the first significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation.[49] While SRI experienced success with deep neural networks in speaker recognition, they were unsuccessful in demonstrating similar success in speech recognition.
The principle of elevating "raw" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the "raw" spectrogram or linear filter-bank features in the late 1990s,[49] showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.[50]
Many aspects of speech recognition were taken over by a deep learning method called long short-term memory (LSTM), a recurrent neural network published by Hochreiter and Schmidhuber in 1997.[51] LSTM RNNs avoid the vanishing gradient problem and can learn "Very Deep Learning" tasks[2] that require memories of events that happened thousands of discrete time steps before, which is important for speech. In 2003, LSTM started to become competitive with traditional speech recognizers on certain tasks.[52] Later it was combined with connectionist temporal classification (CTC)[53] in stacks of LSTM RNNs.[54] In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which they made available through Google Voice Search.[55]
In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh[56]
[57][58] showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation.[59] The papers referred to learning for deep belief nets.
Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved.[60][61][62] Convolutional neural networks (CNNs) were superseded for ASR by CTC[53] for LSTM.[51][55][63][64][65][66][67] but are more successful in computer vision.
The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun.[68] Industrial applications of deep learning to large-scale speech recognition started around 2010.
The 2009 NIPS Workshop on Deep Learning for Speech Recognition[69] was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets (DNN) might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets.[70] However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems.[60][71] The nature of the recognition errors produced by the two types of systems was characteristically different,[72][69] offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems.[10][73][74] Analysis around 2009-2010, contrasted the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition,[72][69] eventually leading to pervasive and dominant use in that industry. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.[60][72][70][75]
In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.[76][77][78][73]
Advances in hardware enabled the renewed interest. In 2009, Nvidia was involved in what was called the “big bang” of deep learning, “as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs).”[79] That year, Google Brain used Nvidia GPUs to create capable DNNs. While there, Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times.[80] In particular, GPUs are well-suited for the matrix/vector math involved in machine learning.[81][82] GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days.[83][84] Specialized hardware and algorithm optimizations can be used for efficient processing.[85]

Deep learning revolution[edit]
 How deep learning is a subset of machine learning and how machine learning is a subset of artificial intelligence (AI).
In 2012, a team led by Dahl won the "Merck Molecular Activity Challenge" using multi-task deep neural networks to predict the biomolecular target of one drug.[86][87] In 2014, Hochreiter's group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients, household products and drugs and won the "Tox21 Data Challenge" of NIH, FDA and NCATS.[88][89][90]
Significant additional impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades, and GPU implementations of NNs for years, including CNNs, fast implementations of CNNs with max-pooling on GPUs in the style of Ciresan and colleagues were needed to progress on computer vision.[81][82][35][91][2] In 2011, this approach achieved for the first time superhuman performance in a visual pattern recognition contest. Also in 2011, it won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest.[92] Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR[4] showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records. In October 2012, a similar system by Krizhevsky et al.[5] won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. In November 2012, Ciresan et al.'s system also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic.[93] In 2013 and 2014, the error rate on the ImageNet task using deep learning was further reduced, following a similar trend in large-scale speech recognition. The Wolfram Image Identification project publicized these improvements.[94]
Image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.[95][96][97][98]
Some researchers assess that the October 2012 ImageNet victory anchored the start of a "deep learning revolution" that has transformed the AI industry.[99]
In March 2019, Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.

Neural networks[edit]
Artificial neural networks[edit]
Main article: Artificial neural network
Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as "cat" or "no cat" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.
An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.
Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.
The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.
Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, playing "Go"[100] ).

Deep neural networks[edit]
This section may be too technical for most readers to understand. Please help improve it to make it understandable to non-experts, without removing the technical details.  (July 2016) (Learn how and when to remove this template message)
A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers.[11][2] The DNN finds the correct mathematical manipulation to turn the input into the output, whether it be a linear relationship or a non-linear relationship. The network moves through the layers calculating the probability of each output. For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name "deep" networks.
DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives.[101] The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.[11]
Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.
DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or "weights", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network didn’t accurately recognize a particular pattern, an algorithm would adjust the weights.[102] That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.
Recurrent neural networks (RNNs), in which data can flow in any direction, are used for applications such as language modeling.[103][104][105][106][107] Long short-term memory is particularly effective for this use.[51][108]
Convolutional deep neural networks (CNNs) are used in computer vision.[109] CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).[67]

Challenges[edit]
As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.
DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning[29] or weight decay (




ℓ

2




{\displaystyle \ell _{2}}

-regularization) or sparsity (




ℓ

1




{\displaystyle \ell _{1}}

-regularization) can be applied during training to combat overfitting.[110] Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies.[111] Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.[112]
DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples)[113] speed up computation. Large processing capabilities of many-core architectures (such as, GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.[114][115]
Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights for CMAC. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.[116][117]

Applications[edit]
Automatic speech recognition[edit]
Main article: Speech recognition
Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn "Very Deep Learning" tasks[2] that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates[108] is competitive with traditional speech recognizers on certain tasks.[52]
The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences.[118] Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.



Method
Percent phoneerror rate (PER) (%)


Randomly Initialized RNN[119]
26.1


Bayesian Triphone GMM-HMM
25.6


Hidden Trajectory (Generative) Model
24.8


Monophone Randomly Initialized DNN
23.4


Monophone DBN-DNN
22.4


Triphone GMM-HMM with BMMI Training
21.7


Monophone DBN-DNN on fbank
20.7


Convolutional DNN[120]
20.0


Convolutional DNN w. Heterogeneous Pooling
18.7


Ensemble DNN/CNN/RNN[121]
18.3


Bidirectional LSTM
17.9


Hierarchical Convolutional Deep Maxout Network[122]
16.5

The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003-2007, accelerated progress in eight major areas:[10][75][73]

Scale-up/out and acclerated DNN training and decoding
Sequence discriminative training
Feature processing by deep models with solid understanding of the underlying mechanisms
Adaptation of DNNs and related deep models
Multi-task and transfer learning by DNNs and related deep models
CNNs and how to design them to best exploit domain knowledge of speech
RNN and its rich LSTM variants
Other types of deep models including tensor-based models and integrated deep generative/discriminative models.
All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.[10][123][124][125]

Image recognition[edit]
Main article: Computer vision
A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.[126]
Deep learning-based image recognition has become "superhuman", producing more accurate results than human contestants. This first occurred in 2011.[127]
Deep learning-trained vehicles now interpret 360° camera views.[128] Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.

Visual art processing[edit]
Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of a) identifying the style period of a given painting, b) Neural Style Transfer - capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video, and c) generating striking imagery based on random visual input fields.[129][130]

Natural language processing[edit]
Main article: Natural language processing
Neural networks have been used for implementing language models since the early 2000s.[103][131] LSTM helped to improve machine translation and language modeling.[104][105][106]
Other key techniques in this field are negative sampling[132] and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN.[133] Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing.[133] Deep neural architectures provide the best results for constituency parsing,[134] sentiment analysis,[135] information retrieval,[136][137] spoken language understanding,[138] machine translation,[104][139] contextual entity linking,[139] writing style recognition,[140] Text classification and others.[141]
Recent developments generalize word embedding to sentence embedding.
Google Translate (GT) uses a large end-to-end long short-term memory network.[142][143][144][145][146][147] Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system "learns from millions of examples."[143] It translates "whole sentences at a time, rather than pieces. Google Translate supports over one hundred languages.[143] The network encodes the "semantics of the sentence rather than simply memorizing phrase-to-phrase translations".[143][148] GT uses English as an intermediate between most language pairs.[148]

Drug discovery and toxicology[edit]
For more information, see Drug discovery and Toxicology.
A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects.[149][150] Research has explored use of deep learning to predict the biomolecular targets,[86][87] off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.[88][89][90]
AtomNet is a deep learning system for structure-based rational drug design.[151] AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus[152] and multiple sclerosis.[153][154]

Customer relationship management[edit]
Main article: Customer relationship management
Deep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.[155]

Recommendation systems[edit]
Main article: Recommender system
Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music recommendations.[156] Multiview deep learning has been applied for learning user preferences from multiple domains.[157] The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.

Bioinformatics[edit]
Main article: Bioinformatics
An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.[158]
In medical informatics, deep learning was used to predict sleep quality based on data from wearables[159] and predictions of health complications from electronic health record data.[160] Deep learning has also showed efficacy in healthcare.[161]

Medical Image Analysis[edit]
Deep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement[162][163]

Mobile advertising[edit]
Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and assimilated before a target segment can be created and used in ad serving by any ad server.[164] Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.

Image restoration[edit]
Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization. These applications include learning methods such as "Shrinkage Fields for Effective Image Restoration"[165] which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.

Financial fraud detection[edit]
Deep learning is being successfully applied to financial fraud detection and anti-money laundering. "Deep anti-money laundering detection system can spot and recognize relationships and similarities between data and, further down the road, learn to detect anomalies or classify and predict specific events". The solution leverages both supervised learning techniques, such as the classification of suspicious transactions, and unsupervised learning, e.g. anomaly detection.
[166]

Military[edit]
The United States Department of Defense applied deep learning to train robots in new tasks through observation.[167]

Relation to human cognitive and brain development[edit]
Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s.[168][169][170][171] These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, "...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature."[172]
A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism.[173][174] Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality.[175][176] In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.[177]
Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons[178][179] and neural populations.[180] Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system[181] both at the single-unit[182] and at the population[183] levels.

Commercial activity[edit]
Many organizations employ deep learning for particular applications. Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.[184]
Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player.[185][186][187] Google Translate uses an LSTM to translate between more than 100 languages.
In 2015, Blippar demonstrated a mobile augmented reality application that uses deep learning to recognize objects in real time.[188]
As of 2008,[189] researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor.[167]
First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot the ability to learn new tasks through observation.[167]
Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as “good job” and “bad job.”[190]

Criticism and comment[edit]
Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science.

Theory[edit]
See also: Explainable AI
A main criticism concerns the lack of theory surrounding some methods.[191] Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear.[citation needed] (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.[192]

Others point out that deep learning should be looked at as a step towards realizing strong AI, not as an all-encompassing solution. Despite the power of deep learning methods, they still lack much of the functionality needed for realizing this goal entirely. Research psychologist Gary Marcus noted:"Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning."[193]As an alternative to this emphasis on the limits of deep learning, one author speculated that it might be possible to train a machine vision stack to perform the sophisticated task of discriminating between "old master" and amateur figure drawings, and hypothesized that such a sensitivity might represent the rudiments of a non-trivial machine empathy.[194] This same author proposed that this would be in line with anthropology, which identifies a concern with aesthetics as a key element of behavioral modernity.[195]
In further reference to the idea that artistic sensitivity might inhere within relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained[196] demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's[197] web site.

Errors[edit]
Some deep learning architectures display problematic behaviors,[198] such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images[199] and misclassifying minuscule perturbations of correctly classified images.[200] Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures.[198] These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar[201] decompositions of observed entities and events.[198] Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition[202] and artificial intelligence (AI).[203]

Cyber threat[edit]
As deep learning moves from the lab into the world, research and experience shows that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such a manipulation is termed an “adversarial attack.” In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system.[204] One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.[205]
Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.[204]
ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.[204]
Another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address that would download malware.[204]
In “data poisoning”, false data is continually smuggled into a machine learning system’s training set to prevent it from achieving mastery.[204]

See also[edit]
Applications of artificial intelligence
Comparison of deep learning software
Compressed sensing
Echo state network
List of artificial intelligence projects
Liquid state machine
List of datasets for machine learning research
Reservoir computing
Sparse coding
References[edit]


^ a b c d e f Bengio, Y.; Courville, A.; Vincent, P. (2013). "Representation Learning: A Review and New Perspectives". IEEE Transactions on Pattern Analysis and Machine Intelligence. 35 (8): 1798–1828. arXiv:1206.5538. doi:10.1109/tpami.2013.50. PMID 23787338..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ a b c d e f g h Schmidhuber, J. (2015). "Deep Learning in Neural Networks: An Overview". Neural Networks. 61: 85–117. arXiv:1404.7828. doi:10.1016/j.neunet.2014.09.003. PMID 25462637.

^ Bengio, Yoshua; LeCun, Yann; Hinton, Geoffrey (2015). "Deep Learning". Nature. 521 (7553): 436–444. Bibcode:2015Natur.521..436L. doi:10.1038/nature14539. PMID 26017442.

^ a b Ciresan, Dan; Meier, U.; Schmidhuber, J. (June 2012). "Multi-column deep neural networks for image classification". 2012 IEEE Conference on Computer Vision and Pattern Recognition: 3642–3649. arXiv:1202.2745. doi:10.1109/cvpr.2012.6248110. ISBN 978-1-4673-1228-8.

^ a b Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffry (2012). "ImageNet Classification with Deep Convolutional Neural Networks" (PDF). NIPS 2012: Neural Information Processing Systems, Lake Tahoe, Nevada.

^ "Google's AlphaGo AI wins three-match series against the world's best Go player". TechCrunch. 25 May 2017.

^ Marblestone, Adam H.; Wayne, Greg; Kording, Konrad P. (2016). "Toward an Integration of Deep Learning and Neuroscience". Frontiers in Computational Neuroscience. 10: 94. doi:10.3389/fncom.2016.00094. PMC 5021692. PMID 27683554.

^ Olshausen, B. A. (1996). "Emergence of simple-cell receptive field properties by learning a sparse code for natural images". Nature. 381 (6583): 607–609. Bibcode:1996Natur.381..607O. doi:10.1038/381607a0. PMID 8637596.

^ Bengio, Yoshua; Lee, Dong-Hyun; Bornschein, Jorg; Mesnard, Thomas; Lin, Zhouhan (2015-02-13). "Towards Biologically Plausible Deep Learning". arXiv:1502.04156 [cs.LG].

^ a b c d e f Deng, L.; Yu, D. (2014). "Deep Learning: Methods and Applications" (PDF). Foundations and Trends in Signal Processing. 7 (3–4): 1–199. doi:10.1561/2000000039.

^ a b c d e Bengio, Yoshua (2009). "Learning Deep Architectures for AI" (PDF). Foundations and Trends in Machine Learning. 2 (1): 1–127. CiteSeerX 10.1.1.701.9550. doi:10.1561/2200000006.

^ LeCun, Yann; Bengio, Yoshua; Hinton, Geoffrey (28 May 2015). "Deep learning". Nature. 521 (7553): 436–444. Bibcode:2015Natur.521..436L. doi:10.1038/nature14539. PMID 26017442.

^ a b Jürgen Schmidhuber (2015). Deep Learning. Scholarpedia, 10(11):32832. Online

^ a b c Hinton, G.E. (2009). "Deep belief networks". Scholarpedia. 4 (5): 5947. Bibcode:2009SchpJ...4.5947H. doi:10.4249/scholarpedia.5947.

^ a b Balázs Csanád Csáji (2001). Approximation with Artificial Neural Networks; Faculty of Sciences; Eötvös Loránd University, Hungary

^ a b c Cybenko (1989). "Approximations by superpositions of sigmoidal functions" (PDF). Mathematics of Control, Signals, and Systems. 2 (4): 303–314. doi:10.1007/bf02551274. Archived from the original (PDF) on 2015-10-10.

^ a b c Hornik, Kurt (1991). "Approximation Capabilities of Multilayer Feedforward Networks". Neural Networks. 4 (2): 251–257. doi:10.1016/0893-6080(91)90009-t.

^ a b Haykin, Simon S. (1999). Neural Networks: A Comprehensive Foundation. Prentice Hall. ISBN 978-0-13-273350-2.

^ a b Hassoun, Mohamad H. (1995). Fundamentals of Artificial Neural Networks. MIT Press. p. 48. ISBN 978-0-262-08239-6.

^ a b Lu, Z., Pu, H., Wang, F., Hu, Z., & Wang, L. (2017). The Expressive Power of Neural Networks: A View from the Width. Neural Information Processing Systems, 6231-6239.

^ a b c d Murphy, Kevin P. (24 August 2012). Machine Learning: A Probabilistic Perspective. MIT Press. ISBN 978-0-262-01802-9.

^ Patel, Ankit; Nguyen, Tan; Baraniuk, Richard (2016). "A Probabilistic Framework for Deep Learning" (PDF). Advances in Neural Information Processing Systems.

^ Hinton, G. E.; Srivastava, N.; Krizhevsky, A.; Sutskever, I.; Salakhutdinov, R.R. (2012). "Improving neural networks by preventing co-adaptation of feature detectors". arXiv:1207.0580 [math.LG].

^ Bishop, Christopher M. (2006). Pattern Recognition and Machine Learning (PDF). Springer. ISBN 978-0-387-31073-2.

^ Rina Dechter (1986). Learning while searching in constraint-satisfaction problems. University of California, Computer Science Department, Cognitive Systems Laboratory.Online

^ Igor Aizenberg, Naum N. Aizenberg, Joos P.L. Vandewalle (2000). Multi-Valued and Universal Binary Neurons: Theory, Learning and Applications. Springer Science & Business Media.

^ Co-evolving recurrent neurons learn deep memory POMDPs. Proc. GECCO, Washington, D. C., pp. 1795-1802, ACM Press, New York, NY, USA, 2005.

^ Ivakhnenko, A. G. (1973). Cybernetic Predicting Devices. CCM Information Corporation.

^ a b Ivakhnenko, Alexey (1971). "Polynomial theory of complex systems". IEEE Transactions on Systems, Man and Cybernetics. 1 (4): 364–378. doi:10.1109/TSMC.1971.4308320.

^ Fukushima, K. (1980). "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position". Biol. Cybern. 36 (4): 193–202. doi:10.1007/bf00344251. PMID 7370364.

^ Seppo Linnainmaa (1970). The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors. Master's Thesis (in Finnish), Univ. Helsinki, 6-7.

^ Griewank, Andreas (2012). "Who Invented the Reverse Mode of Differentiation?" (PDF). Documenta Matematica (Extra Volume ISMP): 389–400.

^ Werbos, P. (1974). "Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences". Harvard University. Retrieved 12 June 2017.

^ Werbos, Paul (1982). "Applications of advances in nonlinear sensitivity analysis" (PDF). System modeling and optimization. Springer. pp. 762–770.

^ a b LeCun et al., "Backpropagation Applied to Handwritten Zip Code Recognition," Neural Computation, 1, pp. 541–551, 1989.

^ J. Weng, N. Ahuja and T. S. Huang, "Cresceptron: a self-organizing neural network which grows adaptively," Proc. International Joint Conference on Neural Networks, Baltimore, Maryland, vol I, pp. 576-581, June, 1992.

^ J. Weng, N. Ahuja and T. S. Huang, "Learning recognition and segmentation of 3-D objects from 2-D images," Proc. 4th International Conf. Computer Vision, Berlin, Germany, pp. 121-128, May, 1993.

^ J. Weng, N. Ahuja and T. S. Huang, "Learning recognition and segmentation using the Cresceptron," International Journal of Computer Vision, vol. 25, no. 2, pp. 105-139, Nov. 1997.

^ de Carvalho, Andre C. L. F.; Fairhurst, Mike C.; Bisset, David (1994-08-08). "An integrated Boolean neural network for pattern classification". Pattern Recognition Letters. 15 (8): 807–813. doi:10.1016/0167-8655(94)90009-4.

^ Hinton, Geoffrey E.; Dayan, Peter; Frey, Brendan J.; Neal, Radford (1995-05-26). "The wake-sleep algorithm for unsupervised neural networks". Science. 268 (5214): 1158–1161. Bibcode:1995Sci...268.1158H. doi:10.1126/science.7761831.

^ a b S. Hochreiter., "Untersuchungen zu dynamischen neuronalen Netzen," Diploma thesis. Institut f. Informatik, Technische Univ. Munich. Advisor: J. Schmidhuber, 1991.

^ Hochreiter, S.;  et al. (15 January 2001). "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies".  In Kolen, John F.; Kremer, Stefan C. (eds.). A Field Guide to Dynamical Recurrent Networks. John Wiley & Sons. ISBN 978-0-7803-5369-5.

^ Morgan, Nelson; Bourlard, Hervé; Renals, Steve; Cohen, Michael; Franco, Horacio (1993-08-01). "Hybrid neural network/hidden markov model systems for continuous speech recognition". International Journal of Pattern Recognition and Artificial Intelligence. 07 (4): 899–916. doi:10.1142/s0218001493000455. ISSN 0218-0014.

^ Robinson, T. (1992). "A real-time recurrent error propagation network word recognition system". ICASSP: 617–620.

^ Waibel, A.; Hanazawa, T.; Hinton, G.; Shikano, K.; Lang, K. J. (March 1989). "Phoneme recognition using time-delay neural networks". IEEE Transactions on Acoustics, Speech, and Signal Processing. 37 (3): 328–339. doi:10.1109/29.21701. ISSN 0096-3518.

^ Baker, J.; Deng, Li; Glass, Jim; Khudanpur, S.; Lee, C.-H.; Morgan, N.; O'Shaughnessy, D. (2009). "Research Developments and Directions in Speech Recognition and Understanding, Part 1". IEEE Signal Processing Magazine. 26 (3): 75–80. Bibcode:2009ISPM...26...75B. doi:10.1109/msp.2009.932166.

^ Bengio, Y. (1991). "Artificial Neural Networks and their Application to Speech/Sequence Recognition". McGill University Ph.D. thesis.

^ Deng, L.; Hassanein, K.; Elmasry, M. (1994). "Analysis of correlation structure for a neural predictive model with applications to speech recognition". Neural Networks. 7 (2): 331–339. doi:10.1016/0893-6080(94)90027-2.

^ a b Heck, L.; Konig, Y.; Sonmez, M.; Weintraub, M. (2000). "Robustness to Telephone Handset Distortion in Speaker Recognition by Discriminative Feature Design". Speech Communication. 31 (2): 181–192. doi:10.1016/s0167-6393(99)00077-1.

^ "Acoustic Modeling with Deep Neural Networks Using Raw Time Signal for LVCSR (PDF Download Available)". ResearchGate. Retrieved 2017-06-14.

^ a b c Hochreiter, Sepp; Schmidhuber, Jürgen (1997-11-01). "Long Short-Term Memory". Neural Computation. 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735. ISSN 0899-7667. PMID 9377276.

^ a b Graves, Alex; Eck, Douglas; Beringer, Nicole; Schmidhuber, Jürgen (2003). "Biologically Plausible Speech Recognition with LSTM Neural Nets" (PDF). 1st Intl. Workshop on Biologically Inspired Approaches to Advanced Information Technology, Bio-ADIT 2004, Lausanne, Switzerland. pp. 175–184.

^ a b Graves, Alex; Fernández, Santiago; Gomez, Faustino (2006). "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks". Proceedings of the International Conference on Machine Learning, ICML 2006: 369–376. CiteSeerX 10.1.1.75.6306.

^ Santiago Fernandez, Alex Graves, and Jürgen Schmidhuber (2007). An application of recurrent neural networks to discriminative keyword spotting. Proceedings of ICANN (2), pp. 220–229.

^ a b Sak, Haşim; Senior, Andrew; Rao, Kanishka; Beaufays, Françoise; Schalkwyk, Johan (September 2015). "Google voice search: faster and more accurate".

^ Hinton, Geoffrey E. (2007-10-01). "Learning multiple layers of representation". Trends in Cognitive Sciences. 11 (10): 428–434. doi:10.1016/j.tics.2007.09.004. ISSN 1364-6613. PMID 17921042.

^ Hinton, G. E.; Osindero, S.; Teh, Y. W. (2006). "A Fast Learning Algorithm for Deep Belief Nets" (PDF). Neural Computation. 18 (7): 1527–1554. doi:10.1162/neco.2006.18.7.1527. PMID 16764513.

^ Bengio, Yoshua (2012). "Practical recommendations for gradient-based training of deep architectures". arXiv:1206.5533 [cs.LG].

^ G. E. Hinton., "Learning multiple layers of representation," Trends in Cognitive Sciences, 11, pp. 428–434, 2007.

^ a b c Hinton, G.; Deng, L.; Yu, D.; Dahl, G.; Mohamed, A.; Jaitly, N.; Senior, A.; Vanhoucke, V.; Nguyen, P.; Sainath, T.; Kingsbury, B. (2012). "Deep Neural Networks for Acoustic Modeling in Speech Recognition --- The shared views of four research groups". IEEE Signal Processing Magazine. 29 (6): 82–97. doi:10.1109/msp.2012.2205597.

^ Deng, Li; Hinton, Geoffrey; Kingsbury, Brian (1 May 2013). "New types of deep neural network learning for speech recognition and related applications: An overview"  – via research.microsoft.com.

^ Deng, L.; Li, J.; Huang, J. T.; Yao, K.; Yu, D.; Seide, F.; Seltzer, M.; Zweig, G.; He, X. (May 2013). "Recent advances in deep learning for speech research at Microsoft". 2013 IEEE International Conference on Acoustics, Speech and Signal Processing: 8604–8608. doi:10.1109/icassp.2013.6639345. ISBN 978-1-4799-0356-6.

^ Sak, Hasim; Senior, Andrew; Beaufays, Francoise (2014). "Long Short-Term Memory recurrent neural network architectures for large scale acoustic modeling" (PDF).

^ Li, Xiangang; Wu, Xihong (2014). "Constructing Long Short-Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition". arXiv:1410.4281 [cs.CL].

^ Zen, Heiga; Sak, Hasim (2015). "Unidirectional Long Short-Term Memory Recurrent Neural Network with Recurrent Output Layer for Low-Latency Speech Synthesis" (PDF). Google.com. ICASSP. pp. 4470–4474.

^ Deng, L.; Abdel-Hamid, O.; Yu, D. (2013). "A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion" (PDF). Google.com. ICASSP.

^ a b Sainath, T. N.; Mohamed, A. r; Kingsbury, B.; Ramabhadran, B. (May 2013). "Deep convolutional neural networks for LVCSR". 2013 IEEE International Conference on Acoustics, Speech and Signal Processing: 8614–8618. doi:10.1109/icassp.2013.6639347. ISBN 978-1-4799-0356-6.

^ Yann LeCun (2016). Slides on Deep Learning Online

^ a b c NIPS Workshop: Deep Learning for Speech Recognition and Related Applications, Whistler, BC, Canada, Dec. 2009 (Organizers: Li Deng, Geoff Hinton, D. Yu).

^ a b Keynote talk: Recent Developments in Deep Neural Networks. ICASSP, 2013 (by Geoff Hinton).

^ D. Yu, L. Deng, G. Li, and F. Seide (2011). "Discriminative pretraining of deep neural networks," U.S. Patent Filing.

^ a b c Deng, L.; Hinton, G.; Kingsbury, B. (2013). "New types of deep neural network learning for speech recognition and related applications: An overview (ICASSP)" (PDF).

^ a b c Yu, D.; Deng, L. (2014). Automatic Speech Recognition: A Deep Learning Approach (Publisher: Springer). ISBN 978-1-4471-5779-3.

^ "Deng receives prestigious IEEE Technical Achievement Award - Microsoft Research". Microsoft Research. 3 December 2015.

^ a b Li, Deng (September 2014). "Keynote talk: 'Achievements and Challenges of Deep Learning - From Speech Analysis and Recognition To Language and Multimodal Processing'". Interspeech.

^ Yu, D.; Deng, L. (2010). "Roles of Pre-Training and Fine-Tuning in Context-Dependent DBN-HMMs for Real-World Speech Recognition". NIPS Workshop on Deep Learning and Unsupervised Feature Learning.

^ Seide, F.; Li, G.; Yu, D. (2011). "Conversational speech transcription using context-dependent deep neural networks". Interspeech.

^ Deng, Li; Li, Jinyu; Huang, Jui-Ting; Yao, Kaisheng; Yu, Dong; Seide, Frank; Seltzer, Mike; Zweig, Geoff; He, Xiaodong (2013-05-01). "Recent Advances in Deep Learning for Speech Research at Microsoft". Microsoft Research.

^ "Nvidia CEO bets big on deep learning and VR". Venture Beat. April 5, 2016.

^ "From not working to neural networking". The Economist.

^ a b Oh, K.-S.; Jung, K. (2004). "GPU implementation of neural networks". Pattern Recognition. 37 (6): 1311–1314. doi:10.1016/j.patcog.2004.01.013.

^ a b Chellapilla, K., Puri, S., and Simard, P. (2006). High performance convolutional neural networks for document processing. International Workshop on Frontiers in Handwriting Recognition.

^ Cireşan, Dan Claudiu; Meier, Ueli; Gambardella, Luca Maria; Schmidhuber, Jürgen (2010-09-21). "Deep, Big, Simple Neural Nets for Handwritten Digit Recognition". Neural Computation. 22 (12): 3207–3220. arXiv:1003.0358. doi:10.1162/neco_a_00052. ISSN 0899-7667. PMID 20858131.

^ Raina, Rajat; Madhavan, Anand; Ng, Andrew Y. (2009). "Large-scale Deep Unsupervised Learning Using Graphics Processors". Proceedings of the 26th Annual International Conference on Machine Learning. ICML '09. New York, NY, USA: ACM: 873–880. CiteSeerX 10.1.1.154.372. doi:10.1145/1553374.1553486. ISBN 9781605585161.

^ Sze, Vivienne; Chen, Yu-Hsin; Yang, Tien-Ju; Emer, Joel (2017). "Efficient Processing of Deep Neural Networks: A Tutorial and Survey". arXiv:1703.09039 [cs.CV].

^ a b "Announcement of the winners of the Merck Molecular Activity Challenge".

^ a b "Multi-task Neural Networks for QSAR Predictions | Data Science Association". www.datascienceassn.org. Retrieved 2017-06-14.

^ a b "Toxicology in the 21st century Data Challenge"

^ a b "NCATS Announces Tox21 Data Challenge Winners".

^ a b "Archived copy". Archived from the original on 2015-02-28. Retrieved 2015-03-05.CS1 maint: Archived copy as title (link)

^ Ciresan, D. C.; Meier, U.; Masci, J.; Gambardella, L. M.; Schmidhuber, J. (2011). "Flexible, High Performance Convolutional Neural Networks for Image Classification" (PDF). International Joint Conference on Artificial Intelligence. doi:10.5591/978-1-57735-516-8/ijcai11-210.

^ Ciresan, Dan; Giusti, Alessandro; Gambardella, Luca M.; Schmidhuber, Juergen (2012).  Pereira, F.; Burges, C. J. C.; Bottou, L.; Weinberger, K. Q. (eds.). Advances in Neural Information Processing Systems 25 (PDF). Curran Associates, Inc. pp. 2843–2851.

^ Ciresan, D.; Giusti, A.; Gambardella, L.M.; Schmidhuber, J. (2013). "Mitosis Detection in Breast Cancer Histology Images using Deep Neural Networks". Proceedings MICCAI. Lecture Notes in Computer Science. 7908: 411–418. doi:10.1007/978-3-642-40763-5_51. ISBN 978-3-642-38708-1.

^ "The Wolfram Language Image Identification Project". www.imageidentify.com. Retrieved 2017-03-22.

^ Vinyals, Oriol; Toshev, Alexander; Bengio, Samy; Erhan, Dumitru (2014). "Show and Tell: A Neural Image Caption Generator". arXiv:1411.4555 [cs.CV]..

^ Fang, Hao; Gupta, Saurabh; Iandola, Forrest; Srivastava, Rupesh; Deng, Li; Dollár, Piotr; Gao, Jianfeng; He, Xiaodong; Mitchell, Margaret; Platt, John C; Lawrence Zitnick, C; Zweig, Geoffrey (2014). "From Captions to Visual Concepts and Back". arXiv:1411.4952 [cs.CV]..

^ Kiros, Ryan; Salakhutdinov, Ruslan; Zemel, Richard S (2014). "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models". arXiv:1411.2539 [cs.LG]..

^ Zhong, Sheng-hua; Liu, Yan; Liu, Yang (2011). "Bilinear Deep Learning for Image Classification". Proceedings of the 19th ACM International Conference on Multimedia. MM '11. New York, NY, USA: ACM: 343–352. doi:10.1145/2072298.2072344. ISBN 9781450306164.

^ "Why Deep Learning Is Suddenly Changing Your Life". Fortune. 2016. Retrieved 13 April 2018.

^ Silver, David; Huang, Aja; Maddison, Chris J.; Guez, Arthur; Sifre, Laurent; Driessche, George van den; Schrittwieser, Julian; Antonoglou, Ioannis; Panneershelvam, Veda (January 2016). "Mastering the game of Go with deep neural networks and tree search". Nature. 529 (7587): 484–489. Bibcode:2016Natur.529..484S. doi:10.1038/nature16961. ISSN 1476-4687. PMID 26819042.

^ Szegedy, Christian; Toshev, Alexander; Erhan, Dumitru (2013). "Deep neural networks for object detection". Advances in Neural Information Processing Systems.

^ Hof, Robert D. "Is Artificial Intelligence Finally Coming into Its Own?". MIT Technology Review. Retrieved 2018-07-10.

^ a b Gers, Felix A.; Schmidhuber, Jürgen (2001). "LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages". IEEE Trans. Neural Netw. 12 (6): 1333–1340. doi:10.1109/72.963769. PMID 18249962.

^ a b c Sutskever, L.; Vinyals, O.; Le, Q. (2014). "Sequence to Sequence Learning with Neural Networks" (PDF). Proc. NIPS.

^ a b Jozefowicz, Rafal; Vinyals, Oriol; Schuster, Mike; Shazeer, Noam; Wu, Yonghui (2016). "Exploring the Limits of Language Modeling". arXiv:1602.02410 [cs.CL].

^ a b Gillick, Dan; Brunk, Cliff; Vinyals, Oriol; Subramanya, Amarnag (2015). "Multilingual Language Processing from Bytes". arXiv:1512.00103 [cs.CL].

^ Mikolov, T.;  et al. (2010). "Recurrent neural network based language model" (PDF). Interspeech.

^ a b "Learning Precise Timing with LSTM Recurrent Networks (PDF Download Available)". ResearchGate. Retrieved 2017-06-13.

^ LeCun, Y.;  et al. (1998). "Gradient-based learning applied to document recognition". Proceedings of the IEEE. 86 (11): 2278–2324. doi:10.1109/5.726791.

^ Bengio, Y.; Boulanger-Lewandowski, N.; Pascanu, R. (May 2013). "Advances in optimizing recurrent networks". 2013 IEEE International Conference on Acoustics, Speech and Signal Processing: 8624–8628. arXiv:1212.0901. CiteSeerX 10.1.1.752.9151. doi:10.1109/icassp.2013.6639349. ISBN 978-1-4799-0356-6.

^ Dahl, G.;  et al. (2013). "Improving DNNs for LVCSR using rectified linear units and dropout" (PDF). ICASSP.

^ "Data Augmentation - deeplearning.ai | Coursera". Coursera. Retrieved 2017-11-30.

^ Hinton, G. E. (2010). "A Practical Guide to Training Restricted Boltzmann Machines". Tech. Rep. UTML TR 2010-003.

^ You, Yang; Buluç, Aydın; Demmel, James (November 2017). "Scaling deep learning on GPU and knights landing clusters". SC '17, ACM. Retrieved 5 March 2018.

^ Viebke, André; Memeti, Suejb; Pllana, Sabri; Abraham, Ajith (March 2017). "CHAOS: a parallelization scheme for training convolutional neural networks on Intel Xeon Phi". The Journal of Supercomputing. 75: 197–227. doi:10.1007/s11227-017-1994-x.

^ Ting Qin, et al. "A learning algorithm of CMAC based on RLS." Neural Processing Letters 19.1 (2004): 49-61.

^ Ting Qin, et al. "Continuous CMAC-QRLS and its systolic array." Neural Processing Letters 22.1 (2005): 1-16.

^ TIMIT Acoustic-Phonetic Continuous Speech Corpus Linguistic Data Consortium, Philadelphia.

^ Robinson, Tony (30 September 1991). "Several Improvements to a Recurrent Error Propagation Network Phone Recognition System". Cambridge University Engineering Department Technical Report. CUED/F-INFENG/TR82. doi:10.13140/RG.2.2.15418.90567.

^ Abdel-Hamid, O.;  et al. (2014). "Convolutional Neural Networks for Speech Recognition". IEEE/ACM Transactions on Audio, Speech, and Language Processing. 22 (10): 1533–1545. doi:10.1109/taslp.2014.2339736.

^ Deng, L.; Platt, J. (2014). "Ensemble Deep Learning for Speech Recognition" (PDF). Proc. Interspeech.

^ Tóth, Laszló (2015). "Phone Recognition with Hierarchical Convolutional Deep Maxout Networks" (PDF). EURASIP Journal on Audio, Speech, and Music Processing. 2015. doi:10.1186/s13636-015-0068-3.

^ "How Skype Used AI to Build Its Amazing New Language Translator | WIRED". www.wired.com. Retrieved 2017-06-14.

^ Hannun, Awni; Case, Carl; Casper, Jared; Catanzaro, Bryan; Diamos, Greg; Elsen, Erich; Prenger, Ryan; Satheesh, Sanjeev; Sengupta, Shubho; Coates, Adam; Ng, Andrew Y (2014). "Deep Speech: Scaling up end-to-end speech recognition". arXiv:1412.5567 [cs.CL].

^ "Plenary presentation at ICASSP-2016" (PDF).

^ "MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges". yann.lecun.com.

^ Cireşan, Dan; Meier, Ueli; Masci, Jonathan; Schmidhuber, Jürgen (August 2012). "Multi-column deep neural network for traffic sign classification". Neural Networks. Selected Papers from IJCNN 2011. 32: 333–338. CiteSeerX 10.1.1.226.8219. doi:10.1016/j.neunet.2012.02.023. PMID 22386783.

^ Nvidia Demos a Car Computer Trained with "Deep Learning" (2015-01-06), David Talbot, MIT Technology Review

^ G. W. Smith; Frederic Fol Leymarie (10 April 2017). "The Machine as Artist: An Introduction". Arts. Retrieved 4 October 2017.

^ Blaise Agüera y Arcas (29 September 2017). "Art in the Age of Machine Intelligence". Arts. Retrieved 4 October 2017.

^ Bengio, Yoshua; Ducharme, Réjean; Vincent, Pascal; Janvin, Christian (March 2003). "A Neural Probabilistic Language Model". J. Mach. Learn. Res. 3: 1137–1155. ISSN 1532-4435.

^ Goldberg, Yoav; Levy, Omar (2014). "word2vec Explained: Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method". arXiv:1402.3722 [cs.CL].

^ a b Socher, Richard; Manning, Christopher. "Deep Learning for NLP" (PDF). Retrieved 26 October 2014.

^ Socher, Richard; Bauer, John; Manning, Christopher; Ng, Andrew (2013). "Parsing With Compositional Vector Grammars" (PDF). Proceedings of the ACL 2013 Conference.

^ Socher, Richard (2013). "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank" (PDF).

^ Shen, Yelong; He, Xiaodong; Gao, Jianfeng; Deng, Li; Mesnil, Gregoire (2014-11-01). "A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval". Microsoft Research.

^ Huang, Po-Sen; He, Xiaodong; Gao, Jianfeng; Deng, Li; Acero, Alex; Heck, Larry (2013-10-01). "Learning Deep Structured Semantic Models for Web Search using Clickthrough Data". Microsoft Research.

^ Mesnil, G.; Dauphin, Y.; Yao, K.; Bengio, Y.; Deng, L.; Hakkani-Tur, D.; He, X.; Heck, L.; Tur, G.; Yu, D.; Zweig, G. (2015). "Using recurrent neural networks for slot filling in spoken language understanding". IEEE Transactions on Audio, Speech, and Language Processing. 23 (3): 530–539. doi:10.1109/taslp.2014.2383614.

^ a b Gao, Jianfeng; He, Xiaodong; Yih, Scott Wen-tau; Deng, Li (2014-06-01). "Learning Continuous Phrase Representations for Translation Modeling". Microsoft Research.

^ Brocardo, Marcelo Luiz; Traore, Issa; Woungang, Isaac; Obaidat, Mohammad S. (2017). "Authorship verification using deep belief network systems". International Journal of Communication Systems. 30 (12): e3259. doi:10.1002/dac.3259.

^ "Deep Learning for Natural Language Processing: Theory and Practice (CIKM2014 Tutorial) - Microsoft Research". Microsoft Research. Retrieved 2017-06-14.

^ Turovsky, Barak (November 15, 2016). "Found in translation: More accurate, fluent sentences in Google Translate". The Keyword Google Blog. Retrieved March 23, 2017.

^ a b c d Schuster, Mike; Johnson, Melvin; Thorat, Nikhil (November 22, 2016). "Zero-Shot Translation with Google's Multilingual Neural Machine Translation System". Google Research Blog. Retrieved March 23, 2017.

^ Sepp Hochreiter; Jürgen Schmidhuber (1997). "Long short-term memory". Neural Computation. 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735. PMID 9377276.

^ Felix A. Gers; Jürgen Schmidhuber; Fred Cummins (2000). "Learning to Forget: Continual Prediction with LSTM". Neural Computation. 12 (10): 2451–2471. CiteSeerX 10.1.1.55.5709. doi:10.1162/089976600300015015.

^ Wu, Yonghui; Schuster, Mike; Chen, Zhifeng; Le, Quoc V; Norouzi, Mohammad; Macherey, Wolfgang; Krikun, Maxim; Cao, Yuan; Gao, Qin; Macherey, Klaus; Klingner, Jeff; Shah, Apurva; Johnson, Melvin; Liu, Xiaobing; Kaiser, Łukasz; Gouws, Stephan; Kato, Yoshikiyo; Kudo, Taku; Kazawa, Hideto; Stevens, Keith; Kurian, George; Patil, Nishant; Wang, Wei; Young, Cliff; Smith, Jason; Riesa, Jason; Rudnick, Alex; Vinyals, Oriol; Corrado, Greg;  et al. (2016). "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation". arXiv:1609.08144 [cs.CL].

^ "An Infusion of AI Makes Google Translate More Powerful Than Ever." Cade Metz, WIRED, Date of Publication: 09.27.16. https://www.wired.com/2016/09/google-claims-ai-breakthrough-machine-translation/

^ a b Boitet, Christian; Blanchon, Hervé; Seligman, Mark; Bellynck, Valérie (2010). "MT on and for the Web" (PDF). Retrieved December 1, 2016.

^ Arrowsmith, J; Miller, P (2013). "Trial watch: Phase II and phase III attrition rates 2011-2012". Nature Reviews Drug Discovery. 12 (8): 569. doi:10.1038/nrd4090. PMID 23903212.

^ Verbist, B; Klambauer, G; Vervoort, L; Talloen, W; The Qstar, Consortium; Shkedy, Z; Thas, O; Bender, A; Göhlmann, H. W.; Hochreiter, S (2015). "Using transcriptomics to guide lead optimization in drug discovery projects: Lessons learned from the QSTAR project". Drug Discovery Today. 20 (5): 505–513. doi:10.1016/j.drudis.2014.12.014. PMID 25582842.

^ Wallach, Izhar; Dzamba, Michael; Heifets, Abraham (2015-10-09). "AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug Discovery". arXiv:1510.02855 [cs.LG].

^ "Toronto startup has a faster way to discover effective medicines". The Globe and Mail. Retrieved 2015-11-09.

^ "Startup Harnesses Supercomputers to Seek Cures". KQED Future of You. Retrieved 2015-11-09.

^ "Toronto startup has a faster way to discover effective medicines".

^ Tkachenko, Yegor (April 8, 2015). "Autonomous CRM Control via CLV Approximation with Deep Reinforcement Learning in Discrete and Continuous Action Space". arXiv:1504.01840 [cs.LG].

^ van den Oord, Aaron; Dieleman, Sander; Schrauwen, Benjamin (2013).  Burges, C. J. C.; Bottou, L.; Welling, M.; Ghahramani, Z.; Weinberger, K. Q. (eds.). Advances in Neural Information Processing Systems 26 (PDF). Curran Associates, Inc. pp. 2643–2651.

^ Elkahky, Ali Mamdouh; Song, Yang; He, Xiaodong (2015-05-01). "A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems". Microsoft Research.

^ Chicco, Davide; Sadowski, Peter; Baldi, Pierre (1 January 2014). Deep Autoencoder Neural Networks for Gene Ontology Annotation Predictions. Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics - BCB '14. ACM. pp. 533–540. doi:10.1145/2649387.2649442. hdl:11311/964622. ISBN 9781450328944.

^ Sathyanarayana, Aarti (2016-01-01). "Sleep Quality Prediction From Wearable Data Using Deep Learning". JMIR mHealth and uHealth. 4 (4): e125. doi:10.2196/mhealth.6562. PMC 5116102. PMID 27815231.

^ Choi, Edward; Schuetz, Andy; Stewart, Walter F.; Sun, Jimeng (2016-08-13). "Using recurrent neural network models for early detection of heart failure onset". Journal of the American Medical Informatics Association. 24 (2): 361–370. doi:10.1093/jamia/ocw112. ISSN 1067-5027. PMC 5391725. PMID 27521897.

^ "Deep Learning in Healthcare: Challenges and Opportunities". Medium. 2016-08-12. Retrieved 2018-04-10.

^ Litjens, Geert; Kooi, Thijs; Bejnordi, Babak Ehteshami; Setio, Arnaud Arindra Adiyoso; Ciompi, Francesco; Ghafoorian, Mohsen; van der Laak, Jeroen A.W.M.; van Ginneken, Bram; Sánchez, Clara I. (December 2017). "A survey on deep learning in medical image analysis". Medical Image Analysis. 42: 60–88. doi:10.1016/j.media.2017.07.005.

^ Forslid, Gustav; Wieslander, Hakan; Bengtsson, Ewert; Wahlby, Carolina; Hirsch, Jan-Michael; Stark, Christina Runow; Sadanandan, Sajith Kecheril (October 2017). "Deep Convolutional Neural Networks for Detecting Cellular Changes Due to Malignancy". 2017 IEEE International Conference on Computer Vision Workshops (ICCVW). Venice: IEEE: 82–89. doi:10.1109/ICCVW.2017.18. ISBN 9781538610343.

^ De, Shaunak; Maity, Abhishek; Goel, Vritti; Shitole, Sanjay; Bhattacharya, Avik (2017). "Predicting the popularity of instagram posts for a lifestyle magazine using deep learning". 2nd IEEE Conference on Communication Systems, Computing and IT Applications: 174–177. doi:10.1109/CSCITA.2017.8066548. ISBN 978-1-5090-4381-1.

^ Schmidt, Uwe; Roth, Stefan. Shrinkage Fields for Effective Image Restoration (PDF). Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on.

^ Czech, Tomasz. "Deep learning: the next frontier for money laundering detection". Global Banking and Finance Review.

^ a b c "Army researchers develop new algorithms to train robots". EurekAlert!. Retrieved 2018-08-29.

^ Utgoff, P. E.; Stracuzzi, D. J. (2002). "Many-layered learning". Neural Computation. 14 (10): 2497–2529. doi:10.1162/08997660260293319. PMID 12396572.

^ Elman, Jeffrey L. (1998). Rethinking Innateness: A Connectionist Perspective on Development. MIT Press. ISBN 978-0-262-55030-7.

^ Shrager, J.; Johnson, MH (1996). "Dynamic plasticity influences the emergence of function in a simple cortical array". Neural Networks. 9 (7): 1119–1129. doi:10.1016/0893-6080(96)00033-0. PMID 12662587.

^ Quartz, SR; Sejnowski, TJ (1997). "The neural basis of cognitive development: A constructivist manifesto". Behavioral and Brain Sciences. 20 (4): 537–556. CiteSeerX 10.1.1.41.7854. doi:10.1017/s0140525x97001581.

^ S. Blakeslee., "In brain's early growth, timetable may be critical," The New York Times, Science Section, pp. B5–B6, 1995.

^ Mazzoni, P.; Andersen, R. A.; Jordan, M. I. (1991-05-15). "A more biologically plausible learning rule for neural networks". Proceedings of the National Academy of Sciences. 88 (10): 4433–4437. Bibcode:1991PNAS...88.4433M. doi:10.1073/pnas.88.10.4433. ISSN 0027-8424. PMC 51674. PMID 1903542.

^ O'Reilly, Randall C. (1996-07-01). "Biologically Plausible Error-Driven Learning Using Local Activation Differences: The Generalized Recirculation Algorithm". Neural Computation. 8 (5): 895–938. doi:10.1162/neco.1996.8.5.895. ISSN 0899-7667.

^ Testolin, Alberto; Zorzi, Marco (2016). "Probabilistic Models and Generative Neural Networks: Towards an Unified Framework for Modeling Normal and Impaired Neurocognitive Functions". Frontiers in Computational Neuroscience. 10: 73. doi:10.3389/fncom.2016.00073. ISSN 1662-5188. PMC 4943066. PMID 27468262.

^ Testolin, Alberto; Stoianov, Ivilin; Zorzi, Marco (September 2017). "Letter perception emerges from unsupervised deep learning and recycling of natural image features". Nature Human Behaviour. 1 (9): 657–664. doi:10.1038/s41562-017-0186-2. ISSN 2397-3374.

^ Buesing, Lars; Bill, Johannes; Nessler, Bernhard; Maass, Wolfgang (2011-11-03). "Neural Dynamics as Sampling: A Model for Stochastic Computation in Recurrent Networks of Spiking Neurons". PLOS Computational Biology. 7 (11): e1002211. Bibcode:2011PLSCB...7E2211B. doi:10.1371/journal.pcbi.1002211. ISSN 1553-7358. PMC 3207943. PMID 22096452.

^ Morel, Danielle; Singh, Chandan; Levy, William B. (2018-01-25). "Linearization of excitatory synaptic integration at no extra cost". Journal of Computational Neuroscience. 44 (2): 173–188. doi:10.1007/s10827-017-0673-5. ISSN 0929-5313. PMID 29372434.

^ Cash, S.; Yuste, R. (February 1999). "Linear summation of excitatory inputs by CA1 pyramidal neurons". Neuron. 22 (2): 383–394. doi:10.1016/s0896-6273(00)81098-3. ISSN 0896-6273. PMID 10069343.

^ Olshausen, B; Field, D (2004-08-01). "Sparse coding of sensory inputs". Current Opinion in Neurobiology. 14 (4): 481–487. doi:10.1016/j.conb.2004.07.007. ISSN 0959-4388.

^ Yamins, Daniel L K; DiCarlo, James J (March 2016). "Using goal-driven deep learning models to understand sensory cortex". Nature Neuroscience. 19 (3): 356–365. doi:10.1038/nn.4244. ISSN 1546-1726.

^ Zorzi, Marco; Testolin, Alberto (2018-02-19). "An emergentist perspective on the origin of number sense". Phil. Trans. R. Soc. B. 373 (1740): 20170043. doi:10.1098/rstb.2017.0043. ISSN 0962-8436. PMC 5784047. PMID 29292348.

^ Güçlü, Umut; van Gerven, Marcel A. J. (2015-07-08). "Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream". Journal of Neuroscience. 35 (27): 10005–10014. arXiv:1411.6422. doi:10.1523/jneurosci.5023-14.2015. PMID 26157000.

^ Metz, C. (12 December 2013). "Facebook's 'Deep Learning' Guru Reveals the Future of AI". Wired.

^ "Google AI algorithm masters ancient game of Go". Nature News & Comment. Retrieved 2016-01-30.

^ Silver, David; Huang, Aja; Maddison, Chris J.; Guez, Arthur; Sifre, Laurent; Driessche, George van den; Schrittwieser, Julian; Antonoglou, Ioannis; Panneershelvam, Veda; Lanctot, Marc; Dieleman, Sander; Grewe, Dominik; Nham, John; Kalchbrenner, Nal; Sutskever, Ilya; Lillicrap, Timothy; Leach, Madeleine; Kavukcuoglu, Koray; Graepel, Thore; Hassabis, Demis (28 January 2016). "Mastering the game of Go with deep neural networks and tree search". Nature. 529 (7587): 484–489. Bibcode:2016Natur.529..484S. doi:10.1038/nature16961. ISSN 0028-0836. PMID 26819042.

^ "A Google DeepMind Algorithm Uses Deep Learning and More to Master the Game of Go | MIT Technology Review". MIT Technology Review. Retrieved 2016-01-30.

^ "Blippar Demonstrates New Real-Time Augmented Reality App". TechCrunch.

^ "TAMER: Training an Agent Manually via Evaluative Reinforcement - IEEE Conference Publication". ieeexplore.ieee.org. Retrieved 2018-08-29.

^ "Talk to the Algorithms: AI Becomes a Faster Learner". governmentciomedia.com. Retrieved 2018-08-29.

^ Marcus, Gary (2018-01-14). "In defense of skepticism about deep learning". Gary Marcus. Retrieved 2018-10-11.

^ Knight, Will (2017-03-14). "DARPA is funding projects that will try to open up AI's black boxes". MIT Technology Review. Retrieved 2017-11-02.

^ Marcus, Gary (November 25, 2012). "Is "Deep Learning" a Revolution in Artificial Intelligence?". The New Yorker. Retrieved 2017-06-14.

^ Smith, G. W. (March 27, 2015). "Art and Artificial Intelligence". ArtEnt. Archived from the original on June 25, 2017. Retrieved March 27, 2015.CS1 maint: BOT: original-url status unknown (link)

^ Mellars, Paul (February 1, 2005). "The Impossible Coincidence: A Single-Species Model for the Origins of Modern Human Behavior in Europe" (PDF). Evolutionary Anthropology: Issues, News, and Reviews. Retrieved April 5, 2017.

^ Alexander Mordvintsev; Christopher Olah; Mike Tyka (June 17, 2015). "Inceptionism: Going Deeper into Neural Networks". Google Research Blog. Retrieved June 20, 2015.

^ Alex Hern (June 18, 2015). "Yes, androids do dream of electric sheep". The Guardian. Retrieved June 20, 2015.

^ a b c Goertzel, Ben (2015). "Are there Deep Reasons Underlying the Pathologies of Today's Deep Learning Algorithms?" (PDF).

^ Nguyen, Anh; Yosinski, Jason; Clune, Jeff (2014). "Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images". arXiv:1412.1897 [cs.CV].

^ Szegedy, Christian; Zaremba, Wojciech; Sutskever, Ilya; Bruna, Joan; Erhan, Dumitru; Goodfellow, Ian; Fergus, Rob (2013). "Intriguing properties of neural networks". arXiv:1312.6199 [cs.CV].

^ Zhu, S.C.; Mumford, D. (2006). "A stochastic grammar of images". Found. Trends Comput. Graph. Vis. 2 (4): 259–362. CiteSeerX 10.1.1.681.2190. doi:10.1561/0600000018.

^ Miller, G. A., and N. Chomsky. "Pattern conception." Paper for Conference on pattern detection, University of Michigan. 1957.

^ Eisner, Jason. "Deep Learning of Recursive Structure: Grammar Induction".

^ a b c d e "AI Is Easy to Fool—Why That Needs to Change". Singularity Hub. 2017-10-10. Retrieved 2017-10-11.

^ Gibney, Elizabeth (2017). "The scientist who spots fake videos". Nature. doi:10.1038/nature.2017.22784.


Further reading[edit]
.mw-parser-output .refbegin{font-size:90%;margin-bottom:0.5em}.mw-parser-output .refbegin-hanging-indents>ul{list-style-type:none;margin-left:0}.mw-parser-output .refbegin-hanging-indents>ul>li,.mw-parser-output .refbegin-hanging-indents>dl>dd{margin-left:0;padding-left:3.2em;text-indent:-3.2em;list-style:none}.mw-parser-output .refbegin-100{font-size:100%}
Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016). Deep Learning. MIT Press. ISBN 978-0-26203561-3, introductory textbook.



For deep versus shallow learning in educational psychology, see Student approaches to learning. For more information, see Artificial neural network.
Machine learning anddata mining
Problems
Classification
Clustering
Regression
Anomaly detection
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction


Supervised learning.mw-parser-output .nobold{font-weight:normal}(classification • regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)


Clustering
BIRCH
CURE
Hierarchical
k-means
Expectation–maximization (EM)
DBSCAN
OPTICS
Mean-shift


Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
t-SNE


Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov


Anomaly detection
k-NN
Local outlier factor


Artificial neural networks
Autoencoder
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net


Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)


Theory
Bias–variance dilemma
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory


Machine-learning venues
NIPS
ICML
ML
JMLR
ArXiv:cs.LG


Glossary of artificial intelligence
Glossary of artificial intelligence


Related articles
List of datasets for machine-learning research
Outline of machine learning


 Machine learning portalvte
Deep learning  (also known as deep structured learning  or hierarchical learning) is part of a broader family of machine learning methods based on artificial neural networks. Learning can be supervised, semi-supervised or unsupervised.[1][2][3]
Deep learning architectures such as deep neural networks, deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases superior to human experts.[4][5][6]
Artificial Neural Networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains.  Specifically, neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.[7][8][9]

.mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}Contents

1 Definition
2 Overview
3 Interpretations
4 History

4.1 Deep learning revolution


5 Neural networks

5.1 Artificial neural networks
5.2 Deep neural networks

5.2.1 Challenges




6 Applications

6.1 Automatic speech recognition
6.2 Image recognition
6.3 Visual art processing
6.4 Natural language processing
6.5 Drug discovery and toxicology
6.6 Customer relationship management
6.7 Recommendation systems
6.8 Bioinformatics
6.9 Medical Image Analysis
6.10 Mobile advertising
6.11 Image restoration
6.12 Financial fraud detection
6.13 Military


7 Relation to human cognitive and brain development
8 Commercial activity
9 Criticism and comment

9.1 Theory
9.2 Errors
9.3 Cyber threat


10 See also
11 References
12 Further reading



Definition[edit]
Deep learning is a class of machine learning algorithms that:[10](pp199–200) use multiple layers to progressively extract higher level features from raw input. For example, in image processing, lower layers may identify edges, while higher layer may identify human-meaningful items such as digits/letters or faces.

Overview[edit]
Most modern deep learning models are based on artificial neural networks, specifically, Convolutional Neural Networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.[11]
In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place in which level on its own. (Of course, this does not completely obviate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.)[1][12]
The "deep" in "deep learning" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.[2] No universally agreed upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth > 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function.[citation needed] Beyond that more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning features.
Deep learning architectures are often constructed with a greedy layer-by-layer method.[clarification needed][further explanation needed][citation needed] Deep learning helps to disentangle these abstractions and pick out which features improve performance.[1]
For supervised learning tasks, deep learning methods obviate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.
Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than labeled data. Examples of deep structures that can be trained in an unsupervised manner are neural history compressors[13] and deep belief networks.[1][14]

Interpretations[edit]
Deep neural networks are generally interpreted in terms of the universal approximation theorem[15][16][17][18][19][20] or probabilistic inference.[10][11][1][2][14][21][22]
The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.[15][16][17][18][19] In 1989, the first proof was published by George Cybenko for sigmoid activation functions[16] and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.[17]
The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al.[20] proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; If the width is smaller or equal to the input dimension, then deep neural network is not a universal approximator.
The probabilistic interpretation[21] derives from the field of machine learning. It features inference,[10][11][1][2][14][21] as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.[21] The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks.[23] The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.[24]

History[edit]
The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986,[25][13] and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.[26][27]
The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1965.[28] A 1971 paper described a deep network with 8 layers trained by the group method of data handling algorithm.[29]
Other deep learning working architectures, specifically those built for computer vision, began with the Neocognitron introduced by Kunihiko Fukushima in 1980.[30] In 1989, Yann LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970,[31][32][33][34] to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days.[35]
By 1991 such systems were used for recognizing isolated 2-D hand-written digits, while recognizing 3-D objects was done by matching 2-D images with a handcrafted 3-D object model. Weng et al. suggested that a human brain does not use a monolithic 3-D object model and in 1992 they published Cresceptron,[36][37][38] a method for performing 3-D object recognition in cluttered scenes. Because it directly used natural images, Cresceptron started the beginning of general-purpose visual learning for natural 3D worlds. Cresceptron is a cascade of layers similar to Neocognitron. But while Neocognitron required a human programmer to hand-merge features, Cresceptron learned an open number of features in each layer without supervision, where each feature is represented by a convolution kernel. Cresceptron segmented each learned object from a cluttered scene through back-analysis through the network. Max pooling, now often adopted by deep neural networks (e.g. ImageNet tests), was first used in Cresceptron to reduce the position resolution by a factor of (2x2) to 1 through the cascade for better generalization.
In 1994, André de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.[39]
In 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton.[40] Many factors contribute to the slow speed, including the vanishing gradient problem analyzed in 1991 by Sepp Hochreiter.[41][42]
Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of artificial neural network's (ANN) computational cost and a lack of understanding of how the brain wires its biological networks.
Both shallow and deep learning (e.g., recurrent nets) of ANNs have been explored for many years.[43][44][45] These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[46] Key difficulties have been analyzed, including gradient diminishing[41] and weak temporal correlation structure in neural predictive models.[47][48] Additional difficulties were the lack of training data and limited computing power.
Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI studied deep neural networks in speech and speaker recognition. Heck's speaker recognition team achieved the first significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation.[49] While SRI experienced success with deep neural networks in speaker recognition, they were unsuccessful in demonstrating similar success in speech recognition.
The principle of elevating "raw" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the "raw" spectrogram or linear filter-bank features in the late 1990s,[49] showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.[50]
Many aspects of speech recognition were taken over by a deep learning method called long short-term memory (LSTM), a recurrent neural network published by Hochreiter and Schmidhuber in 1997.[51] LSTM RNNs avoid the vanishing gradient problem and can learn "Very Deep Learning" tasks[2] that require memories of events that happened thousands of discrete time steps before, which is important for speech. In 2003, LSTM started to become competitive with traditional speech recognizers on certain tasks.[52] Later it was combined with connectionist temporal classification (CTC)[53] in stacks of LSTM RNNs.[54] In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which they made available through Google Voice Search.[55]
In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh[56]
[57][58] showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation.[59] The papers referred to learning for deep belief nets.
Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved.[60][61][62] Convolutional neural networks (CNNs) were superseded for ASR by CTC[53] for LSTM.[51][55][63][64][65][66][67] but are more successful in computer vision.
The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun.[68] Industrial applications of deep learning to large-scale speech recognition started around 2010.
The 2009 NIPS Workshop on Deep Learning for Speech Recognition[69] was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets (DNN) might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets.[70] However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems.[60][71] The nature of the recognition errors produced by the two types of systems was characteristically different,[72][69] offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems.[10][73][74] Analysis around 2009-2010, contrasted the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition,[72][69] eventually leading to pervasive and dominant use in that industry. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.[60][72][70][75]
In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.[76][77][78][73]
Advances in hardware enabled the renewed interest. In 2009, Nvidia was involved in what was called the “big bang” of deep learning, “as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs).”[79] That year, Google Brain used Nvidia GPUs to create capable DNNs. While there, Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times.[80] In particular, GPUs are well-suited for the matrix/vector math involved in machine learning.[81][82] GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days.[83][84] Specialized hardware and algorithm optimizations can be used for efficient processing.[85]

Deep learning revolution[edit]
 How deep learning is a subset of machine learning and how machine learning is a subset of artificial intelligence (AI).
In 2012, a team led by Dahl won the "Merck Molecular Activity Challenge" using multi-task deep neural networks to predict the biomolecular target of one drug.[86][87] In 2014, Hochreiter's group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients, household products and drugs and won the "Tox21 Data Challenge" of NIH, FDA and NCATS.[88][89][90]
Significant additional impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades, and GPU implementations of NNs for years, including CNNs, fast implementations of CNNs with max-pooling on GPUs in the style of Ciresan and colleagues were needed to progress on computer vision.[81][82][35][91][2] In 2011, this approach achieved for the first time superhuman performance in a visual pattern recognition contest. Also in 2011, it won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest.[92] Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR[4] showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records. In October 2012, a similar system by Krizhevsky et al.[5] won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. In November 2012, Ciresan et al.'s system also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic.[93] In 2013 and 2014, the error rate on the ImageNet task using deep learning was further reduced, following a similar trend in large-scale speech recognition. The Wolfram Image Identification project publicized these improvements.[94]
Image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.[95][96][97][98]
Some researchers assess that the October 2012 ImageNet victory anchored the start of a "deep learning revolution" that has transformed the AI industry.[99]
In March 2019, Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.

Neural networks[edit]
Artificial neural networks[edit]
Main article: Artificial neural network
Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as "cat" or "no cat" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.
An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.
Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.
The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.
Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, playing "Go"[100] ).

Deep neural networks[edit]
This section may be too technical for most readers to understand. Please help improve it to make it understandable to non-experts, without removing the technical details.  (July 2016) (Learn how and when to remove this template message)
A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers.[11][2] The DNN finds the correct mathematical manipulation to turn the input into the output, whether it be a linear relationship or a non-linear relationship. The network moves through the layers calculating the probability of each output. For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name "deep" networks.
DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives.[101] The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.[11]
Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.
DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or "weights", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network didn’t accurately recognize a particular pattern, an algorithm would adjust the weights.[102] That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.
Recurrent neural networks (RNNs), in which data can flow in any direction, are used for applications such as language modeling.[103][104][105][106][107] Long short-term memory is particularly effective for this use.[51][108]
Convolutional deep neural networks (CNNs) are used in computer vision.[109] CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).[67]

Challenges[edit]
As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.
DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning[29] or weight decay (




ℓ

2




{\displaystyle \ell _{2}}

-regularization) or sparsity (




ℓ

1




{\displaystyle \ell _{1}}

-regularization) can be applied during training to combat overfitting.[110] Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies.[111] Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.[112]
DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples)[113] speed up computation. Large processing capabilities of many-core architectures (such as, GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.[114][115]
Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights for CMAC. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.[116][117]

Applications[edit]
Automatic speech recognition[edit]
Main article: Speech recognition
Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn "Very Deep Learning" tasks[2] that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates[108] is competitive with traditional speech recognizers on certain tasks.[52]
The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences.[118] Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.



Method
Percent phoneerror rate (PER) (%)


Randomly Initialized RNN[119]
26.1


Bayesian Triphone GMM-HMM
25.6


Hidden Trajectory (Generative) Model
24.8


Monophone Randomly Initialized DNN
23.4


Monophone DBN-DNN
22.4


Triphone GMM-HMM with BMMI Training
21.7


Monophone DBN-DNN on fbank
20.7


Convolutional DNN[120]
20.0


Convolutional DNN w. Heterogeneous Pooling
18.7


Ensemble DNN/CNN/RNN[121]
18.3


Bidirectional LSTM
17.9


Hierarchical Convolutional Deep Maxout Network[122]
16.5

The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003-2007, accelerated progress in eight major areas:[10][75][73]

Scale-up/out and acclerated DNN training and decoding
Sequence discriminative training
Feature processing by deep models with solid understanding of the underlying mechanisms
Adaptation of DNNs and related deep models
Multi-task and transfer learning by DNNs and related deep models
CNNs and how to design them to best exploit domain knowledge of speech
RNN and its rich LSTM variants
Other types of deep models including tensor-based models and integrated deep generative/discriminative models.
All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.[10][123][124][125]

Image recognition[edit]
Main article: Computer vision
A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.[126]
Deep learning-based image recognition has become "superhuman", producing more accurate results than human contestants. This first occurred in 2011.[127]
Deep learning-trained vehicles now interpret 360° camera views.[128] Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.

Visual art processing[edit]
Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of a) identifying the style period of a given painting, b) Neural Style Transfer - capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video, and c) generating striking imagery based on random visual input fields.[129][130]

Natural language processing[edit]
Main article: Natural language processing
Neural networks have been used for implementing language models since the early 2000s.[103][131] LSTM helped to improve machine translation and language modeling.[104][105][106]
Other key techniques in this field are negative sampling[132] and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN.[133] Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing.[133] Deep neural architectures provide the best results for constituency parsing,[134] sentiment analysis,[135] information retrieval,[136][137] spoken language understanding,[138] machine translation,[104][139] contextual entity linking,[139] writing style recognition,[140] Text classification and others.[141]
Recent developments generalize word embedding to sentence embedding.
Google Translate (GT) uses a large end-to-end long short-term memory network.[142][143][144][145][146][147] Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system "learns from millions of examples."[143] It translates "whole sentences at a time, rather than pieces. Google Translate supports over one hundred languages.[143] The network encodes the "semantics of the sentence rather than simply memorizing phrase-to-phrase translations".[143][148] GT uses English as an intermediate between most language pairs.[148]

Drug discovery and toxicology[edit]
For more information, see Drug discovery and Toxicology.
A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects.[149][150] Research has explored use of deep learning to predict the biomolecular targets,[86][87] off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.[88][89][90]
AtomNet is a deep learning system for structure-based rational drug design.[151] AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus[152] and multiple sclerosis.[153][154]

Customer relationship management[edit]
Main article: Customer relationship management
Deep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.[155]

Recommendation systems[edit]
Main article: Recommender system
Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music recommendations.[156] Multiview deep learning has been applied for learning user preferences from multiple domains.[157] The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.

Bioinformatics[edit]
Main article: Bioinformatics
An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.[158]
In medical informatics, deep learning was used to predict sleep quality based on data from wearables[159] and predictions of health complications from electronic health record data.[160] Deep learning has also showed efficacy in healthcare.[161]

Medical Image Analysis[edit]
Deep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement[162][163]

Mobile advertising[edit]
Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and assimilated before a target segment can be created and used in ad serving by any ad server.[164] Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.

Image restoration[edit]
Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization. These applications include learning methods such as "Shrinkage Fields for Effective Image Restoration"[165] which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.

Financial fraud detection[edit]
Deep learning is being successfully applied to financial fraud detection and anti-money laundering. "Deep anti-money laundering detection system can spot and recognize relationships and similarities between data and, further down the road, learn to detect anomalies or classify and predict specific events". The solution leverages both supervised learning techniques, such as the classification of suspicious transactions, and unsupervised learning, e.g. anomaly detection.
[166]

Military[edit]
The United States Department of Defense applied deep learning to train robots in new tasks through observation.[167]

Relation to human cognitive and brain development[edit]
Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s.[168][169][170][171] These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, "...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature."[172]
A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism.[173][174] Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality.[175][176] In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.[177]
Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons[178][179] and neural populations.[180] Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system[181] both at the single-unit[182] and at the population[183] levels.

Commercial activity[edit]
Many organizations employ deep learning for particular applications. Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.[184]
Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player.[185][186][187] Google Translate uses an LSTM to translate between more than 100 languages.
In 2015, Blippar demonstrated a mobile augmented reality application that uses deep learning to recognize objects in real time.[188]
As of 2008,[189] researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor.[167]
First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot the ability to learn new tasks through observation.[167]
Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as “good job” and “bad job.”[190]

Criticism and comment[edit]
Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science.

Theory[edit]
See also: Explainable AI
A main criticism concerns the lack of theory surrounding some methods.[191] Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear.[citation needed] (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.[192]

Others point out that deep learning should be looked at as a step towards realizing strong AI, not as an all-encompassing solution. Despite the power of deep learning methods, they still lack much of the functionality needed for realizing this goal entirely. Research psychologist Gary Marcus noted:"Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning."[193]As an alternative to this emphasis on the limits of deep learning, one author speculated that it might be possible to train a machine vision stack to perform the sophisticated task of discriminating between "old master" and amateur figure drawings, and hypothesized that such a sensitivity might represent the rudiments of a non-trivial machine empathy.[194] This same author proposed that this would be in line with anthropology, which identifies a concern with aesthetics as a key element of behavioral modernity.[195]
In further reference to the idea that artistic sensitivity might inhere within relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained[196] demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's[197] web site.

Errors[edit]
Some deep learning architectures display problematic behaviors,[198] such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images[199] and misclassifying minuscule perturbations of correctly classified images.[200] Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures.[198] These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar[201] decompositions of observed entities and events.[198] Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition[202] and artificial intelligence (AI).[203]

Cyber threat[edit]
As deep learning moves from the lab into the world, research and experience shows that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such a manipulation is termed an “adversarial attack.” In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system.[204] One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.[205]
Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.[204]
ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.[204]
Another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address that would download malware.[204]
In “data poisoning”, false data is continually smuggled into a machine learning system’s training set to prevent it from achieving mastery.[204]

See also[edit]
Applications of artificial intelligence
Comparison of deep learning software
Compressed sensing
Echo state network
List of artificial intelligence projects
Liquid state machine
List of datasets for machine learning research
Reservoir computing
Sparse coding
References[edit]


^ a b c d e f Bengio, Y.; Courville, A.; Vincent, P. (2013). "Representation Learning: A Review and New Perspectives". IEEE Transactions on Pattern Analysis and Machine Intelligence. 35 (8): 1798–1828. arXiv:1206.5538. doi:10.1109/tpami.2013.50. PMID 23787338..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ a b c d e f g h Schmidhuber, J. (2015). "Deep Learning in Neural Networks: An Overview". Neural Networks. 61: 85–117. arXiv:1404.7828. doi:10.1016/j.neunet.2014.09.003. PMID 25462637.

^ Bengio, Yoshua; LeCun, Yann; Hinton, Geoffrey (2015). "Deep Learning". Nature. 521 (7553): 436–444. Bibcode:2015Natur.521..436L. doi:10.1038/nature14539. PMID 26017442.

^ a b Ciresan, Dan; Meier, U.; Schmidhuber, J. (June 2012). "Multi-column deep neural networks for image classification". 2012 IEEE Conference on Computer Vision and Pattern Recognition: 3642–3649. arXiv:1202.2745. doi:10.1109/cvpr.2012.6248110. ISBN 978-1-4673-1228-8.

^ a b Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffry (2012). "ImageNet Classification with Deep Convolutional Neural Networks" (PDF). NIPS 2012: Neural Information Processing Systems, Lake Tahoe, Nevada.

^ "Google's AlphaGo AI wins three-match series against the world's best Go player". TechCrunch. 25 May 2017.

^ Marblestone, Adam H.; Wayne, Greg; Kording, Konrad P. (2016). "Toward an Integration of Deep Learning and Neuroscience". Frontiers in Computational Neuroscience. 10: 94. doi:10.3389/fncom.2016.00094. PMC 5021692. PMID 27683554.

^ Olshausen, B. A. (1996). "Emergence of simple-cell receptive field properties by learning a sparse code for natural images". Nature. 381 (6583): 607–609. Bibcode:1996Natur.381..607O. doi:10.1038/381607a0. PMID 8637596.

^ Bengio, Yoshua; Lee, Dong-Hyun; Bornschein, Jorg; Mesnard, Thomas; Lin, Zhouhan (2015-02-13). "Towards Biologically Plausible Deep Learning". arXiv:1502.04156 [cs.LG].

^ a b c d e f Deng, L.; Yu, D. (2014). "Deep Learning: Methods and Applications" (PDF). Foundations and Trends in Signal Processing. 7 (3–4): 1–199. doi:10.1561/2000000039.

^ a b c d e Bengio, Yoshua (2009). "Learning Deep Architectures for AI" (PDF). Foundations and Trends in Machine Learning. 2 (1): 1–127. CiteSeerX 10.1.1.701.9550. doi:10.1561/2200000006.

^ LeCun, Yann; Bengio, Yoshua; Hinton, Geoffrey (28 May 2015). "Deep learning". Nature. 521 (7553): 436–444. Bibcode:2015Natur.521..436L. doi:10.1038/nature14539. PMID 26017442.

^ a b Jürgen Schmidhuber (2015). Deep Learning. Scholarpedia, 10(11):32832. Online

^ a b c Hinton, G.E. (2009). "Deep belief networks". Scholarpedia. 4 (5): 5947. Bibcode:2009SchpJ...4.5947H. doi:10.4249/scholarpedia.5947.

^ a b Balázs Csanád Csáji (2001). Approximation with Artificial Neural Networks; Faculty of Sciences; Eötvös Loránd University, Hungary

^ a b c Cybenko (1989). "Approximations by superpositions of sigmoidal functions" (PDF). Mathematics of Control, Signals, and Systems. 2 (4): 303–314. doi:10.1007/bf02551274. Archived from the original (PDF) on 2015-10-10.

^ a b c Hornik, Kurt (1991). "Approximation Capabilities of Multilayer Feedforward Networks". Neural Networks. 4 (2): 251–257. doi:10.1016/0893-6080(91)90009-t.

^ a b Haykin, Simon S. (1999). Neural Networks: A Comprehensive Foundation. Prentice Hall. ISBN 978-0-13-273350-2.

^ a b Hassoun, Mohamad H. (1995). Fundamentals of Artificial Neural Networks. MIT Press. p. 48. ISBN 978-0-262-08239-6.

^ a b Lu, Z., Pu, H., Wang, F., Hu, Z., & Wang, L. (2017). The Expressive Power of Neural Networks: A View from the Width. Neural Information Processing Systems, 6231-6239.

^ a b c d Murphy, Kevin P. (24 August 2012). Machine Learning: A Probabilistic Perspective. MIT Press. ISBN 978-0-262-01802-9.

^ Patel, Ankit; Nguyen, Tan; Baraniuk, Richard (2016). "A Probabilistic Framework for Deep Learning" (PDF). Advances in Neural Information Processing Systems.

^ Hinton, G. E.; Srivastava, N.; Krizhevsky, A.; Sutskever, I.; Salakhutdinov, R.R. (2012). "Improving neural networks by preventing co-adaptation of feature detectors". arXiv:1207.0580 [math.LG].

^ Bishop, Christopher M. (2006). Pattern Recognition and Machine Learning (PDF). Springer. ISBN 978-0-387-31073-2.

^ Rina Dechter (1986). Learning while searching in constraint-satisfaction problems. University of California, Computer Science Department, Cognitive Systems Laboratory.Online

^ Igor Aizenberg, Naum N. Aizenberg, Joos P.L. Vandewalle (2000). Multi-Valued and Universal Binary Neurons: Theory, Learning and Applications. Springer Science & Business Media.

^ Co-evolving recurrent neurons learn deep memory POMDPs. Proc. GECCO, Washington, D. C., pp. 1795-1802, ACM Press, New York, NY, USA, 2005.

^ Ivakhnenko, A. G. (1973). Cybernetic Predicting Devices. CCM Information Corporation.

^ a b Ivakhnenko, Alexey (1971). "Polynomial theory of complex systems". IEEE Transactions on Systems, Man and Cybernetics. 1 (4): 364–378. doi:10.1109/TSMC.1971.4308320.

^ Fukushima, K. (1980). "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position". Biol. Cybern. 36 (4): 193–202. doi:10.1007/bf00344251. PMID 7370364.

^ Seppo Linnainmaa (1970). The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors. Master's Thesis (in Finnish), Univ. Helsinki, 6-7.

^ Griewank, Andreas (2012). "Who Invented the Reverse Mode of Differentiation?" (PDF). Documenta Matematica (Extra Volume ISMP): 389–400.

^ Werbos, P. (1974). "Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences". Harvard University. Retrieved 12 June 2017.

^ Werbos, Paul (1982). "Applications of advances in nonlinear sensitivity analysis" (PDF). System modeling and optimization. Springer. pp. 762–770.

^ a b LeCun et al., "Backpropagation Applied to Handwritten Zip Code Recognition," Neural Computation, 1, pp. 541–551, 1989.

^ J. Weng, N. Ahuja and T. S. Huang, "Cresceptron: a self-organizing neural network which grows adaptively," Proc. International Joint Conference on Neural Networks, Baltimore, Maryland, vol I, pp. 576-581, June, 1992.

^ J. Weng, N. Ahuja and T. S. Huang, "Learning recognition and segmentation of 3-D objects from 2-D images," Proc. 4th International Conf. Computer Vision, Berlin, Germany, pp. 121-128, May, 1993.

^ J. Weng, N. Ahuja and T. S. Huang, "Learning recognition and segmentation using the Cresceptron," International Journal of Computer Vision, vol. 25, no. 2, pp. 105-139, Nov. 1997.

^ de Carvalho, Andre C. L. F.; Fairhurst, Mike C.; Bisset, David (1994-08-08). "An integrated Boolean neural network for pattern classification". Pattern Recognition Letters. 15 (8): 807–813. doi:10.1016/0167-8655(94)90009-4.

^ Hinton, Geoffrey E.; Dayan, Peter; Frey, Brendan J.; Neal, Radford (1995-05-26). "The wake-sleep algorithm for unsupervised neural networks". Science. 268 (5214): 1158–1161. Bibcode:1995Sci...268.1158H. doi:10.1126/science.7761831.

^ a b S. Hochreiter., "Untersuchungen zu dynamischen neuronalen Netzen," Diploma thesis. Institut f. Informatik, Technische Univ. Munich. Advisor: J. Schmidhuber, 1991.

^ Hochreiter, S.;  et al. (15 January 2001). "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies".  In Kolen, John F.; Kremer, Stefan C. (eds.). A Field Guide to Dynamical Recurrent Networks. John Wiley & Sons. ISBN 978-0-7803-5369-5.

^ Morgan, Nelson; Bourlard, Hervé; Renals, Steve; Cohen, Michael; Franco, Horacio (1993-08-01). "Hybrid neural network/hidden markov model systems for continuous speech recognition". International Journal of Pattern Recognition and Artificial Intelligence. 07 (4): 899–916. doi:10.1142/s0218001493000455. ISSN 0218-0014.

^ Robinson, T. (1992). "A real-time recurrent error propagation network word recognition system". ICASSP: 617–620.

^ Waibel, A.; Hanazawa, T.; Hinton, G.; Shikano, K.; Lang, K. J. (March 1989). "Phoneme recognition using time-delay neural networks". IEEE Transactions on Acoustics, Speech, and Signal Processing. 37 (3): 328–339. doi:10.1109/29.21701. ISSN 0096-3518.

^ Baker, J.; Deng, Li; Glass, Jim; Khudanpur, S.; Lee, C.-H.; Morgan, N.; O'Shaughnessy, D. (2009). "Research Developments and Directions in Speech Recognition and Understanding, Part 1". IEEE Signal Processing Magazine. 26 (3): 75–80. Bibcode:2009ISPM...26...75B. doi:10.1109/msp.2009.932166.

^ Bengio, Y. (1991). "Artificial Neural Networks and their Application to Speech/Sequence Recognition". McGill University Ph.D. thesis.

^ Deng, L.; Hassanein, K.; Elmasry, M. (1994). "Analysis of correlation structure for a neural predictive model with applications to speech recognition". Neural Networks. 7 (2): 331–339. doi:10.1016/0893-6080(94)90027-2.

^ a b Heck, L.; Konig, Y.; Sonmez, M.; Weintraub, M. (2000). "Robustness to Telephone Handset Distortion in Speaker Recognition by Discriminative Feature Design". Speech Communication. 31 (2): 181–192. doi:10.1016/s0167-6393(99)00077-1.

^ "Acoustic Modeling with Deep Neural Networks Using Raw Time Signal for LVCSR (PDF Download Available)". ResearchGate. Retrieved 2017-06-14.

^ a b c Hochreiter, Sepp; Schmidhuber, Jürgen (1997-11-01). "Long Short-Term Memory". Neural Computation. 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735. ISSN 0899-7667. PMID 9377276.

^ a b Graves, Alex; Eck, Douglas; Beringer, Nicole; Schmidhuber, Jürgen (2003). "Biologically Plausible Speech Recognition with LSTM Neural Nets" (PDF). 1st Intl. Workshop on Biologically Inspired Approaches to Advanced Information Technology, Bio-ADIT 2004, Lausanne, Switzerland. pp. 175–184.

^ a b Graves, Alex; Fernández, Santiago; Gomez, Faustino (2006). "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks". Proceedings of the International Conference on Machine Learning, ICML 2006: 369–376. CiteSeerX 10.1.1.75.6306.

^ Santiago Fernandez, Alex Graves, and Jürgen Schmidhuber (2007). An application of recurrent neural networks to discriminative keyword spotting. Proceedings of ICANN (2), pp. 220–229.

^ a b Sak, Haşim; Senior, Andrew; Rao, Kanishka; Beaufays, Françoise; Schalkwyk, Johan (September 2015). "Google voice search: faster and more accurate".

^ Hinton, Geoffrey E. (2007-10-01). "Learning multiple layers of representation". Trends in Cognitive Sciences. 11 (10): 428–434. doi:10.1016/j.tics.2007.09.004. ISSN 1364-6613. PMID 17921042.

^ Hinton, G. E.; Osindero, S.; Teh, Y. W. (2006). "A Fast Learning Algorithm for Deep Belief Nets" (PDF). Neural Computation. 18 (7): 1527–1554. doi:10.1162/neco.2006.18.7.1527. PMID 16764513.

^ Bengio, Yoshua (2012). "Practical recommendations for gradient-based training of deep architectures". arXiv:1206.5533 [cs.LG].

^ G. E. Hinton., "Learning multiple layers of representation," Trends in Cognitive Sciences, 11, pp. 428–434, 2007.

^ a b c Hinton, G.; Deng, L.; Yu, D.; Dahl, G.; Mohamed, A.; Jaitly, N.; Senior, A.; Vanhoucke, V.; Nguyen, P.; Sainath, T.; Kingsbury, B. (2012). "Deep Neural Networks for Acoustic Modeling in Speech Recognition --- The shared views of four research groups". IEEE Signal Processing Magazine. 29 (6): 82–97. doi:10.1109/msp.2012.2205597.

^ Deng, Li; Hinton, Geoffrey; Kingsbury, Brian (1 May 2013). "New types of deep neural network learning for speech recognition and related applications: An overview"  – via research.microsoft.com.

^ Deng, L.; Li, J.; Huang, J. T.; Yao, K.; Yu, D.; Seide, F.; Seltzer, M.; Zweig, G.; He, X. (May 2013). "Recent advances in deep learning for speech research at Microsoft". 2013 IEEE International Conference on Acoustics, Speech and Signal Processing: 8604–8608. doi:10.1109/icassp.2013.6639345. ISBN 978-1-4799-0356-6.

^ Sak, Hasim; Senior, Andrew; Beaufays, Francoise (2014). "Long Short-Term Memory recurrent neural network architectures for large scale acoustic modeling" (PDF).

^ Li, Xiangang; Wu, Xihong (2014). "Constructing Long Short-Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition". arXiv:1410.4281 [cs.CL].

^ Zen, Heiga; Sak, Hasim (2015). "Unidirectional Long Short-Term Memory Recurrent Neural Network with Recurrent Output Layer for Low-Latency Speech Synthesis" (PDF). Google.com. ICASSP. pp. 4470–4474.

^ Deng, L.; Abdel-Hamid, O.; Yu, D. (2013). "A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion" (PDF). Google.com. ICASSP.

^ a b Sainath, T. N.; Mohamed, A. r; Kingsbury, B.; Ramabhadran, B. (May 2013). "Deep convolutional neural networks for LVCSR". 2013 IEEE International Conference on Acoustics, Speech and Signal Processing: 8614–8618. doi:10.1109/icassp.2013.6639347. ISBN 978-1-4799-0356-6.

^ Yann LeCun (2016). Slides on Deep Learning Online

^ a b c NIPS Workshop: Deep Learning for Speech Recognition and Related Applications, Whistler, BC, Canada, Dec. 2009 (Organizers: Li Deng, Geoff Hinton, D. Yu).

^ a b Keynote talk: Recent Developments in Deep Neural Networks. ICASSP, 2013 (by Geoff Hinton).

^ D. Yu, L. Deng, G. Li, and F. Seide (2011). "Discriminative pretraining of deep neural networks," U.S. Patent Filing.

^ a b c Deng, L.; Hinton, G.; Kingsbury, B. (2013). "New types of deep neural network learning for speech recognition and related applications: An overview (ICASSP)" (PDF).

^ a b c Yu, D.; Deng, L. (2014). Automatic Speech Recognition: A Deep Learning Approach (Publisher: Springer). ISBN 978-1-4471-5779-3.

^ "Deng receives prestigious IEEE Technical Achievement Award - Microsoft Research". Microsoft Research. 3 December 2015.

^ a b Li, Deng (September 2014). "Keynote talk: 'Achievements and Challenges of Deep Learning - From Speech Analysis and Recognition To Language and Multimodal Processing'". Interspeech.

^ Yu, D.; Deng, L. (2010). "Roles of Pre-Training and Fine-Tuning in Context-Dependent DBN-HMMs for Real-World Speech Recognition". NIPS Workshop on Deep Learning and Unsupervised Feature Learning.

^ Seide, F.; Li, G.; Yu, D. (2011). "Conversational speech transcription using context-dependent deep neural networks". Interspeech.

^ Deng, Li; Li, Jinyu; Huang, Jui-Ting; Yao, Kaisheng; Yu, Dong; Seide, Frank; Seltzer, Mike; Zweig, Geoff; He, Xiaodong (2013-05-01). "Recent Advances in Deep Learning for Speech Research at Microsoft". Microsoft Research.

^ "Nvidia CEO bets big on deep learning and VR". Venture Beat. April 5, 2016.

^ "From not working to neural networking". The Economist.

^ a b Oh, K.-S.; Jung, K. (2004). "GPU implementation of neural networks". Pattern Recognition. 37 (6): 1311–1314. doi:10.1016/j.patcog.2004.01.013.

^ a b Chellapilla, K., Puri, S., and Simard, P. (2006). High performance convolutional neural networks for document processing. International Workshop on Frontiers in Handwriting Recognition.

^ Cireşan, Dan Claudiu; Meier, Ueli; Gambardella, Luca Maria; Schmidhuber, Jürgen (2010-09-21). "Deep, Big, Simple Neural Nets for Handwritten Digit Recognition". Neural Computation. 22 (12): 3207–3220. arXiv:1003.0358. doi:10.1162/neco_a_00052. ISSN 0899-7667. PMID 20858131.

^ Raina, Rajat; Madhavan, Anand; Ng, Andrew Y. (2009). "Large-scale Deep Unsupervised Learning Using Graphics Processors". Proceedings of the 26th Annual International Conference on Machine Learning. ICML '09. New York, NY, USA: ACM: 873–880. CiteSeerX 10.1.1.154.372. doi:10.1145/1553374.1553486. ISBN 9781605585161.

^ Sze, Vivienne; Chen, Yu-Hsin; Yang, Tien-Ju; Emer, Joel (2017). "Efficient Processing of Deep Neural Networks: A Tutorial and Survey". arXiv:1703.09039 [cs.CV].

^ a b "Announcement of the winners of the Merck Molecular Activity Challenge".

^ a b "Multi-task Neural Networks for QSAR Predictions | Data Science Association". www.datascienceassn.org. Retrieved 2017-06-14.

^ a b "Toxicology in the 21st century Data Challenge"

^ a b "NCATS Announces Tox21 Data Challenge Winners".

^ a b "Archived copy". Archived from the original on 2015-02-28. Retrieved 2015-03-05.CS1 maint: Archived copy as title (link)

^ Ciresan, D. C.; Meier, U.; Masci, J.; Gambardella, L. M.; Schmidhuber, J. (2011). "Flexible, High Performance Convolutional Neural Networks for Image Classification" (PDF). International Joint Conference on Artificial Intelligence. doi:10.5591/978-1-57735-516-8/ijcai11-210.

^ Ciresan, Dan; Giusti, Alessandro; Gambardella, Luca M.; Schmidhuber, Juergen (2012).  Pereira, F.; Burges, C. J. C.; Bottou, L.; Weinberger, K. Q. (eds.). Advances in Neural Information Processing Systems 25 (PDF). Curran Associates, Inc. pp. 2843–2851.

^ Ciresan, D.; Giusti, A.; Gambardella, L.M.; Schmidhuber, J. (2013). "Mitosis Detection in Breast Cancer Histology Images using Deep Neural Networks". Proceedings MICCAI. Lecture Notes in Computer Science. 7908: 411–418. doi:10.1007/978-3-642-40763-5_51. ISBN 978-3-642-38708-1.

^ "The Wolfram Language Image Identification Project". www.imageidentify.com. Retrieved 2017-03-22.

^ Vinyals, Oriol; Toshev, Alexander; Bengio, Samy; Erhan, Dumitru (2014). "Show and Tell: A Neural Image Caption Generator". arXiv:1411.4555 [cs.CV]..

^ Fang, Hao; Gupta, Saurabh; Iandola, Forrest; Srivastava, Rupesh; Deng, Li; Dollár, Piotr; Gao, Jianfeng; He, Xiaodong; Mitchell, Margaret; Platt, John C; Lawrence Zitnick, C; Zweig, Geoffrey (2014). "From Captions to Visual Concepts and Back". arXiv:1411.4952 [cs.CV]..

^ Kiros, Ryan; Salakhutdinov, Ruslan; Zemel, Richard S (2014). "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models". arXiv:1411.2539 [cs.LG]..

^ Zhong, Sheng-hua; Liu, Yan; Liu, Yang (2011). "Bilinear Deep Learning for Image Classification". Proceedings of the 19th ACM International Conference on Multimedia. MM '11. New York, NY, USA: ACM: 343–352. doi:10.1145/2072298.2072344. ISBN 9781450306164.

^ "Why Deep Learning Is Suddenly Changing Your Life". Fortune. 2016. Retrieved 13 April 2018.

^ Silver, David; Huang, Aja; Maddison, Chris J.; Guez, Arthur; Sifre, Laurent; Driessche, George van den; Schrittwieser, Julian; Antonoglou, Ioannis; Panneershelvam, Veda (January 2016). "Mastering the game of Go with deep neural networks and tree search". Nature. 529 (7587): 484–489. Bibcode:2016Natur.529..484S. doi:10.1038/nature16961. ISSN 1476-4687. PMID 26819042.

^ Szegedy, Christian; Toshev, Alexander; Erhan, Dumitru (2013). "Deep neural networks for object detection". Advances in Neural Information Processing Systems.

^ Hof, Robert D. "Is Artificial Intelligence Finally Coming into Its Own?". MIT Technology Review. Retrieved 2018-07-10.

^ a b Gers, Felix A.; Schmidhuber, Jürgen (2001). "LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages". IEEE Trans. Neural Netw. 12 (6): 1333–1340. doi:10.1109/72.963769. PMID 18249962.

^ a b c Sutskever, L.; Vinyals, O.; Le, Q. (2014). "Sequence to Sequence Learning with Neural Networks" (PDF). Proc. NIPS.

^ a b Jozefowicz, Rafal; Vinyals, Oriol; Schuster, Mike; Shazeer, Noam; Wu, Yonghui (2016). "Exploring the Limits of Language Modeling". arXiv:1602.02410 [cs.CL].

^ a b Gillick, Dan; Brunk, Cliff; Vinyals, Oriol; Subramanya, Amarnag (2015). "Multilingual Language Processing from Bytes". arXiv:1512.00103 [cs.CL].

^ Mikolov, T.;  et al. (2010). "Recurrent neural network based language model" (PDF). Interspeech.

^ a b "Learning Precise Timing with LSTM Recurrent Networks (PDF Download Available)". ResearchGate. Retrieved 2017-06-13.

^ LeCun, Y.;  et al. (1998). "Gradient-based learning applied to document recognition". Proceedings of the IEEE. 86 (11): 2278–2324. doi:10.1109/5.726791.

^ Bengio, Y.; Boulanger-Lewandowski, N.; Pascanu, R. (May 2013). "Advances in optimizing recurrent networks". 2013 IEEE International Conference on Acoustics, Speech and Signal Processing: 8624–8628. arXiv:1212.0901. CiteSeerX 10.1.1.752.9151. doi:10.1109/icassp.2013.6639349. ISBN 978-1-4799-0356-6.

^ Dahl, G.;  et al. (2013). "Improving DNNs for LVCSR using rectified linear units and dropout" (PDF). ICASSP.

^ "Data Augmentation - deeplearning.ai | Coursera". Coursera. Retrieved 2017-11-30.

^ Hinton, G. E. (2010). "A Practical Guide to Training Restricted Boltzmann Machines". Tech. Rep. UTML TR 2010-003.

^ You, Yang; Buluç, Aydın; Demmel, James (November 2017). "Scaling deep learning on GPU and knights landing clusters". SC '17, ACM. Retrieved 5 March 2018.

^ Viebke, André; Memeti, Suejb; Pllana, Sabri; Abraham, Ajith (March 2017). "CHAOS: a parallelization scheme for training convolutional neural networks on Intel Xeon Phi". The Journal of Supercomputing. 75: 197–227. doi:10.1007/s11227-017-1994-x.

^ Ting Qin, et al. "A learning algorithm of CMAC based on RLS." Neural Processing Letters 19.1 (2004): 49-61.

^ Ting Qin, et al. "Continuous CMAC-QRLS and its systolic array." Neural Processing Letters 22.1 (2005): 1-16.

^ TIMIT Acoustic-Phonetic Continuous Speech Corpus Linguistic Data Consortium, Philadelphia.

^ Robinson, Tony (30 September 1991). "Several Improvements to a Recurrent Error Propagation Network Phone Recognition System". Cambridge University Engineering Department Technical Report. CUED/F-INFENG/TR82. doi:10.13140/RG.2.2.15418.90567.

^ Abdel-Hamid, O.;  et al. (2014). "Convolutional Neural Networks for Speech Recognition". IEEE/ACM Transactions on Audio, Speech, and Language Processing. 22 (10): 1533–1545. doi:10.1109/taslp.2014.2339736.

^ Deng, L.; Platt, J. (2014). "Ensemble Deep Learning for Speech Recognition" (PDF). Proc. Interspeech.

^ Tóth, Laszló (2015). "Phone Recognition with Hierarchical Convolutional Deep Maxout Networks" (PDF). EURASIP Journal on Audio, Speech, and Music Processing. 2015. doi:10.1186/s13636-015-0068-3.

^ "How Skype Used AI to Build Its Amazing New Language Translator | WIRED". www.wired.com. Retrieved 2017-06-14.

^ Hannun, Awni; Case, Carl; Casper, Jared; Catanzaro, Bryan; Diamos, Greg; Elsen, Erich; Prenger, Ryan; Satheesh, Sanjeev; Sengupta, Shubho; Coates, Adam; Ng, Andrew Y (2014). "Deep Speech: Scaling up end-to-end speech recognition". arXiv:1412.5567 [cs.CL].

^ "Plenary presentation at ICASSP-2016" (PDF).

^ "MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges". yann.lecun.com.

^ Cireşan, Dan; Meier, Ueli; Masci, Jonathan; Schmidhuber, Jürgen (August 2012). "Multi-column deep neural network for traffic sign classification". Neural Networks. Selected Papers from IJCNN 2011. 32: 333–338. CiteSeerX 10.1.1.226.8219. doi:10.1016/j.neunet.2012.02.023. PMID 22386783.

^ Nvidia Demos a Car Computer Trained with "Deep Learning" (2015-01-06), David Talbot, MIT Technology Review

^ G. W. Smith; Frederic Fol Leymarie (10 April 2017). "The Machine as Artist: An Introduction". Arts. Retrieved 4 October 2017.

^ Blaise Agüera y Arcas (29 September 2017). "Art in the Age of Machine Intelligence". Arts. Retrieved 4 October 2017.

^ Bengio, Yoshua; Ducharme, Réjean; Vincent, Pascal; Janvin, Christian (March 2003). "A Neural Probabilistic Language Model". J. Mach. Learn. Res. 3: 1137–1155. ISSN 1532-4435.

^ Goldberg, Yoav; Levy, Omar (2014). "word2vec Explained: Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method". arXiv:1402.3722 [cs.CL].

^ a b Socher, Richard; Manning, Christopher. "Deep Learning for NLP" (PDF). Retrieved 26 October 2014.

^ Socher, Richard; Bauer, John; Manning, Christopher; Ng, Andrew (2013). "Parsing With Compositional Vector Grammars" (PDF). Proceedings of the ACL 2013 Conference.

^ Socher, Richard (2013). "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank" (PDF).

^ Shen, Yelong; He, Xiaodong; Gao, Jianfeng; Deng, Li; Mesnil, Gregoire (2014-11-01). "A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval". Microsoft Research.

^ Huang, Po-Sen; He, Xiaodong; Gao, Jianfeng; Deng, Li; Acero, Alex; Heck, Larry (2013-10-01). "Learning Deep Structured Semantic Models for Web Search using Clickthrough Data". Microsoft Research.

^ Mesnil, G.; Dauphin, Y.; Yao, K.; Bengio, Y.; Deng, L.; Hakkani-Tur, D.; He, X.; Heck, L.; Tur, G.; Yu, D.; Zweig, G. (2015). "Using recurrent neural networks for slot filling in spoken language understanding". IEEE Transactions on Audio, Speech, and Language Processing. 23 (3): 530–539. doi:10.1109/taslp.2014.2383614.

^ a b Gao, Jianfeng; He, Xiaodong; Yih, Scott Wen-tau; Deng, Li (2014-06-01). "Learning Continuous Phrase Representations for Translation Modeling". Microsoft Research.

^ Brocardo, Marcelo Luiz; Traore, Issa; Woungang, Isaac; Obaidat, Mohammad S. (2017). "Authorship verification using deep belief network systems". International Journal of Communication Systems. 30 (12): e3259. doi:10.1002/dac.3259.

^ "Deep Learning for Natural Language Processing: Theory and Practice (CIKM2014 Tutorial) - Microsoft Research". Microsoft Research. Retrieved 2017-06-14.

^ Turovsky, Barak (November 15, 2016). "Found in translation: More accurate, fluent sentences in Google Translate". The Keyword Google Blog. Retrieved March 23, 2017.

^ a b c d Schuster, Mike; Johnson, Melvin; Thorat, Nikhil (November 22, 2016). "Zero-Shot Translation with Google's Multilingual Neural Machine Translation System". Google Research Blog. Retrieved March 23, 2017.

^ Sepp Hochreiter; Jürgen Schmidhuber (1997). "Long short-term memory". Neural Computation. 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735. PMID 9377276.

^ Felix A. Gers; Jürgen Schmidhuber; Fred Cummins (2000). "Learning to Forget: Continual Prediction with LSTM". Neural Computation. 12 (10): 2451–2471. CiteSeerX 10.1.1.55.5709. doi:10.1162/089976600300015015.

^ Wu, Yonghui; Schuster, Mike; Chen, Zhifeng; Le, Quoc V; Norouzi, Mohammad; Macherey, Wolfgang; Krikun, Maxim; Cao, Yuan; Gao, Qin; Macherey, Klaus; Klingner, Jeff; Shah, Apurva; Johnson, Melvin; Liu, Xiaobing; Kaiser, Łukasz; Gouws, Stephan; Kato, Yoshikiyo; Kudo, Taku; Kazawa, Hideto; Stevens, Keith; Kurian, George; Patil, Nishant; Wang, Wei; Young, Cliff; Smith, Jason; Riesa, Jason; Rudnick, Alex; Vinyals, Oriol; Corrado, Greg;  et al. (2016). "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation". arXiv:1609.08144 [cs.CL].

^ "An Infusion of AI Makes Google Translate More Powerful Than Ever." Cade Metz, WIRED, Date of Publication: 09.27.16. https://www.wired.com/2016/09/google-claims-ai-breakthrough-machine-translation/

^ a b Boitet, Christian; Blanchon, Hervé; Seligman, Mark; Bellynck, Valérie (2010). "MT on and for the Web" (PDF). Retrieved December 1, 2016.

^ Arrowsmith, J; Miller, P (2013). "Trial watch: Phase II and phase III attrition rates 2011-2012". Nature Reviews Drug Discovery. 12 (8): 569. doi:10.1038/nrd4090. PMID 23903212.

^ Verbist, B; Klambauer, G; Vervoort, L; Talloen, W; The Qstar, Consortium; Shkedy, Z; Thas, O; Bender, A; Göhlmann, H. W.; Hochreiter, S (2015). "Using transcriptomics to guide lead optimization in drug discovery projects: Lessons learned from the QSTAR project". Drug Discovery Today. 20 (5): 505–513. doi:10.1016/j.drudis.2014.12.014. PMID 25582842.

^ Wallach, Izhar; Dzamba, Michael; Heifets, Abraham (2015-10-09). "AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug Discovery". arXiv:1510.02855 [cs.LG].

^ "Toronto startup has a faster way to discover effective medicines". The Globe and Mail. Retrieved 2015-11-09.

^ "Startup Harnesses Supercomputers to Seek Cures". KQED Future of You. Retrieved 2015-11-09.

^ "Toronto startup has a faster way to discover effective medicines".

^ Tkachenko, Yegor (April 8, 2015). "Autonomous CRM Control via CLV Approximation with Deep Reinforcement Learning in Discrete and Continuous Action Space". arXiv:1504.01840 [cs.LG].

^ van den Oord, Aaron; Dieleman, Sander; Schrauwen, Benjamin (2013).  Burges, C. J. C.; Bottou, L.; Welling, M.; Ghahramani, Z.; Weinberger, K. Q. (eds.). Advances in Neural Information Processing Systems 26 (PDF). Curran Associates, Inc. pp. 2643–2651.

^ Elkahky, Ali Mamdouh; Song, Yang; He, Xiaodong (2015-05-01). "A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems". Microsoft Research.

^ Chicco, Davide; Sadowski, Peter; Baldi, Pierre (1 January 2014). Deep Autoencoder Neural Networks for Gene Ontology Annotation Predictions. Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics - BCB '14. ACM. pp. 533–540. doi:10.1145/2649387.2649442. hdl:11311/964622. ISBN 9781450328944.

^ Sathyanarayana, Aarti (2016-01-01). "Sleep Quality Prediction From Wearable Data Using Deep Learning". JMIR mHealth and uHealth. 4 (4): e125. doi:10.2196/mhealth.6562. PMC 5116102. PMID 27815231.

^ Choi, Edward; Schuetz, Andy; Stewart, Walter F.; Sun, Jimeng (2016-08-13). "Using recurrent neural network models for early detection of heart failure onset". Journal of the American Medical Informatics Association. 24 (2): 361–370. doi:10.1093/jamia/ocw112. ISSN 1067-5027. PMC 5391725. PMID 27521897.

^ "Deep Learning in Healthcare: Challenges and Opportunities". Medium. 2016-08-12. Retrieved 2018-04-10.

^ Litjens, Geert; Kooi, Thijs; Bejnordi, Babak Ehteshami; Setio, Arnaud Arindra Adiyoso; Ciompi, Francesco; Ghafoorian, Mohsen; van der Laak, Jeroen A.W.M.; van Ginneken, Bram; Sánchez, Clara I. (December 2017). "A survey on deep learning in medical image analysis". Medical Image Analysis. 42: 60–88. doi:10.1016/j.media.2017.07.005.

^ Forslid, Gustav; Wieslander, Hakan; Bengtsson, Ewert; Wahlby, Carolina; Hirsch, Jan-Michael; Stark, Christina Runow; Sadanandan, Sajith Kecheril (October 2017). "Deep Convolutional Neural Networks for Detecting Cellular Changes Due to Malignancy". 2017 IEEE International Conference on Computer Vision Workshops (ICCVW). Venice: IEEE: 82–89. doi:10.1109/ICCVW.2017.18. ISBN 9781538610343.

^ De, Shaunak; Maity, Abhishek; Goel, Vritti; Shitole, Sanjay; Bhattacharya, Avik (2017). "Predicting the popularity of instagram posts for a lifestyle magazine using deep learning". 2nd IEEE Conference on Communication Systems, Computing and IT Applications: 174–177. doi:10.1109/CSCITA.2017.8066548. ISBN 978-1-5090-4381-1.

^ Schmidt, Uwe; Roth, Stefan. Shrinkage Fields for Effective Image Restoration (PDF). Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on.

^ Czech, Tomasz. "Deep learning: the next frontier for money laundering detection". Global Banking and Finance Review.

^ a b c "Army researchers develop new algorithms to train robots". EurekAlert!. Retrieved 2018-08-29.

^ Utgoff, P. E.; Stracuzzi, D. J. (2002). "Many-layered learning". Neural Computation. 14 (10): 2497–2529. doi:10.1162/08997660260293319. PMID 12396572.

^ Elman, Jeffrey L. (1998). Rethinking Innateness: A Connectionist Perspective on Development. MIT Press. ISBN 978-0-262-55030-7.

^ Shrager, J.; Johnson, MH (1996). "Dynamic plasticity influences the emergence of function in a simple cortical array". Neural Networks. 9 (7): 1119–1129. doi:10.1016/0893-6080(96)00033-0. PMID 12662587.

^ Quartz, SR; Sejnowski, TJ (1997). "The neural basis of cognitive development: A constructivist manifesto". Behavioral and Brain Sciences. 20 (4): 537–556. CiteSeerX 10.1.1.41.7854. doi:10.1017/s0140525x97001581.

^ S. Blakeslee., "In brain's early growth, timetable may be critical," The New York Times, Science Section, pp. B5–B6, 1995.

^ Mazzoni, P.; Andersen, R. A.; Jordan, M. I. (1991-05-15). "A more biologically plausible learning rule for neural networks". Proceedings of the National Academy of Sciences. 88 (10): 4433–4437. Bibcode:1991PNAS...88.4433M. doi:10.1073/pnas.88.10.4433. ISSN 0027-8424. PMC 51674. PMID 1903542.

^ O'Reilly, Randall C. (1996-07-01). "Biologically Plausible Error-Driven Learning Using Local Activation Differences: The Generalized Recirculation Algorithm". Neural Computation. 8 (5): 895–938. doi:10.1162/neco.1996.8.5.895. ISSN 0899-7667.

^ Testolin, Alberto; Zorzi, Marco (2016). "Probabilistic Models and Generative Neural Networks: Towards an Unified Framework for Modeling Normal and Impaired Neurocognitive Functions". Frontiers in Computational Neuroscience. 10: 73. doi:10.3389/fncom.2016.00073. ISSN 1662-5188. PMC 4943066. PMID 27468262.

^ Testolin, Alberto; Stoianov, Ivilin; Zorzi, Marco (September 2017). "Letter perception emerges from unsupervised deep learning and recycling of natural image features". Nature Human Behaviour. 1 (9): 657–664. doi:10.1038/s41562-017-0186-2. ISSN 2397-3374.

^ Buesing, Lars; Bill, Johannes; Nessler, Bernhard; Maass, Wolfgang (2011-11-03). "Neural Dynamics as Sampling: A Model for Stochastic Computation in Recurrent Networks of Spiking Neurons". PLOS Computational Biology. 7 (11): e1002211. Bibcode:2011PLSCB...7E2211B. doi:10.1371/journal.pcbi.1002211. ISSN 1553-7358. PMC 3207943. PMID 22096452.

^ Morel, Danielle; Singh, Chandan; Levy, William B. (2018-01-25). "Linearization of excitatory synaptic integration at no extra cost". Journal of Computational Neuroscience. 44 (2): 173–188. doi:10.1007/s10827-017-0673-5. ISSN 0929-5313. PMID 29372434.

^ Cash, S.; Yuste, R. (February 1999). "Linear summation of excitatory inputs by CA1 pyramidal neurons". Neuron. 22 (2): 383–394. doi:10.1016/s0896-6273(00)81098-3. ISSN 0896-6273. PMID 10069343.

^ Olshausen, B; Field, D (2004-08-01). "Sparse coding of sensory inputs". Current Opinion in Neurobiology. 14 (4): 481–487. doi:10.1016/j.conb.2004.07.007. ISSN 0959-4388.

^ Yamins, Daniel L K; DiCarlo, James J (March 2016). "Using goal-driven deep learning models to understand sensory cortex". Nature Neuroscience. 19 (3): 356–365. doi:10.1038/nn.4244. ISSN 1546-1726.

^ Zorzi, Marco; Testolin, Alberto (2018-02-19). "An emergentist perspective on the origin of number sense". Phil. Trans. R. Soc. B. 373 (1740): 20170043. doi:10.1098/rstb.2017.0043. ISSN 0962-8436. PMC 5784047. PMID 29292348.

^ Güçlü, Umut; van Gerven, Marcel A. J. (2015-07-08). "Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream". Journal of Neuroscience. 35 (27): 10005–10014. arXiv:1411.6422. doi:10.1523/jneurosci.5023-14.2015. PMID 26157000.

^ Metz, C. (12 December 2013). "Facebook's 'Deep Learning' Guru Reveals the Future of AI". Wired.

^ "Google AI algorithm masters ancient game of Go". Nature News & Comment. Retrieved 2016-01-30.

^ Silver, David; Huang, Aja; Maddison, Chris J.; Guez, Arthur; Sifre, Laurent; Driessche, George van den; Schrittwieser, Julian; Antonoglou, Ioannis; Panneershelvam, Veda; Lanctot, Marc; Dieleman, Sander; Grewe, Dominik; Nham, John; Kalchbrenner, Nal; Sutskever, Ilya; Lillicrap, Timothy; Leach, Madeleine; Kavukcuoglu, Koray; Graepel, Thore; Hassabis, Demis (28 January 2016). "Mastering the game of Go with deep neural networks and tree search". Nature. 529 (7587): 484–489. Bibcode:2016Natur.529..484S. doi:10.1038/nature16961. ISSN 0028-0836. PMID 26819042.

^ "A Google DeepMind Algorithm Uses Deep Learning and More to Master the Game of Go | MIT Technology Review". MIT Technology Review. Retrieved 2016-01-30.

^ "Blippar Demonstrates New Real-Time Augmented Reality App". TechCrunch.

^ "TAMER: Training an Agent Manually via Evaluative Reinforcement - IEEE Conference Publication". ieeexplore.ieee.org. Retrieved 2018-08-29.

^ "Talk to the Algorithms: AI Becomes a Faster Learner". governmentciomedia.com. Retrieved 2018-08-29.

^ Marcus, Gary (2018-01-14). "In defense of skepticism about deep learning". Gary Marcus. Retrieved 2018-10-11.

^ Knight, Will (2017-03-14). "DARPA is funding projects that will try to open up AI's black boxes". MIT Technology Review. Retrieved 2017-11-02.

^ Marcus, Gary (November 25, 2012). "Is "Deep Learning" a Revolution in Artificial Intelligence?". The New Yorker. Retrieved 2017-06-14.

^ Smith, G. W. (March 27, 2015). "Art and Artificial Intelligence". ArtEnt. Archived from the original on June 25, 2017. Retrieved March 27, 2015.CS1 maint: BOT: original-url status unknown (link)

^ Mellars, Paul (February 1, 2005). "The Impossible Coincidence: A Single-Species Model for the Origins of Modern Human Behavior in Europe" (PDF). Evolutionary Anthropology: Issues, News, and Reviews. Retrieved April 5, 2017.

^ Alexander Mordvintsev; Christopher Olah; Mike Tyka (June 17, 2015). "Inceptionism: Going Deeper into Neural Networks". Google Research Blog. Retrieved June 20, 2015.

^ Alex Hern (June 18, 2015). "Yes, androids do dream of electric sheep". The Guardian. Retrieved June 20, 2015.

^ a b c Goertzel, Ben (2015). "Are there Deep Reasons Underlying the Pathologies of Today's Deep Learning Algorithms?" (PDF).

^ Nguyen, Anh; Yosinski, Jason; Clune, Jeff (2014). "Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images". arXiv:1412.1897 [cs.CV].

^ Szegedy, Christian; Zaremba, Wojciech; Sutskever, Ilya; Bruna, Joan; Erhan, Dumitru; Goodfellow, Ian; Fergus, Rob (2013). "Intriguing properties of neural networks". arXiv:1312.6199 [cs.CV].

^ Zhu, S.C.; Mumford, D. (2006). "A stochastic grammar of images". Found. Trends Comput. Graph. Vis. 2 (4): 259–362. CiteSeerX 10.1.1.681.2190. doi:10.1561/0600000018.

^ Miller, G. A., and N. Chomsky. "Pattern conception." Paper for Conference on pattern detection, University of Michigan. 1957.

^ Eisner, Jason. "Deep Learning of Recursive Structure: Grammar Induction".

^ a b c d e "AI Is Easy to Fool—Why That Needs to Change". Singularity Hub. 2017-10-10. Retrieved 2017-10-11.

^ Gibney, Elizabeth (2017). "The scientist who spots fake videos". Nature. doi:10.1038/nature.2017.22784.


Further reading[edit]
.mw-parser-output .refbegin{font-size:90%;margin-bottom:0.5em}.mw-parser-output .refbegin-hanging-indents>ul{list-style-type:none;margin-left:0}.mw-parser-output .refbegin-hanging-indents>ul>li,.mw-parser-output .refbegin-hanging-indents>dl>dd{margin-left:0;padding-left:3.2em;text-indent:-3.2em;list-style:none}.mw-parser-output .refbegin-100{font-size:100%}
Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016). Deep Learning. MIT Press. ISBN 978-0-26203561-3, introductory textbook.



This article may rely excessively on sources too closely associated with the subject, potentially preventing the article from being verifiable and neutral. Please help improve it by replacing them with more appropriate citations to reliable, independent, third-party sources. (May 2019) (Learn how and when to remove this template message)
In U.S. education, deeper learning is a set of student educational outcomes including acquisition of robust core academic content, higher-order thinking skills, and learning dispositions. Deeper learning is based on the premise that the nature of work, civic, and everyday life is changing and therefore increasingly requires that formal education provides young people with mastery of skills like analytic reasoning, complex problem solving, and teamwork. 
Deeper learning is associated with a growing movement in U.S. education that places special emphasis on the ability to apply knowledge to real-world circumstances and to solve novel problems.[1]
A number of U.S. schools and school districts serving a broad socio-economic spectrum apply deeper learning as an integral component of their instructional approach.[2]

Contents

1 History
2 Skills and competencies
3 Instructional reforms
4 Network of schools
5 Assessment
6 See also
7 References
8 External links


History[edit]
While the term "deeper learning" is relatively new, the notion of enabling students to develop skills that empower them to apply learning and to adapt to and thrive in post-secondary education as well as career and life is not. A number of significant antecedents to deeper learning exist.
For example, American philosopher, psychologist and educational reformer John Dewey (1859–1952) made a strong case for the importance of education not only as a place to gain content knowledge, but also as a place to learn how to live. Like modern proponents of deeper learning, Dewey believed that students thrive in an environment where they are allowed to experience and interact with the curriculum, and all students should have the opportunity to take part in their own learning. Dewey's arguments undergirded the movements of progressive education and constructivist education, which called for teaching and learning beyond rote content knowledge.
In the 1990s, skills-based education saw a resurgence with the advent of the "21st Century Skills" movements and the "Partnership for 21st Century skills".[3] In 2012 the National Research Council of the National Academies issued Education for Life and Work: Developing Transferrable Knowledge and Skill in the 21st Century, a report on deeper learning re-elevating the issue and summarizing research evidence on its outcomes to date.[4]

Skills and competencies[edit]
According to labor economists Frank Levy of MIT and Richard Murnane of Harvard’s Graduate School of Education, since 1970, with the economic changes brought about by technology and globalization, employers’ demands for workers with routine, repetitive skills—whether manual or cognitive—have dropped steeply, while demand for those with deeper learning competencies like complex thinking and communications skills has soared.[5]
Research by Cassel and Kolstad found that by the year 2000 the top skills demanded by U.S. Fortune 500 companies had shifted from traditional reading, writing and arithmetic to teamwork, problem solving, and interpersonal skills.[6]
A 2006 Conference Board survey of some 400 employers revealed that key deeper learning competencies were the most important for new entrants into the workforce. Essential capabilities included oral and written communications and critical thinking/problem solving. The Conference Board findings indicate that "applied skills on all educational levels trump basic knowledge and skills, such as Reading Comprehension and Mathematics ... while the ‘three Rs’ are still fundamental to any new workforce entrant’s ability to do the job, employers emphasize that applied skills like Teamwork/Collaboration and Critical Thinking are ‘very important’ to success at work."[7]
In 2002 a coalition of national business community, education leaders, and policymakers founded the Partnership for 21st Century Skills (now the Partnership for 21st Century Learning, or P21), a non-profit organization.  P21's goal is to foster a national conversation on "the importance of 21st century skills for all students" and "position 21st century readiness at the center of US K-12 education".  The organization has released reports exploring how to integrate the Four Cs approach into learning environments.[8]  Their research and publications included an identification of deeper learning competencies and skills they called the Four Cs of 21st century learning (collaboration, communication, critical thinking, creativity).  In a 2012 survey conducted by the American Management Association (AMA), executives found a need for highly skilled employees to keep up with the fast pace of change in business and to compete on a global level.  The survey identified three of the "Four Cs" (critical thinking, communication and collaboration) as the top three skills necessary for these employees.[9]
"Deeper learning" was described by the William and Flora Hewlett Foundation in 2010[10] specifying a set of educational outcomes:[11]

Mastery of rigorous academic content
Development of critical thinking and problem-solving skills
The ability to work collaboratively
Effective oral and written communication
Learning how to learn
Developing and maintaining an academic mindset.
Instructional reforms[edit]
Deeper learning practitioners have developed a number instructional reform methods and built a variety of classroom, school, and district models. While stressing robust content mastery, instructors ask students to "move beyond basic comprehension and algorithmic procedures and engage in skills that lie at the top of traditional learning taxonomies—analysis, synthesis, and creation," according to Harvard education scholars Jal Mehta and Sarah Fine.[12] "Students are treated as active meaning makers with the capacity to do interesting and valuable work now ... the purpose of school is not so much to prepare students for a hypothetical future as to support them in engaging with the complex challenges that professional work at its best entails."

In its 2012 report Education for Life and Work, the National Research Council identified the following research-based methods for developing deeper learning:[4]

Use multiple and varied representations of concepts and tasks
Encourage elaboration, questioning, and self-explanation
Engage learners in challenging tasks, with supportive guidance and feedback
Teach with examples and cases
Prime student motivation
Use formative assessment
Deeper examination of what "best practices" evidence shows connect teaching methods to the development of the Partnership for 21st Century Learning's 4C framework[13] and the competencies identified in the Hewlett model for deeper learning,[14] give a sharper picture of "what works" in terms of instructional strategies and tools. For instance, the Marzano Lab has identified the high effects of cooperative learning to develop collaboration, graphic organizers to advance critical thinking, feedback to sharpen communication, advance organizers to enrich entry activities in PBLs, etc.[15] John Hattie's meta-analysis of visible learning is even more specific. Strategies that promote metacognition, reflection, student feedback, creativity, inquiry and more support the type of teaching that most enriches mindful, deeper learning. In addition, his studies detail how surface teaching strategies such as lectures, worksheets, overly frequent testing and others do little for achievement or deeper learning.[16] For young learners, the Center for Childhood Creativity has identified the powerful role of creative thinking in the classroom.[17]
While evidence supporting the direct impact of education organized around deeper learning outcomes in driving academic achievement is not robust to date, it continues to build. P21 is leading an effort at the University of Connecticut to remedy this.  As early as 2008 a study of seven hundred California students demonstrated that students exposed to math instruction designed to develop deeper learning competencies significantly outperformed peers taught through more traditional methods.[1]

Network of schools[edit]
A number of educational reform school networks across the country focus on developing deeper learning competencies.[18] While committed to deeper learning educational outcomes, these networks, however, vary in their instructional models and approaches to school design. Notable networks include Asia Society International Studies Schools Network, EdVisions Schools, and Envision Education.
Because of limits imposed by state and federal laws, public school districts face the largest challenges to bring deeper learning back into their schools. The Partnership for 21st Century Learning ([19] initiated the identification of exemplar schools which were relying on inclusion of 21st Century Skills as a base component for bringing deeper learning experiences to all children. Some of these exemplar schools come from the reform networks, but many are schools and districts that targeted deeper learning instruction and outcomes as their mission but without the benefits in money, public relations and compliance given to charter schools.
To further advance the notion, P21 created a Blogazine to "connect the dots between 21st Century skills and deeper learning outcomes".[20] The blog articles are written pro bono by major educational writers who advocate for the paradigm shift to Deeper Learning as well as by a balance of school leaders, teachers, professional learning specialists and others who are incorporating deeper learning practices into their curricula, instruction, assessment and system change plans. In its second year, the no-fee online P21 Blogazine expanded into a three times weekly online journal.
As more schools, especially public schools, began to plan to integrate deeper learning, a group of Illinois advocates, aligned with P21, searched for assistance to scale best 21st Century teaching practices into classrooms. Already successful exemplars in the US and abroad were relying on versions of project based learning (problem-based, inquiry-based, product-making, project- based);[21] there was great variation in effectiveness. After reviewing models from multiple sources, the Illinois Consortium for 21st Century Schools determined none were adequate for systemic integration into schools or systems. The consortium team, made up of volunteer, long experienced professional developers, classroom teachers, administrators and school change specialists, all with experience in public school reform, adapted and redesigned the most effective PBL models and designed a new school-wide approach of PBL that included explicit instruction and assessment of the 4CS as advocated by the Partnership, technology, reflection and a 5th C, cultural responsiveness. These elements were integrated into a PBL design cycle, called MindQuest21. Creative making was balanced with critical thinking to allow for teachers to challenge the narrow framework of the standards which ignored the creative C.[22]
The MindQuest21 approach was not an isolated example. As the P21 Exemplar identification program showed, more and more schools, often acting alone, sometimes in concert with other schools in a district, were shifting the learning paradigm from surface learning pushed by NCLB recall tests to deeper learning stimulated by entrepreneurial administrators and teachers. In a like manner, creative teachers who were able to defy the punishment threats of NCLB, did the same.[23]

Assessment[edit]
The majority of tests used in the current U.S. school system focus mainly on achievement of content knowledge and rely heavily on multiple-choice items, measuring primarily low-level knowledge and some basic skills.[24] A study by the RAND Corporation found that, in the 17 states studied, fewer than 2% of mathematics items and only about 20% of English language arts (ELA) items on state tests ask students to analyze, synthesize, compare, critique, investigate, prove, or explain their ideas.[24]
However, two federally funded multi-state assessment consortia—the Partnership for Assessment of Readiness for College and Careers (PARCC) and the Smarter Balanced Assessment Consortium (SBAC)[25] —were formed to develop next-generation assessment tools, to be launched in 2014–15. Research conducted by UCLA's CRESST show marked increases in the amount of higher-order skills to be assessed as measured by the Depth of Knowledge scale.[26] The Innovation Lab Network (ILN) of states,[27] coordinated by the Council of Chief State School Officers, convenes a smaller, informal consortium of ten states to develop strategies to create and deploy even more intellectually ambitious assessments. The performance assessments under development by participating states includes tasks that require students to analyze, critique, evaluate, and apply knowledge. The new tests also intend to encourage instruction aimed at helping students acquire and use knowledge in more complex ways.[24]
In September 2014, a report was released by the American Institutes for Research on a three-year, quasi-experimental comparison of traditional and Deeper Learning schools. The research findings demonstrated the following improved student outcomes: students attending deeper learning network schools benefited from greater opportunities to engage in deeper learning and reported higher levels of academic engagement, motivation to learn, self-efficacy, and collaboration skills; students had higher state standardized assessment scores regardless of student background; students scored higher on PISA-based Test for Schools[28] on measures of core content knowledge and complex problem-solving skills; students graduated on time at statistically significantly higher rates (9 percent); and after graduation students were more likely to attend four-year colleges and enroll in more selective institutions.[29]

See also[edit]
Design-based learning
Hands-on learning
Problem-based learning
Project-based learning
References[edit]


^ a b Martinez, Monica; McGrath, Dennis (2014). Deeper Learning: How Eight Innovative Public Schools Are Transforming Education in the Twenty-First Century. New York: The New Press. pp. 1–21. ISBN 978-1-59558-959-0..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ Mehta, Jal; Fine, Sarah (July–August 2014). "The Elusive Quest for Deeper Learning". Harvard Education Letter. 30 (4).

^ "About us". Partnership for 21st Century. Retrieved 28 September 2014.

^ a b Pellegrino, James W.; Hilton, Margaret L. (2012). Education for Life and Work: Developing Transferrable Knowledge and Skill in the 21st Century. Washington, D.C.: The National Academies Press.

^ Murnane, Richard J.; Levy, Frank (1996). Teaching the New Basic Skills: Principles for Educating Children to Thrive in a Changing Economy. New York: Free Press.

^ Cassel, R.N.; Kolstad, R. (1998). "The critical job-skills requirements for the 21st century: Living and working with people". Journal of Instructional Psychology. 25 (3): 176–180.

^ Are They Ready to Work? Employers' Perspectives on the Basic Knowledge and Applied Skills of New Entrants to the 21st Century U.S. Workforce (PDF). Washington, D.C.: Partnership for 21st Century Skills. 2006.

^ P21 Our History. Retrieved 2016-03-05

^ Critical Skills Survey (PDF). New York: American Management Association. 2012.

^ "Deeper Learning Strategic Plan Summary Education Program December 2012 Update" (PDF). Hewlett Foundation. Hewlett Foundation. December 2012. Retrieved 5 May 2019.

^ "Deeper Learning Defined" (PDF). Hewlett Foundation. Hewlett Foundation. April 2013. Retrieved 13 March 2019.

^ Mehta, Jal; Fine, Sarah (2012). "Teaching differently ... Learning deeply" (PDF). Kappan Magazine. 94 (2): 31–35.

^ http://www.p21.org/framework

^ http://www.hewlett.org/education/deeper learning

^ http://www.marzanoresearch.com/instructional strategies

^ http://www.treasury/gov.nz/publications//hattie

^ http://centerforchildhoodcreativity.org

^ Rothman, Robert (March–April 2013). "Diving into Deeper Learning: Schools gear up to promote thinking skills". Harvard Education Letter. 29 (2).

^ http://www.p21.org/exemplars

^ http://www.p21.org/our-work/p21blog

^ Bellanca, James A. (2010). Enriched Learning Projects (1st ed.). Bloomington, In: Solution Tree Press. p. 223. ISBN 978-1-934009-74-1.

^ http://www.ilc21.org/MindQuest21

^ http://www.p21.org/blogazine

^ a b c Darling-Hammond, Linda; Adamson, Frank (2013). Developing Assessments of Deeper Learning: The Costs and Benefits of Using Tests that Help Students Learn (PDF). Stanford, CA: Stanford Center for Opportunity Policy in Education. p. i.

^ "About". Smarter Balanced Assessment Consortium (SBAC). Retrieved 28 September 2014.

^ "Depth of Knowledge scale" (PDF). Archived from the original (PDF) on 12 June 2014.

^ "Innovation Lab Network". Council of Chief State School Officers. Retrieved 28 September 2014.

^ "PISA-based Test for Schools". Organisation for Economic Co-operation and Development (OECD). Retrieved 13 March 2019.

^ "Evidence of Deeper Learning Outcomes". American Institutes for Research. 24 September 2014. Retrieved 28 September 2014.


External links[edit]
Need a Job? Invent It. Friedman, Thomas L. New York Times, 3/31/2013
'The Banality of Deeper Learning', Loveless, Tom. The Brown Center Chalkboard Blog, Brookings, 5/29/13
8-Part Blog Series on Deeper Learning, Edutopia, George Lucas Education Foundation
Spotlight on Deeper Learning, Education Week
Teachers Embrace "Deep Learning," Translating Lessons into Practical Skills, PBS NewsHour
Kentucky School Aims for "Deeper Learning", PBS NewsHour
Can "Deeper Learning" Close the Achievement Gap? PBS NewsHour
OpEd: The Quest for Deeper Learning, Chow, Barbara. Education Week



The following table compares notable software frameworks, libraries and computer programs for deep learning.


Contents

1 Deep-learning software by name
2 Related software
3 See also
4 References


Deep-learning software by name[edit]


Software

Creator

Initial Release

Software license[a]

Open source

Platform

Written in

Interface

OpenMP support

OpenCL support

CUDA support

Automatic differentiation[1]

Has pretrained models

Recurrent nets

Convolutional nets

RBM/DBNs

Parallel execution (multi node)

Actively Developed


BigDL

Jason Dai (Intel)

2016

Apache 2.0

Yes

Apache Spark

Scala

Scala, Python





No



Yes

Yes

Yes








Caffe

Berkeley Vision and Learning Center

2013

BSD

Yes

Linux, macOS, Windows[2]

C++

Python, MATLAB, C++

Yes

Under development[3]

Yes

Yes

Yes[4]

Yes

Yes

No

?




Chainer

Preferred Networks

2015

BSD

Yes

Linux, macOS

Python

Python

No

No

Yes

Yes

Yes

Yes

Yes

No

Yes

Yes


Deeplearning4j

Skymind engineering team; Deeplearning4j community; originally Adam Gibson

2014

Apache 2.0

Yes

Linux, macOS, Windows, Android (Cross-platform)

C++, Java

Java, Scala, Clojure, Python (Keras), Kotlin

Yes

No[5]

Yes[6][7]

Computational Graph

Yes[8]

Yes

Yes

Yes

Yes[9]




Dlib

Davis King

2002

Boost Software License

Yes

Cross-Platform

C++

C++

Yes

No

Yes

Yes

Yes

No

Yes

Yes

Yes




Intel Data Analytics Acceleration Library

Intel

2015

Apache License 2.0

Yes

Linux, macOS, Windows on Intel CPU[10]

C++, Python, Java

C++, Python, Java[10]

Yes

No

No

Yes

No



Yes



Yes




Intel Math Kernel Library

Intel



Proprietary

No

Linux, macOS, Windows on Intel CPU[11]



C[12]

Yes[13]

No

No

Yes

No

Yes[14]

Yes[14]



No




Keras

François Chollet

2015

MIT license

Yes

Linux, macOS, Windows

Python

Python, R

Only if using Theano as backend

Can use Theano, Tensorflow or PlaidML as backends

Yes

Yes

Yes[15]

Yes

Yes

No[16]

Yes[17]

Yes


MATLAB + Deep Learning Toolbox

MathWorks



Proprietary

No

Linux, macOS, Windows

C, C++, Java, MATLAB

MATLAB

No

No

Train with Parallel Computing Toolbox and generate CUDA code with GPU Coder[18]

No

Yes[19][20]

Yes[19]

Yes[19]

No

With Parallel Computing Toolbox[21]

Yes


Microsoft Cognitive Toolkit (CNTK)

Microsoft Research

2016

MIT license[22]

Yes

Windows, Linux[23] (macOS via Docker on roadmap)

C++

Python (Keras), C++,  Command line,[24] BrainScript[25] (.NET on roadmap[26])

Yes[27]

No

Yes

Yes

Yes[28]

Yes[29]

Yes[29]

No[30]

Yes[31]

No[32]


Apache MXNet

Apache Software Foundation

2015

Apache 2.0

Yes

Linux, macOS, Windows,[33][34] AWS, Android,[35] iOS, JavaScript[36]

Small C++ core library

C++, Python, Julia, Matlab, JavaScript, Go, R, Scala, Perl

Yes

On roadmap[37]

Yes

Yes[38]

Yes[39]

Yes

Yes

Yes

Yes[40]

Yes


Neural Designer

Artelnics



Proprietary

No

Linux, macOS, Windows

C++

Graphical user interface

Yes

No

No

?

?

No

No

No

?




OpenNN

Artelnics

2003

GNU LGPL

Yes

Cross-platform

C++

C++

Yes

No

Yes

?

?

No

No

No

?




PyTorch

Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan (Facebook)

2016

BSD

Yes

Linux, macOS, Windows

Python, C, C++, CUDA

Python, C++

Yes

Via separately maintained package[41][42][43]

Yes

Yes

Yes

Yes

Yes



Yes

Yes


Apache SINGA

Apache Incubator

2015

Apache 2.0

Yes

Linux, macOS, Windows

C++

Python, C++, Java

No

Supported in V1.0

Yes

?

Yes

Yes

Yes

Yes

Yes




TensorFlow

Google Brain

2015

Apache 2.0

Yes

Linux, macOS, Windows,[44] Android

C++, Python, CUDA

Python (Keras), C/C++, Java, Go, JavaScript, R,[45] Julia, Swift

No

On roadmap[46] but already with SYCL[47] support

Yes

Yes[48]

Yes[49]

Yes

Yes

Yes

Yes

Yes


Theano

Université de Montréal

2007

BSD

Yes

Cross-platform

Python

Python (Keras)

Yes

Under development[50]

Yes

Yes[51][52]

Through Lasagne's model zoo[53]

Yes

Yes

Yes

Yes[54]

No


Torch

Ronan Collobert, Koray Kavukcuoglu, Clement Farabet

2002

BSD

Yes

Linux, macOS, Windows,[55] Android,[56] iOS

C, Lua

Lua, LuaJIT,[57] C, utility library for C++/OpenCL[58]

Yes

Third party implementations[59][60]

Yes[61][62]

Through Twitter's Autograd[63]

Yes[64]

Yes

Yes

Yes

Yes[65]

No


Wolfram Mathematica

Wolfram Research

1988

Proprietary

No

Windows, macOS, Linux, Cloud computing

C++, Wolfram Language, CUDA

Wolfram Language

Yes

No

Yes

Yes

Yes[66]

Yes

Yes

Yes

Under Development

Yes



^ Licenses here are a summary, and are not taken to be complete statements of the licenses. Some libraries may use other libraries internally under different licenses


Related software[edit]
Neural Engineering Object (NENGO) – A graphical and scripting software for simulating large-scale neural systems
Numenta Platform for Intelligent Computing – Numenta's open source implementation of their hierarchical temporal memory model
See also[edit]
Comparison of numerical-analysis software
Comparison of statistical packages
List of datasets for machine-learning research
List of numerical-analysis software
References[edit]


^ Atilim Gunes Baydin; Barak A. Pearlmutter; Alexey Andreyevich Radul; Jeffrey Mark Siskind (20 February 2015). "Automatic differentiation in machine learning: a survey". arXiv:1502.05767 [cs.LG]..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ "Microsoft/caffe". GitHub.

^ "OpenCL Caffe".

^ "Caffe Model Zoo".

^ "Support for Open CL · Issue #27 · deeplearning4j/nd4j". GitHub.

^ "N-Dimensional Scientific Computing for Java".

^ "Comparing Top Deep Learning Frameworks". Deeplearning4j.

^ Chris Nicholson; Adam Gibson. "Deeplearning4j Models".

^ Deeplearning4j. "Deeplearning4j on Spark". Deeplearning4j.

^ a b Intel® Data Analytics Acceleration Library (Intel® DAAL) | Intel® Software

^ Intel® Math Kernel Library (Intel® MKL) | Intel® Software

^ Deep Neural Network Functions

^ Using Intel® MKL with Threaded Applications | Intel® Software

^ a b Intel® Xeon Phi™ Delivers Competitive Performance For Deep Learning—And Getting Better Fast | Intel® Software

^ https://keras.io/applications/

^ https://github.com/keras-team/keras/issues/461

^ Does Keras support using multiple GPUs? · Issue #2436 · fchollet/keras

^ "GPU Coder - MATLAB & Simulink". MathWorks. Retrieved 13 November 2017.

^ a b c "Neural Network Toolbox - MATLAB". MathWorks. Retrieved 13 November 2017.

^ "Deep Learning Models - MATLAB & Simulink". MathWorks. Retrieved 13 November 2017.

^ "Parallel Computing Toolbox - MATLAB". MathWorks. Retrieved 13 November 2017.

^ "CNTK/LICENSE.md at master · Microsoft/CNTK · GitHub". GitHub.

^ "Setup CNTK on your machine". GitHub.

^ "CNTK usage overview". GitHub.

^ "BrainScript Network Builder". GitHub.

^ ".NET Support · Issue #960 · Microsoft/CNTK". GitHub.

^ "How to train a model using multiple machines? · Issue #59 · Microsoft/CNTK". GitHub.

^ https://github.com/Microsoft/CNTK/issues/140#issuecomment-186466820

^ a b "CNTK - Computational Network Toolkit". Microsoft Corporation.

^ url=https://github.com/Microsoft/CNTK/issues/534

^ "Multiple GPUs and machines". Microsoft Corporation.

^ "Disclaimer". CNTK TEAM.

^ "Releases · dmlc/mxnet". Github.

^ "Installation Guide — mxnet documentation". Readthdocs.

^ "MXNet Smart Device". ReadTheDocs.

^ "MXNet.js". Github.

^ "Support for other Device Types, OpenCL AMD GPU · Issue #621 · dmlc/mxnet". GitHub.

^ https://mxnet.readthedocs.io/

^ "Model Gallery". GitHub.

^ "Run MXNet on Multiple CPU/GPUs with Data Parallel". GitHub.

^ https://github.com/hughperkins/pytorch-coriander

^ https://github.com/pytorch/pytorch/issues/488

^ https://github.com/pytorch/pytorch/issues/488#issuecomment-273626736

^ https://developers.googleblog.com/2016/11/tensorflow-0-12-adds-support-for-windows.html

^ interface), JJ Allaire (R; RStudio; Eddelbuettel, Dirk; Golding, Nick; Tang, Yuan; Tutorials), Google Inc (Examples and (2017-05-26), tensorflow: R Interface to TensorFlow, retrieved 2017-06-14

^ "tensorflow/roadmap.md at master · tensorflow/tensorflow · GitHub". GitHub. January 23, 2017. Retrieved May 21, 2017.

^ "OpenCL support · Issue #22 · tensorflow/tensorflow". GitHub.

^ https://www.tensorflow.org/

^ https://github.com/tensorflow/models

^ "Using the GPU — Theano 0.8.2 documentation".

^ http://deeplearning.net/software/theano/library/gradient.html

^ https://groups.google.com/d/msg/theano-users/mln5g2IuBSU/gespG36Lf_QJ

^ "Recipes/modelzoo at master · Lasagne/Recipes · GitHub". GitHub.

^ Using multiple GPUs — Theano 0.8.2 documentation

^ https://github.com/torch/torch7/wiki/Windows

^ "GitHub - soumith/torch-android: Torch-7 for Android". GitHub.

^ "Torch7: A Matlab-like Environment for Machine Learning" (PDF).

^ "GitHub - jonathantompson/jtorch: An OpenCL Torch Utility Library". GitHub.

^ "Cheatsheet". GitHub.

^ "cltorch". GitHub.

^ "Torch CUDA backend". GitHub.

^ "Torch CUDA backend for nn". GitHub.

^ https://github.com/twitter/torch-autograd

^ "ModelZoo". GitHub.

^ https://github.com/torch/torch7/wiki/Cheatsheet#distributed-computing--parallel-processing

^ http://resources.wolframcloud.com/NeuralNetRepository





Deep reinforcement learning (DRL) uses deep learning and reinforcement learning principles in order to create efficient algorithms that can be applied on areas like robotics, video games, finance, healthcare.[1] Implementing deep learning architecture (deep neural networks or etc) with reinforcement learning algorithm (Q-learning, actor critic or etc), a powerful model (DRL) can be created that is capable to scale to problems that were previously unsolvable.[2]
That is because DRL usually uses raw sensor or image signals as input as can be seen in DQN for ATARI games[3], and can receive the benefit of end-to-end reinforcement learning as well as that of convolutional neural network.

References[edit]


^ Francois-Lavet, Vincent; Henderson, Peter; Islam, Riashat; Bellemare, Marc G.; Pineau, Joelle (2018). "An Introduction to Deep Reinforcement Learning". Foundations and Trends in Machine Learning. 11 (3–4): 219–354. doi:10.1561/2200000071. ISSN 1935-8237..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ Arulkumaran, K.; Deisenroth, M. P.; Brundage, M.; Bharath, A. A. (November 2017). "Deep Reinforcement Learning: A Brief Survey". IEEE Signal Processing Magazine. 34 (6): 26–38. doi:10.1109/MSP.2017.2743240. ISSN 1053-5888.

^ Mnih, Volodymyr;  et al. (December 2013). Playing Atari with Deep Reinforcement Learning (PDF). NIPS Deep Learning Workshop 2013.





Machine learning anddata mining
Problems
Classification
Clustering
Regression
Anomaly detection
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction


Supervised learning.mw-parser-output .nobold{font-weight:normal}(classification • regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)


Clustering
BIRCH
CURE
Hierarchical
k-means
Expectation–maximization (EM)
DBSCAN
OPTICS
Mean-shift


Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
t-SNE


Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov


Anomaly detection
k-NN
Local outlier factor


Artificial neural networks
Autoencoder
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net


Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)


Theory
Bias–variance dilemma
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory


Machine-learning venues
NIPS
ICML
ML
JMLR
ArXiv:cs.LG


Glossary of artificial intelligence
Glossary of artificial intelligence


Related articles
List of datasets for machine-learning research
Outline of machine learning


 Machine learning portalvte
Q-learning is a model-free reinforcement learning algorithm. The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. It does not require a model (hence the connotation "model-free") of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations.
For any finite Markov decision process (FMDP), Q-learning finds a policy that is optimal in the sense that it maximizes the expected value of the total reward over any and  all successive steps, starting from the current state.[1] Q-learning can identify an optimal action-selection policy for any given FMDP, given infinite exploration time and a partly-random policy.[1] "Q" names the function that returns the reward used to provide the reinforcement and can be said to stand for the "quality" of an action taken in a given state.[2]

Contents

1 Reinforcement learning
2 Algorithm
3 Influence of variables

3.1 Learning Rate
3.2 Discount factor
3.3 Initial conditions (Q0)


4 Implementation

4.1 Function approximation
4.2 Quantization


5 History
6 Variants

6.1 Deep Q-learning
6.2 Double Q-learning
6.3 Others


7 See also
8 References
9 External links


Reinforcement learning[edit]
Reinforcement learning involves an agent, a set of states 



S


{\displaystyle S}

, and a set 



A


{\displaystyle A}

 of actions per state. By performing an action 



a
∈
A


{\displaystyle a\in A}

, the agent transitions from state to state. Executing an action in a specific state provides the agent with a reward (a numerical score).  
The goal of the agent is to maximize its total (future) reward. It does this by adding the maximum reward attainable from future states to the reward for achieving its current state, effectively influencing the current action by the potential future reward. This potential reward is a weighted sum of the expected values of the rewards of all future steps starting from the current state. 
As an example, consider the process of boarding a train, in which the reward is measured by the negative of the total time spent boarding (alternatively, the cost of boarding the train is equal to the boarding time).  One strategy is to enter the train door as soon as they open, minimizing the initial wait time for yourself. If the train is crowded, however, then you will have a slow entry after the initial action of entering the door as people are fighting you to depart the train as you attempt to board.  The total boarding time, or cost, is then:

0 seconds wait time + 15 seconds fight time
On the next day, by random chance (exploration), you decide to wait and let other people depart first.  This initially results in a longer wait time.  However, time fighting other passengers is less. Overall, this path has a higher reward than that of the previous day, since the total boarding time is now: 

5 second wait time + 0 second fight time.
Through exploration, despite the initial (patient) action resulting in a larger cost (or negative reward) than in the forceful strategy, the overall cost is lower, thus revealing a more rewarding strategy.

Algorithm[edit]
 Q-Learning table of states by actions that is initialized to zero, then each cell is updated through training.
The weight for a step from a state 



Δ
t


{\displaystyle \Delta t}

 steps into the future is calculated as 




γ

Δ
t




{\displaystyle \gamma ^{\Delta t}}

. 



γ


{\displaystyle \gamma }

 (the discount factor) is a number between 0 and 1 (



0
≤
γ
≤
1


{\displaystyle 0\leq \gamma \leq 1}

) and has the effect of valuing rewards received earlier higher than those received later (reflecting the value of a "good start"). 



γ


{\displaystyle \gamma }

 may also be interpreted as the probability to succeed (or survive) at every step 



Δ
t


{\displaystyle \Delta t}

.
The algorithm, therefore, has a function that calculates the quality of a state-action combination:





Q
:
S
×
A
→

R



{\displaystyle Q:S\times A\to \mathbb {R} }

 .
Before learning begins, 



Q


{\displaystyle Q}

 is initialized to a possibly arbitrary fixed value (chosen by the programmer). Then, at each time 



t


{\displaystyle t}

 the agent selects an action 




a

t




{\displaystyle a_{t}}

, observes a reward 




r

t




{\displaystyle r_{t}}

, enters a new state 




s

t
+
1




{\displaystyle s_{t+1}}

 (that may depend on both the previous state 




s

t




{\displaystyle s_{t}}

 and the selected action), and 



Q


{\displaystyle Q}

 is updated. The core of the algorithm is a simple value iteration update, using the weighted average of the old value and the new information:






Q

n
e
w


(

s

t


,

a

t


)
←
(
1
−
α
)
⋅




Q
(

s

t


,

a

t


)

⏟



old value


+



α
⏟



learning rate


⋅






(






r

t


⏟



reward


+



γ
⏟



discount factor


⋅





max

a


Q
(

s

t
+
1


,
a
)

⏟



estimate of optimal future value




)



⏞



learned value




{\displaystyle Q^{new}(s_{t},a_{t})\leftarrow (1-\alpha )\cdot \underbrace {Q(s_{t},a_{t})} _{\text{old value}}+\underbrace {\alpha } _{\text{learning rate}}\cdot \overbrace {{\bigg (}\underbrace {r_{t}} _{\text{reward}}+\underbrace {\gamma } _{\text{discount factor}}\cdot \underbrace {\max _{a}Q(s_{t+1},a)} _{\text{estimate of optimal future value}}{\bigg )}} ^{\text{learned value}}}


where 




r

t




{\displaystyle r_{t}}

 is the reward received when moving from the state 




s

t




{\displaystyle s_{t}}

 to the state 




s

t
+
1




{\displaystyle s_{t+1}}

, and 



α


{\displaystyle \alpha }

 is the learning rate (



0
<
α
≤
1


{\displaystyle 0<\alpha \leq 1}

).
An episode of the algorithm ends when state 




s

t
+
1




{\displaystyle s_{t+1}}

 is a final or terminal state. However, Q-learning can also learn in non-episodic tasks.[citation needed] If the discount factor is lower than 1, the action values are finite even if the problem can contain infinite loops.
For all final states 




s

f




{\displaystyle s_{f}}

, 



Q
(

s

f


,
a
)


{\displaystyle Q(s_{f},a)}

 is never updated, but is set to the reward value 



r


{\displaystyle r}

 observed for state 




s

f




{\displaystyle s_{f}}

. In most cases, 



Q
(

s

f


,
a
)


{\displaystyle Q(s_{f},a)}

 can be taken to equal zero.

Influence of variables[edit]
Learning Rate[edit]
The learning rate or step size determines to what extent newly acquired information overrides old information. A factor of 0 makes the agent learn nothing (exclusively exploiting prior knowledge), while a factor of 1 makes the agent consider only the most recent information (ignoring prior knowledge to explore possibilities). In fully deterministic environments, a learning rate of 




α

t


=
1


{\displaystyle \alpha _{t}=1}

 is optimal. When the problem is stochastic, the algorithm converges under some technical conditions on the learning rate that require it to decrease to zero. In practice, often a constant learning rate is used, such as 




α

t


=
0.1


{\displaystyle \alpha _{t}=0.1}

 for all 



t


{\displaystyle t}

.[3]

Discount factor[edit]
The discount factor 



γ


{\displaystyle \gamma }

 determines the importance of future rewards. A factor of 0 will make the agent "myopic" (or short-sighted) by only considering current rewards, i.e. 




r

t




{\displaystyle r_{t}}

 (in the update rule above), while a factor approaching 1 will make it strive for a long-term high reward. If the discount factor meets or exceeds 1, the action values may diverge. For 



γ
=
1


{\displaystyle \gamma =1}

, without a terminal state, or if the agent never reaches one, all environment histories become infinitely long, and utilities with additive, undiscounted rewards generally become infinite.[4] Even with a discount factor only slightly lower than 1, Q-function learning leads to propagation of errors and instabilities when the value function is approximated with an artificial neural network.[5] In that case, starting with a lower discount factor and increasing it towards its final value accelerates learning.[6]

Initial conditions (Q0)[edit]
Since Q-learning is an iterative algorithm, it implicitly assumes an initial condition before the first update occurs. High initial values, also known as "optimistic initial conditions",[7] can encourage exploration: no matter what action is selected, the update rule will cause it to have lower values than the other alternative, thus increasing their choice probability. The first reward 



r


{\displaystyle r}

 can be used to reset the initial conditions.[8] According to this idea, the first time an action is taken the reward is used to set the value of 



Q


{\displaystyle Q}

. This allows immediate learning in case of fixed deterministic rewards. A model that incorporates reset of initial conditions (RIC) is expected to predict participants' behavior better than a model that assumes any arbitrary initial condition (AIC).[8] RIC seems to be consistent with human behaviour in repeated binary choice experiments.[8]

Implementation[edit]
Q-learning at its simplest stores data in tables. This approach falters with increasing numbers of states/actions.

Function approximation[edit]
Q-learning can be combined with function approximation.[9] This makes it possible to apply the algorithm to larger problems, even when the state space is continuous. 
One solution is to use an (adapted) artificial neural network as a function approximator.[10] Function approximation may speed up learning in finite problems, due to the fact that the algorithm can generalize earlier experiences to previously unseen states.

Quantization[edit]
Another technique to decrease the state/action space quantizes possible values. Consider the example of learning to balance a stick on a finger. To describe a state at a certain point in time involves the position of the finger in space, its velocity, the angle of the stick and the angular velocity of the stick. This yields a four-element vector that describes one state, i.e. a snapshot of one state encoded into four values. The problem is that infinitely many possible states are present. To shrink the possible space of valid actions multiple values can be assigned to a bucket. The exact distance of the finger from its starting position (-Infinity to Infinity) is not known, but rather whether it is far away or not (Near, Far).

History[edit]
Q-learning was introduced by Watkins[11] in 1989. A convergence proof was presented by Watkins and Dayan[12] in 1992. A more detailed mathematical proof was given by Tsitsiklis[13] in 1994, and by Bertsekas and Tsitsiklis in their 1996 Neuro-Dynamic Programming book.[14]
Watkins was addressing “Learning from delayed rewards”, the title of his PhD Thesis. Eight years earlier in 1981 the same problem, under the name of “Delayed reinforcement learning”, was solved by Bozinovski's Crossbar Adaptive Array (CAA).[15][16] The memory matrix W =||w(a,s)|| was the same as the eight years later Q-table of Q-learning. The architecture introduced the term “state evaluation” in reinforcement learning. The crossbar learning algorithm, written in mathematical pseudocode in the paper, in each iteration performs the following computation:

In state s perform action a;
Receive consequence state s’;
Compute state evaluation v(s’);
Update crossbar value w’(a,s) = w(a,s) + v(s’).
The term “secondary reinforcement” is borrowed from animal learning theory, to model state values via backpropagation: the state value v(s’) of the consequence situation is backpropagated to the previously encountered situations. CAA computes state values vertically and actions horizontally (the "crossbar"). Demonstration graphs showing delayed reinforcement learning contained states (desirable, undesirable, and neutral states), which were computed by the state evaluation function. This learning system was a forerunner of the Q-learning algorithm.[17]
In 2014 Google DeepMind patented[18] an application of Q-learning to deep learning, titled "deep reinforcement learning" or "deep Q-learning" that can play Atari 2600 games at expert human levels.

Variants[edit]
Deep Q-learning[edit]
The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q. This instability comes from the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and the data distribution, and the correlations between Q and the target values. 
The technique used experience replay, a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed.[2] This removes correlations in the observation sequence and smooths changes in the data distribution. Iterative update adjusts Q towards target values that are only periodically updated, further reducing correlations with the target.[19]

Double Q-learning[edit]
Because the future maximum approximated action value in Q-learning is evaluated using the same Q function as in current action selection policy, in noisy environments Q-learning can sometimes overestimate the action values, slowing the learning. A variant called Double Q-learning was proposed to correct this. Double Q-learning[20] is an off-policy reinforcement learning algorithm, where a different policy is used for value evaluation than what is used to select the next action.
In practice, two separate value functions are trained in a mutually symmetric fashion using separate experiences, 




Q

A




{\displaystyle Q^{A}}

 and 




Q

B




{\displaystyle Q^{B}}

. The double Q-learning update step is then as follows:






Q

t
+
1


A


(

s

t


,

a

t


)
=

Q

t


A


(

s

t


,

a

t


)
+

α

t


(

s

t


,

a

t


)

(


r

t


+
γ

Q

t


B



(


s

t
+
1


,



a
r
g
 
m
a
x



a


⁡

Q

t


A


(

s

t
+
1


,
a
)

)

−

Q

t


A


(

s

t


,

a

t


)

)



{\displaystyle Q_{t+1}^{A}(s_{t},a_{t})=Q_{t}^{A}(s_{t},a_{t})+\alpha _{t}(s_{t},a_{t})\left(r_{t}+\gamma Q_{t}^{B}\left(s_{t+1},\mathop {\operatorname {arg~max} } _{a}Q_{t}^{A}(s_{t+1},a)\right)-Q_{t}^{A}(s_{t},a_{t})\right)}

, and





Q

t
+
1


B


(

s

t


,

a

t


)
=

Q

t


B


(

s

t


,

a

t


)
+

α

t


(

s

t


,

a

t


)

(


r

t


+
γ

Q

t


A



(


s

t
+
1


,



a
r
g
 
m
a
x



a


⁡

Q

t


B


(

s

t
+
1


,
a
)

)

−

Q

t


B


(

s

t


,

a

t


)

)

.


{\displaystyle Q_{t+1}^{B}(s_{t},a_{t})=Q_{t}^{B}(s_{t},a_{t})+\alpha _{t}(s_{t},a_{t})\left(r_{t}+\gamma Q_{t}^{A}\left(s_{t+1},\mathop {\operatorname {arg~max} } _{a}Q_{t}^{B}(s_{t+1},a)\right)-Q_{t}^{B}(s_{t},a_{t})\right).}


Now the estimated value of the discounted future is evaluated using a different policy, which solves the overestimation issue.
This algorithm was later combined with deep learning, as in the DQN algorithm, resulting in Double DQN, which outperforms the original DQN algorithm.[21]

Others[edit]
Delayed Q-learning is an alternative implementation of the online Q-learning algorithm, with probably approximately correct (PAC) learning.[22]
Greedy GQ is a variant of Q-learning to use in combination with (linear) function approximation.[23] The advantage of Greedy GQ is that convergence is guaranteed even when function approximation is used to estimate the action values.

See also[edit]
Reinforcement learning
Temporal difference learning
SARSA
Iterated prisoner's dilemma
Game theory
References[edit]


^ a b Melo, Francisco S. "Convergence of Q-learning: a simple proof" (PDF)..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ a b Matiisen, Tambet (December 19, 2015). "Demystifying Deep Reinforcement Learning". neuro.cs.ut.ee. Computational Neuroscience Lab. Retrieved 2018-04-06.

^ Sutton, Richard; Barto, Andrew (1998). Reinforcement Learning: An Introduction. MIT Press.

^ Russell, Stuart J.; Norvig, Peter (2010). Artificial Intelligence: A Modern Approach (Third ed.). Prentice Hall. p. 649. ISBN 978-0136042594.

^ Baird, Leemon (1995). "Residual algorithms: Reinforcement learning with function approximation" (PDF). ICML: 30–37.

^ François-Lavet, Vincent; Fonteneau, Raphael; Ernst, Damien (2015-12-07). "How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies". arXiv:1512.02011 [cs.LG].

^ Sutton, Richard S.; Barto, Andrew G. "2.7 Optimistic Initial Values". Reinforcement Learning: An Introduction. Archived from the original on 2013-09-08. Retrieved 2013-07-18.

^ a b c Shteingart, Hanan; Neiman, Tal; Loewenstein, Yonatan (May 2013). "The role of first impression in operant learning" (PDF). Journal of Experimental Psychology: General. 142 (2): 476–488. doi:10.1037/a0029550. ISSN 1939-2222. PMID 22924882.

^ Hasselt, Hado van (5 March 2012). "Reinforcement Learning in Continuous State and Action Spaces".  In Wiering, Marco; Otterlo, Martijn van (eds.). Reinforcement Learning: State-of-the-Art. Springer Science & Business Media. pp. 207–251. ISBN 978-3-642-27645-3.

^ Tesauro, Gerald (March 1995). "Temporal Difference Learning and TD-Gammon". Communications of the ACM. 38 (3): 58–68. doi:10.1145/203330.203343. Retrieved 2010-02-08.

^ Watkins, C.J.C.H. (1989), Learning from Delayed Rewards (PDF) (Ph.D. thesis), Cambridge University

^ Watkins and Dayan, C.J.C.H., (1992), 'Q-learning.Machine Learning'

^  Tsitsiklis, J., (1994), 'Asynchronous Stochastic Approximation and Q-learning. Machine Learning'

^ Bertsekas and Tsitsiklis, (1996), 'Neuro-Dynamic Programming. Athena Scientific'

^ Bozinovski, S. (15 July 1999). "Crossbar Adaptive Array: The first connectionist network that solved the delayed reinforcement learning problem".  In Dobnikar, Andrej; Steele, Nigel C.; Pearson, David W.; Albrecht, Rudolf F. (eds.). Artificial Neural Nets and Genetic Algorithms: Proceedings of the International Conference in Portorož, Slovenia, 1999. Springer Science & Business Media. pp. 320–325. ISBN 978-3-211-83364-3.

^ Bozinovski, S. (1982). "A self learning system using secondary reinforcement".  In Trappl, Robert (ed.). Cybernetics and Systems Research: Proceedings of the Sixth European Meeting on Cybernetics and Systems Research. North Holland. pp. 397–402. ISBN 978-0-444-86488-8.

^ Barto, A. (24 February 1997). "Reinforcement learning".  In Omidvar, Omid; Elliott, David L. (eds.). Neural Systems for Control. Elsevier. ISBN 978-0-08-053739-9.

^ "Methods and Apparatus for Reinforcement Learning, US Patent #20150100530A1" (PDF). US Patent Office. 9 April 2015. Retrieved 28 July 2018.

^ Mnih, Volodymyr; Kavukcuoglu, Koray; Silver, David; Rusu, Andrei A.; Veness, Joel; Bellemare, Marc G.; Graves, Alex; Riedmiller, Martin; Fidjeland, Andreas K. (Feb 2015). "Human-level control through deep reinforcement learning". Nature. 518 (7540): 529–533. doi:10.1038/nature14236. ISSN 0028-0836. PMID 25719670.

^ van Hasselt, Hado (2011). "Double Q-learning" (PDF). Advances in Neural Information Processing Systems. 23: 2613–2622.

^ van Hasselt, Hado; Guez, Arthur; Silver, David (2015). "Deep reinforcement learning with double Q-learning" (PDF). AAAI Conference on Artificial Intelligence: 2094–2100.

^ Strehl, Alexander L.; Li, Lihong; Wiewiora, Eric; Langford, John; Littman, Michael L. (2006). "Pac model-free reinforcement learning" (PDF). Proc. 22nd ICML: 881–888.

^ Maei, Hamid; Szepesvári, Csaba; Bhatnagar, Shalabh; Sutton, Richard (2010). "Toward off-policy learning control with function approximation in Proceedings of the 27th International Conference on Machine Learning" (PDF). pp. 719–726.


External links[edit]
Watkins, C.J.C.H. (1989). Learning from Delayed Rewards. PhD thesis, Cambridge University, Cambridge, England.
Strehl, Li, Wiewiora, Langford, Littman (2006). PAC model-free reinforcement learning
Reinforcement Learning: An Introduction by Richard Sutton and Andrew S. Barto, an online textbook. See "6.5 Q-Learning: Off-Policy TD Control".
Piqle: a Generic Java Platform for Reinforcement Learning
Reinforcement Learning Maze, a demonstration of guiding an ant through a maze using Q-learning.
Q-learning work by Gerald Tesauro
JavaScript Example with Reward Driven RNN learning
A Brain Library
A Genetics Library used by the Brain



It has been suggested that Neural network be merged into this article. (Discuss) Proposed since May 2019.
Machine learning anddata mining
Problems
Classification
Clustering
Regression
Anomaly detection
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction


Supervised learning.mw-parser-output .nobold{font-weight:normal}(classification • regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)


Clustering
BIRCH
CURE
Hierarchical
k-means
Expectation–maximization (EM)
DBSCAN
OPTICS
Mean-shift


Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
t-SNE


Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov


Anomaly detection
k-NN
Local outlier factor


Artificial neural networks
Autoencoder
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net


Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)


Theory
Bias–variance dilemma
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory


Machine-learning venues
NIPS
ICML
ML
JMLR
ArXiv:cs.LG


Glossary of artificial intelligence
Glossary of artificial intelligence


Related articles
List of datasets for machine-learning research
Outline of machine learning


 Machine learning portalvte


 An artificial neural network is an interconnected group of nodes, inspired by a simplification of neurons in a brain. Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another.
Artificial neural networks (ANN) or connectionist systems are computing systems that are inspired by, but not necessarily identical to, the biological neural networks that constitute animal brains. Such systems "learn" to perform tasks by considering examples, generally without being programmed with any task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as "cat" or "no cat" and using the results to identify cats in other images. They do this without any prior knowledge about cats, for example, that they have fur, tails, whiskers and cat-like faces. Instead, they automatically generate identifying characteristics from the learning material that they process.
An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it.
In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called 'edges'. Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.
The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.

.mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}Contents

1 History

1.1 Hebbian learning
1.2 Backpropagation
1.3 Hardware-based designs
1.4 Contests
1.5 Convolutional neural networks


2 Models

2.1 Components of an artificial neural network

2.1.1 Neurons
2.1.2 Connections, weights and biases
2.1.3 Propagation function
2.1.4 Learning rule


2.2 Neural networks as functions
2.3 Learning

2.3.1 Choosing a cost function
2.3.2 Backpropagation


2.4 Learning paradigms

2.4.1 Supervised learning
2.4.2 Unsupervised learning
2.4.3 Reinforcement learning


2.5 Learning algorithms

2.5.1 Convergent recursive learning algorithm




3 Optimization

3.1 Algorithm


4 Algorithm in code

4.1 Phase 1: propagation
4.2 Phase 2: weight update
4.3 Pseudocode


5 Extension

5.1 Adaptive learning rate
5.2 Inertia


6 Modes of learning
7 Variants

7.1 Group method of data handling
7.2 Convolutional neural networks
7.3 Long short-term memory
7.4 Deep reservoir computing
7.5 Deep belief networks
7.6 Large memory storage and retrieval neural networks
7.7 Stacked (de-noising) auto-encoders
7.8 Deep stacking networks
7.9 Tensor deep stacking networks
7.10 Spike-and-slab RBMs
7.11 Compound hierarchical-deep models
7.12 Deep predictive coding networks
7.13 Networks with separate memory structures

7.13.1 LSTM-related differentiable memory structures
7.13.2 Neural Turing machines
7.13.3 Semantic hashing
7.13.4 Memory networks
7.13.5 Pointer networks
7.13.6 Encoder–decoder networks


7.14 Multilayer kernel machine


8 Neural architecture search
9 Use
10 Applications

10.1 Types of models


11 Theoretical properties

11.1 Computational power
11.2 Capacity
11.3 Convergence
11.4 Generalization and statistics


12 Criticism

12.1 Training issues
12.2 Theoretical issues
12.3 Hardware issues
12.4 Practical counterexamples to criticisms
12.5 Hybrid approaches


13 Types
14 Gallery
15 See also
16 References
17 Bibliography



History[edit]
Warren McCulloch and Walter Pitts[1] (1943) created a computational model for neural networks based on mathematics and algorithms called threshold logic. This model paved the way for neural network research to split into two approaches. One approach focused on biological processes in the brain while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.[2]

Hebbian learning[edit]
In the late 1940s, D. O. Hebb[3] created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines. Farley and Clark[4] (1954) first used computational machines, then called "calculators", to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956).[5] Rosenblatt[6] (1958) created the perceptron, an algorithm for pattern recognition. With mathematical notation, Rosenblatt described circuitry not in the basic perceptron, such as the exclusive-or circuit that could not be processed by neural networks at the time.[7] In 1959, a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells.[8] The first functional networks with many layers were published by Ivakhnenko and Lapa in 1965, becoming the Group Method of Data Handling.[9][10][11]
Neural network research stagnated after machine learning research by Minsky and Papert (1969),[12] who discovered two key issues with the computational machines that processed neural networks. The first was that basic perceptrons were incapable of processing the exclusive-or circuit. The second was that computers didn't have enough processing power to effectively handle the work required by large neural networks. Neural network research slowed until computers achieved far greater processing power. Much of artificial intelligence had focused on high-level (symbolic) models that are processed by using algorithms, characterized for example by expert systems with knowledge embodied in if-then rules, until in the late 1980s research expanded to low-level (sub-symbolic) machine learning, characterized by knowledge embodied in the parameters of a cognitive model.[citation needed]

Backpropagation[edit]
A key trigger for renewed interest in neural networks and learning was Werbos's (1975) backpropagation algorithm that made the training of multi-layer networks feasible and efficient. Backpropagation distributed the error term back up through the layers, by modifying the weights at each node.[7]
In the mid-1980s, parallel distributed processing became popular under the name connectionism. Rumelhart and McClelland (1986) described the use of connectionism to simulate neural processes.[13]
Support vector machines and other, much simpler methods such as linear classifiers gradually overtook neural networks in machine learning popularity. However, using neural networks transformed some domains, such as the prediction of protein structures.[14][15]
In 1992, max-pooling was introduced to help with least shift invariance and tolerance to deformation to aid in 3D object recognition.[16][17][18]
In 2010, Backpropagation training through max-pooling was accelerated by GPUs and shown to perform better than other pooling variants.[19]
The vanishing gradient problem affects many-layered feedforward networks that used backpropagation and also recurrent neural networks (RNNs).[20][21] As errors propagate from layer to layer, they shrink exponentially with the number of layers, impeding the tuning of neuron weights that is based on those errors, particularly affecting deep networks.
To overcome this problem, Schmidhuber adopted a multi-level hierarchy of networks (1992) pre-trained one level at a time by unsupervised learning and fine-tuned by backpropagation.[22] Behnke (2003) relied only on the sign of the gradient (Rprop)[23] on problems such as image reconstruction and face localization.
Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine[24] to model each layer. Once sufficiently many layers have been learned, the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an "ancestral pass") from the top level feature activations.[25][26] In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.[27]
Earlier challenges in training deep neural networks were successfully addressed with methods such as unsupervised pre-training, while available computing power increased through the use of GPUs and distributed computing. Neural networks were deployed on a large scale, particularly in image and visual recognition problems. This became known as "deep learning".[citation needed]

Hardware-based designs[edit]
Computational devices were created in CMOS, for both biophysical simulation and neuromorphic computing. Nanodevices[28] for very large scale principal components analyses and convolution may create a new class of neural computing because they are fundamentally analog rather than digital (even though the first implementations may use digital devices).[29] Ciresan and colleagues (2010)[30] in Schmidhuber's group showed that despite the vanishing gradient problem, GPUs make back-propagation feasible for many-layered feedforward neural networks.

Contests[edit]
Between 2009 and 2012, recurrent neural networks and deep feedforward neural networks developed in Schmidhuber's research group won eight international competitions in pattern recognition and machine learning.[31][32] For example, the bi-directional and multi-dimensional long short-term memory (LSTM)[33][34][35][36] of Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three languages to be learned.[35][34]
Ciresan and colleagues won pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition,[37] the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge[38] and others. Their neural networks were the first pattern recognizers to achieve human-competitive or even superhuman performance[39] on benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem.
Researchers demonstrated (2010) that deep neural networks interfaced to a hidden Markov model with context-dependent states that define the neural network output layer can drastically reduce errors in large-vocabulary speech recognition tasks such as voice search.
GPU-based implementations[40] of this approach won many pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition,[37] the ISBI 2012 Segmentation of neuronal structures in EM stacks challenge,[38] the ImageNet Competition[41] and others.
Deep, highly nonlinear neural architectures similar to the neocognitron[42] and the "standard architecture of vision",[43] inspired by simple and complex cells, were pre-trained by unsupervised methods by Hinton.[44][25] A team from his lab won a 2012 contest sponsored by Merck to design software to help find molecules that might identify new drugs.[45]

Convolutional neural networks[edit]
As of 2011[update], the state of the art in deep learning feedforward networks alternated between convolutional layers and max-pooling layers,[40][46] topped by several fully or sparsely connected layers followed by a final classification layer. Learning is usually done without unsupervised pre-training. In the convolutional layer, there are filters that are convolved with the input. Each filter is equivalent to a weights vector that has to be trained.
Such supervised deep learning methods were the first to achieve human-competitive performance on certain practical applications.[39]
Artificial neural networks were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes, only when invariance extended beyond shift, to all ANN-learned concepts, such as location, type (object class label), scale, lighting and others. This was realized in Developmental Networks (DNs)[47] whose embodiments are Where-What Networks, WWN-1 (2008)[48] through WWN-7 (2013).[49]

Models[edit]
This section may be confusing or unclear to readers. Please help us clarify the section. There might be a discussion about this on the talk page. (April 2017) (Learn how and when to remove this template message)
 Neuron and myelinated axon, with signal flow from inputs at dendrites to outputs at axon terminals
An artificial neural network is a network of simple elements called artificial neurons, which receive input, change their internal state (activation) according to that input, and produce output depending on the input and activation.
An artificial neuron mimics the working of a biophysical neuron with inputs and outputs, but is not a biological neuron model.
The network forms by connecting the output of certain neurons to the input of other neurons forming a directed, weighted graph. The weights as well as the functions that compute the activation can be modified by a process called learning which is governed by a learning rule.[50]

Components of an artificial neural network[edit]
Neurons[edit]
A neuron with label 



j


{\displaystyle j}

 receiving an input 




p

j


(
t
)


{\displaystyle p_{j}(t)}

 from predecessor neurons consists of the following components:[50]

an activation 




a

j


(
t
)


{\displaystyle a_{j}(t)}

, the neuron's state, depending on a discrete time parameter,
possibly a threshold 




θ

j




{\displaystyle \theta _{j}}

, which stays fixed unless changed by a learning function,
an activation function 



f


{\displaystyle f}

 that computes the new activation at a given time 



t
+
1


{\displaystyle t+1}

 from 




a

j


(
t
)


{\displaystyle a_{j}(t)}

, 




θ

j




{\displaystyle \theta _{j}}

 and the net input 




p

j


(
t
)


{\displaystyle p_{j}(t)}

 giving rise to the relation





a

j


(
t
+
1
)
=
f
(

a

j


(
t
)
,

p

j


(
t
)
,

θ

j


)


{\displaystyle a_{j}(t+1)=f(a_{j}(t),p_{j}(t),\theta _{j})}

,
and an output function 




f

o
u
t




{\displaystyle f_{out}}

 computing the output from the activation





o

j


(
t
)
=

f

o
u
t


(

a

j


(
t
)
)


{\displaystyle o_{j}(t)=f_{out}(a_{j}(t))}

.
Often the output function is simply the Identity function.
An input neuron has no predecessor but serves as input interface for the whole network. Similarly an output neuron has no successor and thus serves as output interface of the whole network.

Connections, weights and biases[edit]
The network consists of connections, each connection transferring the output of a neuron 



i


{\displaystyle i}

 to the input of a neuron 



j


{\displaystyle j}

. In this sense 



i


{\displaystyle i}

 is the predecessor of 



j


{\displaystyle j}

 and 



j


{\displaystyle j}

 is the successor of 



i


{\displaystyle i}

. Each connection is assigned a weight 




w

i
j




{\displaystyle w_{ij}}

.[50] Sometimes a bias term is added to the total weighted sum of inputs to serve as a threshold to shift the activation function.[51]

Propagation function[edit]
The propagation function computes the input 




p

j


(
t
)


{\displaystyle p_{j}(t)}

 to the neuron 



j


{\displaystyle j}

 from the outputs 




o

i


(
t
)


{\displaystyle o_{i}(t)}

 of predecessor neurons and typically has the form[50]






p

j


(
t
)
=

∑

i



o

i


(
t
)

w

i
j




{\displaystyle p_{j}(t)=\sum _{i}o_{i}(t)w_{ij}}

.
When a bias value is added with the function, the above form changes to the following:[52]






p

j


(
t
)
=

∑

i



o

i


(
t
)

w

i
j


+

w

0
j




{\displaystyle p_{j}(t)=\sum _{i}o_{i}(t)w_{ij}+w_{0j}}

 , where 




w

0
j




{\displaystyle w_{0j}}

 is a bias.
Learning rule[edit]
The learning rule is a rule or an algorithm which modifies the parameters of the neural network, in order for a given input to the network to produce a favored output. This learning process typically amounts to modifying the weights and thresholds of the variables within the network.[50]

Neural networks as functions[edit]
See also: Graphical models
Neural network models can be viewed as simple mathematical models defining a function 




f
:
X
→
Y



{\displaystyle \textstyle f:X\rightarrow Y}

 or a distribution over 




X



{\displaystyle \textstyle X}

 or both 




X



{\displaystyle \textstyle X}

 and 




Y



{\displaystyle \textstyle Y}

. Sometimes models are intimately associated with a particular learning rule. A common use of the phrase "ANN model" is really the definition of a class of such functions (where members of the class are obtained by varying parameters, connection weights, or specifics of the architecture such as the number of neurons or their connectivity).
Mathematically, a neuron's network function 




f
(
x
)



{\displaystyle \textstyle f(x)}

 is defined as a composition of other functions 





g

i


(
x
)



{\displaystyle \textstyle g_{i}(x)}

, that can further be decomposed into other functions. This can be conveniently represented as a network structure, with arrows depicting the dependencies between functions. A widely used type of composition is the nonlinear weighted sum, where 




f
(
x
)
=
K

(


∑

i



w

i



g

i


(
x
)

)




{\displaystyle \textstyle f(x)=K\left(\sum _{i}w_{i}g_{i}(x)\right)}

, where 




K



{\displaystyle \textstyle K}

 (commonly referred to as the activation function[53]) is some predefined function, such as the hyperbolic tangent or sigmoid function or softmax function or rectifier function. The important characteristic of the activation function is that it provides a smooth transition as input values change, i.e. a small change in input produces a small change in output. The following refers to a collection of functions 





g

i





{\displaystyle \textstyle g_{i}}

 as a vector 




g
=
(

g

1


,

g

2


,
…
,

g

n


)



{\displaystyle \textstyle g=(g_{1},g_{2},\ldots ,g_{n})}

.

 ANN dependency graph
This figure depicts such a decomposition of 




f



{\displaystyle \textstyle f}

, with dependencies between variables indicated by arrows. These can be interpreted in two ways.
The first view is the functional view: the input 




x



{\displaystyle \textstyle x}

 is transformed into a 3-dimensional vector 




h



{\displaystyle \textstyle h}

, which is then transformed into a 2-dimensional vector 




g



{\displaystyle \textstyle g}

, which is finally transformed into 




f



{\displaystyle \textstyle f}

. This view is most commonly encountered in the context of optimization.
The second view is the probabilistic view: the random variable 




F
=
f
(
G
)



{\displaystyle \textstyle F=f(G)}

 depends upon the random variable 




G
=
g
(
H
)



{\displaystyle \textstyle G=g(H)}

, which depends upon 




H
=
h
(
X
)



{\displaystyle \textstyle H=h(X)}

, which depends upon the random variable 




X



{\displaystyle \textstyle X}

. This view is most commonly encountered in the context of graphical models.
The two views are largely equivalent. In either case, for this particular architecture, the components of individual layers are independent of each other (e.g., the components of 




g



{\displaystyle \textstyle g}

 are independent of each other given their input 




h



{\displaystyle \textstyle h}

). This naturally enables a degree of parallelism in the implementation.

 Two separate depictions of the recurrent ANN dependency graph
Networks such as the previous one are commonly called feedforward, because their graph is a directed acyclic graph. Networks with cycles are commonly called recurrent. Such networks are commonly depicted in the manner shown at the top of the figure, where 




f



{\displaystyle \textstyle f}

 is shown as being dependent upon itself. However, an implied temporal dependence is not shown.

Learning[edit]
See also: Mathematical optimization, Estimation theory, and Machine learning
The possibility of learning has attracted the most interest in neural networks. Given a specific task to solve, and a class of functions 




F



{\displaystyle \textstyle F}

, learning means using a set of observations to find 





f

∗


∈
F



{\displaystyle \textstyle f^{*}\in F}

 which solves the task in some optimal sense.
This entails defining a cost function 




C
:
F
→

R




{\displaystyle \textstyle C:F\rightarrow \mathbb {R} }

 such that, for the optimal solution 





f

∗





{\displaystyle \textstyle f^{*}}

, 




C
(

f

∗


)
≤
C
(
f
)



{\displaystyle \textstyle C(f^{*})\leq C(f)}

 




∀
f
∈
F



{\displaystyle \textstyle \forall f\in F}

 –  i.e., no solution has a cost less than the cost of the optimal solution (see mathematical optimization).
The cost function 




C



{\displaystyle \textstyle C}

 is an important concept in learning, as it is a measure of how far away a particular solution is from an optimal solution to the problem to be solved. Learning algorithms search through the solution space to find a function that has the smallest possible cost.
For applications where the solution is data dependent, the cost must necessarily be a function of the observations, otherwise the model would not relate to the data. It is frequently defined as a statistic to which only approximations can be made. As a simple example, consider the problem of finding the model 




f



{\displaystyle \textstyle f}

, which minimizes 




C
=
E

[

(
f
(
x
)
−
y

)

2



]




{\displaystyle \textstyle C=E\left[(f(x)-y)^{2}\right]}

, for data pairs 




(
x
,
y
)



{\displaystyle \textstyle (x,y)}

 drawn from some distribution 






D





{\displaystyle \textstyle {\mathcal {D}}}

. In practical situations we would only have 




N



{\displaystyle \textstyle N}

 samples from 






D





{\displaystyle \textstyle {\mathcal {D}}}

 and thus, for the above example, we would only minimize 







C
^



=


1
N



∑

i
=
1


N


(
f
(

x

i


)
−

y

i



)

2





{\displaystyle \textstyle {\hat {C}}={\frac {1}{N}}\sum _{i=1}^{N}(f(x_{i})-y_{i})^{2}}

. Thus, the cost is minimized over a sample of the data rather than the entire distribution.
When 




N
→
∞



{\displaystyle \textstyle N\rightarrow \infty }

 some form of online machine learning must be used, where the cost is reduced as each new example is seen. While online machine learning is often used when 






D





{\displaystyle \textstyle {\mathcal {D}}}

 is fixed, it is most useful in the case where the distribution changes slowly over time. In neural network methods, some form of online machine learning is frequently used for finite datasets.

Choosing a cost function[edit]
While it is possible to define an ad hoc cost function, frequently a particular cost function is used, either because it has desirable properties (such as convexity) or because it arises naturally from a particular formulation of the problem (e.g., in a probabilistic formulation the posterior probability of the model can be used as an inverse cost). Ultimately, the cost function depends on the task.

Backpropagation[edit]
Main article: Backpropagation
A DNN can be discriminatively trained with the standard backpropagation algorithm. Backpropagation is a method to calculate the gradient of the loss function (produces the cost associated with a given state) with respect to the weights in an ANN.
The basics of continuous backpropagation[9][54][55][56] were derived in the context of control theory by Kelley[57] in 1960 and by Bryson in 1961,[58] using principles of dynamic programming. In 1962, Dreyfus published a simpler derivation based only on the chain rule.[59] Bryson and Ho described it as a multi-stage dynamic system optimization method in 1969.[60][61] In 1970, Linnainmaa finally published the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions.[62][63] This corresponds to the modern version of backpropagation which is efficient even when the networks are sparse.[9][54][64][65] In 1973, Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients.[66] In 1974, Werbos mentioned the possibility of applying this principle to Artificial neural networks,[67] and in 1982, he applied Linnainmaa's AD method to neural networks in the way that is widely used today.[54][68] In 1986, Rumelhart, Hinton and Williams noted that this method can generate useful internal representations of incoming data in hidden layers of neural networks.[69] In 1993, Wan was the first[9] to win an international pattern recognition contest through backpropagation.[70]
The weight updates of backpropagation can be done via stochastic gradient descent using the following equation:






w

i
j


(
t
+
1
)
=

w

i
j


(
t
)
−
η



∂
C


∂

w

i
j





+
ξ
(
t
)


{\displaystyle w_{ij}(t+1)=w_{ij}(t)-\eta {\frac {\partial C}{\partial w_{ij}}}+\xi (t)}


where, 



η


{\displaystyle \eta }

 is the learning rate, 



C


{\displaystyle C}

 is the cost (loss) function and 



ξ
(
t
)


{\displaystyle \xi (t)}

 a stochastic term. The choice of the cost function depends on factors such as the learning type (supervised, unsupervised, reinforcement, etc.) and the activation function. For example, when performing supervised learning on a multiclass classification problem, common choices for the activation function and cost function are the softmax function and cross entropy function, respectively. The softmax function is defined as 




p

j


=



exp
⁡
(

x

j


)



∑

k


exp
⁡
(

x

k


)





{\displaystyle p_{j}={\frac {\exp(x_{j})}{\sum _{k}\exp(x_{k})}}}

 where 




p

j




{\displaystyle p_{j}}

 represents the class probability (output of the unit 



j


{\displaystyle j}

) and 




x

j




{\displaystyle x_{j}}

 and 




x

k




{\displaystyle x_{k}}

 represent the total input to units 



j


{\displaystyle j}

 and 



k


{\displaystyle k}

 of the same level respectively. Cross entropy is defined as 



C
=
−

∑

j



d

j


log
⁡
(

p

j


)


{\displaystyle C=-\sum _{j}d_{j}\log(p_{j})}

 where 




d

j




{\displaystyle d_{j}}

 represents the target probability for output unit 



j


{\displaystyle j}

 and 




p

j




{\displaystyle p_{j}}

 is the probability output for 



j


{\displaystyle j}

 after applying the activation function.[71]
These can be used to output object bounding boxes in the form of a binary mask. They are also used for multi-scale regression to increase localization precision. DNN-based regression can learn features that capture geometric information in addition to serving as a good classifier. They remove the requirement to explicitly model parts and their relations. This helps to broaden the variety of objects that can be learned. The model consists of multiple layers, each of which has a rectified linear unit as its activation function for non-linear transformation. Some layers are convolutional, while others are fully connected. Every convolutional layer has an additional max pooling. The network is trained to minimize L2 error for predicting the mask ranging over the entire training set containing bounding boxes represented as masks.
Alternatives to backpropagation include Extreme Learning Machines,[72] "No-prop" networks,[73] training without backtracking,[74] "weightless" networks,[75][76] and non-connectionist neural networks.

Learning paradigms[edit]
The three major learning paradigms each correspond to a particular learning task. These are supervised learning, unsupervised learning and reinforcement learning.

Supervised learning[edit]
Supervised learning uses a set of example pairs 



(
x
,
y
)
,
x
∈
X
,
y
∈
Y


{\displaystyle (x,y),x\in X,y\in Y}

 and the aim is to find a function 



f
:
X
→
Y


{\displaystyle f:X\rightarrow Y}

 in the allowed class of functions that matches the examples. In other words, we wish to infer the mapping implied by the data; the cost function is related to the mismatch between our mapping and the data and it implicitly contains prior knowledge about the problem domain.[77]
A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output, 



f
(
x
)


{\displaystyle f(x)}

, and the target value 



y


{\displaystyle y}

 over all the example pairs. Minimizing this cost using gradient descent for the class of neural networks called multilayer perceptrons (MLP), produces the backpropagation algorithm for training neural networks.
Tasks that fall within the paradigm of supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). The supervised learning paradigm is also applicable to sequential data (e.g., for hand writing, speech and gesture recognition). This can be thought of as learning with a "teacher", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.

Unsupervised learning[edit]
In unsupervised learning, some data 




x



{\displaystyle \textstyle x}

 is given and the cost function to be minimized, that can be any function of the data 




x



{\displaystyle \textstyle x}

 and the network's output, 




f



{\displaystyle \textstyle f}

.
The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables).
As a trivial example, consider the model 




f
(
x
)
=
a



{\displaystyle \textstyle f(x)=a}

 where 




a



{\displaystyle \textstyle a}

 is a constant and the cost 




C
=
E
[
(
x
−
f
(
x
)

)

2


]



{\displaystyle \textstyle C=E[(x-f(x))^{2}]}

. Minimizing this cost produces a value of 




a



{\displaystyle \textstyle a}

 that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between 




x



{\displaystyle \textstyle x}

 and 




f
(
x
)



{\displaystyle \textstyle f(x)}

, whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples those quantities would be maximized rather than minimized).
Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.

Reinforcement learning[edit]
See also: Stochastic control
In reinforcement learning, data 




x



{\displaystyle \textstyle x}

 are usually not given, but generated by an agent's interactions with the environment. At each point in time 




t



{\displaystyle \textstyle t}

, the agent performs an action 





y

t





{\displaystyle \textstyle y_{t}}

 and the environment generates an observation 





x

t





{\displaystyle \textstyle x_{t}}

 and an instantaneous cost 





c

t





{\displaystyle \textstyle c_{t}}

, according to some (usually unknown) dynamics. The aim is to discover a policy for selecting actions that minimizes some measure of a long-term cost, e.g., the expected cumulative cost. The environment's dynamics and the long-term cost for each policy are usually unknown, but can be estimated.
More formally the environment is modeled as a Markov decision process (MDP) with states 






s

1


,
.
.
.
,

s

n



∈
S



{\displaystyle \textstyle {s_{1},...,s_{n}}\in S}

 and actions 






a

1


,
.
.
.
,

a

m



∈
A



{\displaystyle \textstyle {a_{1},...,a_{m}}\in A}

 with the following probability distributions: the instantaneous cost distribution 




P
(

c

t



|


s

t


)



{\displaystyle \textstyle P(c_{t}|s_{t})}

, the observation distribution 




P
(

x

t



|


s

t


)



{\displaystyle \textstyle P(x_{t}|s_{t})}

 and the transition 




P
(

s

t
+
1



|


s

t


,

a

t


)



{\displaystyle \textstyle P(s_{t+1}|s_{t},a_{t})}

, while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two then define a Markov chain (MC). The aim is to discover the policy (i.e., the MC) that minimizes the cost.
Artificial neural networks are frequently used in reinforcement learning as part of the overall algorithm.[78][79] Dynamic programming was coupled with Artificial neural networks (giving neurodynamic programming) by Bertsekas and Tsitsiklis[80] and applied to multi-dimensional nonlinear problems such as those involved in vehicle routing,[81] natural resources management[82][83] or medicine[84] because of the ability of Artificial neural networks to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of the original control problems.
Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.

Learning algorithms[edit]
See also: Machine learning
Training a neural network model essentially means selecting one model from the set of allowed models (or, in a Bayesian framework, determining a distribution over the set of allowed models) that minimizes the cost. Numerous algorithms are available for training neural network models; most of them can be viewed as a straightforward application of optimization theory and statistical estimation.
Most employ some form of gradient descent, using backpropagation to compute the actual gradients. This is done by simply taking the derivative of the cost function with respect to the network parameters and then changing those parameters in a gradient-related direction. Backpropagation training algorithms fall into three categories:

steepest descent (with variable learning rate and momentum, resilient backpropagation);
quasi-Newton (Broyden-Fletcher-Goldfarb-Shanno, one step secant);
Levenberg-Marquardt and conjugate gradient (Fletcher-Reeves update, Polak-Ribiére update, Powell-Beale restart, scaled conjugate gradient).[85]
Evolutionary methods,[86] gene expression programming,[87] simulated annealing,[88] expectation-maximization, non-parametric methods and particle swarm optimization[89] are other methods for training neural networks.

Convergent recursive learning algorithm[edit]
This is a learning method specially designed for cerebellar model articulation controller (CMAC) neural networks. In 2004, a recursive least squares algorithm was introduced to train CMAC neural network online.[90] This algorithm can converge in one step and update all weights in one step with any new input data. Initially, this algorithm had computational complexity of O(N3). Based on QR decomposition, this recursive learning algorithm was simplified to be O(N).[91]

Optimization[edit]
The optimization algorithm repeats a two phase cycle, propagation and weight update. When an input vector is presented to the network, it is propagated forward through the network, layer by layer, until it reaches the output layer. The output of the network is then compared to the desired output, using a loss function. The resulting error value is calculated for each of the neurons in the output layer. The error values are then propagated from the output back through the network, until each neuron has an associated error value that reflects its contribution to the original output.
Backpropagation uses these error values to calculate the gradient of the loss function. In the second phase, this gradient is fed to the optimization method, which in turn uses it to update the weights, in an attempt to minimize the loss function.

Algorithm[edit]
Let 



N


{\displaystyle N}

 be a neural network with 



e


{\displaystyle e}

 connections, 



m


{\displaystyle m}

 inputs, and 



n


{\displaystyle n}

 outputs.
Below, 




x

1


,

x

2


,
…


{\displaystyle x_{1},x_{2},\dots }

 will denote vectors in 





R


m




{\displaystyle \mathbb {R} ^{m}}

, 




y

1


,

y

2


,
…


{\displaystyle y_{1},y_{2},\dots }

 vectors in 





R


n




{\displaystyle \mathbb {R} ^{n}}

, and 




w

0


,

w

1


,

w

2


,
…


{\displaystyle w_{0},w_{1},w_{2},\ldots }

 vectors in 





R


e




{\displaystyle \mathbb {R} ^{e}}

. 
These are called inputs, outputs and weights respectively.
The neural network corresponds to a function 



y
=

f

N


(
w
,
x
)


{\displaystyle y=f_{N}(w,x)}

 which, given a weight 



w


{\displaystyle w}

, maps an input 



x


{\displaystyle x}

 to an output 



y


{\displaystyle y}

.
The optimization takes as input a sequence of training examples 



(

x

1


,

y

1


)
,
…
,
(

x

p


,

y

p


)


{\displaystyle (x_{1},y_{1}),\dots ,(x_{p},y_{p})}

 and produces a sequence of weights 




w

0


,

w

1


,
…
,

w

p




{\displaystyle w_{0},w_{1},\dots ,w_{p}}

 starting from some initial weight 




w

0




{\displaystyle w_{0}}

, usually chosen at random.
These weights are computed in turn: first compute 




w

i




{\displaystyle w_{i}}

 using only 



(

x

i


,

y

i


,

w

i
−
1


)


{\displaystyle (x_{i},y_{i},w_{i-1})}

 for 



i
=
1
,
…
,
p


{\displaystyle i=1,\dots ,p}

. The output of the algorithm is then 




w

p




{\displaystyle w_{p}}

, giving us a new function 



x
↦

f

N


(

w

p


,
x
)


{\displaystyle x\mapsto f_{N}(w_{p},x)}

. The computation is the same in each step, hence only the case 



i
=
1


{\displaystyle i=1}

 is described.
Calculating 




w

1




{\displaystyle w_{1}}

 from 



(

x

1


,

y

1


,

w

0


)


{\displaystyle (x_{1},y_{1},w_{0})}

 is done by considering a variable weight 



w


{\displaystyle w}

 and applying gradient descent to the function 



w
↦
E
(

f

N


(
w
,

x

1


)
,

y

1


)


{\displaystyle w\mapsto E(f_{N}(w,x_{1}),y_{1})}

 to find a local minimum, 
starting at 



w
=

w

0




{\displaystyle w=w_{0}}

.
This makes 




w

1




{\displaystyle w_{1}}

 the minimizing weight found by gradient descent.

Algorithm in code[edit]
This article's tone or style may not reflect the encyclopedic tone used on Wikipedia. See Wikipedia's guide to writing better articles for suggestions. (December 2016) (Learn how and when to remove this template message)
To implement the algorithm above, explicit formulas are required for the gradient of the function 



w
↦
E
(

f

N


(
w
,
x
)
,
y
)


{\displaystyle w\mapsto E(f_{N}(w,x),y)}

 where the function is 



E
(
y
,

y
′

)
=

|

y
−

y
′



|


2




{\displaystyle E(y,y')=|y-y'|^{2}}

.
The learning algorithm can be divided into two phases: propagation and weight update.

Phase 1: propagation[edit]
Each propagation involves the following steps:

Propagation forward through the network to generate the output value(s)
Calculation of the cost (error term)
Propagation of the output activations back through the network using the training pattern target to generate the deltas (the difference between the targeted and actual output values) of all output and hidden neurons.
Phase 2: weight update[edit]
For each weight, the following steps must be followed:

The weight's output delta and input activation are multiplied to find the gradient of the weight.
A ratio (percentage) of the weight's gradient is subtracted from the weight.
This ratio (percentage) influences the speed and quality of learning; it is called the learning rate. The greater the ratio, the faster the neuron trains, but the lower the ratio, the more accurate the training is. The sign of the gradient of a weight indicates whether the error varies directly with, or inversely to, the weight. Therefore, the weight must be updated in the opposite direction, "descending" the gradient.
Learning is repeated (on new batches) until the network performs adequately.

Pseudocode[edit]
The following is pseudocode for a stochastic gradient descent algorithm for training a three-layer network (only one hidden layer):

  initialize network weights (often small random values)
  do
     forEach training example named ex
        prediction = neural-net-output(network, ex)  // forward pass
        actual = teacher-output(ex)
        compute error (prediction - actual) at the output units
        compute 
  
    
      
        Δ
        
          w
          
            h
          
        
      
    
    {\displaystyle \Delta w_{h}}
  
 for all weights from hidden layer to output layer  // backward pass
        compute 
  
    
      
        Δ
        
          w
          
            i
          
        
      
    
    {\displaystyle \Delta w_{i}}
  
 for all weights from input layer to hidden layer   // backward pass continued
        update network weights // input layer not modified by error estimate
  until all examples classified correctly or another stopping criterion satisfied
  return the network

The lines labeled "backward pass" can be implemented using the backpropagation algorithm, which calculates the gradient of the error of the network regarding the network's modifiable weights.[92]

Extension[edit]
The choice of learning rate 



η


{\textstyle \eta }

 is important, since a high value can cause too strong a change, causing the minimum to be missed, while a too low learning rate slows the training unnecessarily.
Optimizations such as Quickprop are primarily aimed at speeding up error minimization; other improvements mainly try to increase reliability.

Adaptive learning rate[edit]
In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements of this algorithm use an adaptive learning rate.[93]

Inertia[edit]
By using a variable inertia term (Momentum) 



α


{\textstyle \alpha }

 the gradient and the last change can be weighted such that the weight adjustment additionally depends on the previous change. If the Momentum 



α


{\textstyle \alpha }

 is equal to 0, the change depends solely on the gradient, while a value of 1 will only depend on the last change.
Similar to a ball rolling down a mountain, whose current speed is determined not only by the current slope of the mountain but also by its own inertia, inertia can be added:



Δ

w

i
j


(
t
+
1
)
=
(
1
−
α
)
η

δ

j



o

i


+
α

Δ

w

i
j


(
t
)


{\displaystyle \Delta w_{ij}(t+1)=(1-\alpha )\eta \delta _{j}o_{i}+\alpha \,\Delta w_{ij}(t)}

where:





Δ

w

i
j


(
t
+
1
)


{\textstyle \Delta w_{ij}(t+1)}

 is the change in weight 




w

i
j


(
t
+
1
)


{\textstyle w_{ij}(t+1)}

 in the connection of neuron 



i


{\textstyle i}

 to neuron 



j


{\textstyle j}

 at time 



(
t
+
1
)
,


{\textstyle (t+1),}






η


{\textstyle \eta }

 a learning rate (



η
<
0
)
,


{\textstyle \eta <0),}







δ

j




{\textstyle \delta _{j}}

 the error signal of neuron 



j


{\textstyle j}

 and





o

i




{\textstyle o_{i}}

 the output of neuron 



i


{\textstyle i}

, which is also an input of the current neuron (neuron 



j


{\textstyle j}

),




α


{\textstyle \alpha }

 the influence of the inertial term 



Δ

w

i
j


(
t
)


{\textstyle \Delta w_{ij}(t)}

 (in 



[
0
,
1
]


{\textstyle [0,1]}

). This corresponds to the weight change at the previous point in time.
Inertia makes the current weight change 



(
t
+
1
)


{\textstyle (t+1)}

 depend both on the current gradient of the error function (slope of the mountain, 1st summand), as well as on the weight change from the previous point in time (inertia, 2nd summand).
With inertia, the problems of getting stuck (in steep ravines and flat plateaus) are avoided. Since, for example, the gradient of the error function becomes very small in flat plateaus, a plateau would immediately lead to a "deceleration" of the gradient descent. This deceleration is delayed by the addition of the inertia term so that a flat plateau can be escaped more quickly.

Modes of learning[edit]
Two modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces "noise" into the gradient descent process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the average error of the batch. A common compromise choice is to use "mini-batches", meaning small batches and with samples in each batch selected stochastically from the entire data set.

Variants[edit]
Group method of data handling[edit]
Main article: Group method of data handlingThe Group Method of Data Handling (GMDH)[94] features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov-Gabor polynomials that permit additions and multiplications. It used a deep feedforward multilayer perceptron with eight layers.[95] It is a supervised learning network that grows layer by layer, where each layer is trained by regression analysis. Useless items are detected using a validation set, and pruned through regularization. The size and depth of the resulting network depends on the task.[96]
Convolutional neural networks[edit]
Main article: Convolutional neural networkA convolutional neural network (CNN) is a class of deep, feed-forward networks, composed of one or more convolutional layers with fully connected layers (matching those in typical Artificial neural networks) on top. It uses tied weights and pooling layers. In particular, max-pooling[17] is often structured via Fukushima's convolutional architecture.[97] This architecture allows CNNs to take advantage of the 2D structure of input data.
CNNs are suitable for processing visual and other two-dimensional data.[98][99] They have shown superior results in both image and speech applications. They can be trained with standard backpropagation. CNNs are easier to train than other regular, deep, feed-forward neural networks and have many fewer parameters to estimate.[100] Examples of applications in computer vision include DeepDream[101] and robot navigation.[102]
A recent development has been that of Capsule Neural Network (CapsNet), the idea behind which is to add structures called capsules to a CNN and to reuse output from several of those capsules to form more stable (with respect to various perturbations) representations for higher order capsules.[103]

Long short-term memory[edit]
Main article: Long short-term memoryLong short-term memory (LSTM) networks are RNNs that avoid the vanishing gradient problem.[104] LSTM is normally augmented by recurrent gates called forget gates.[105] LSTM networks prevent backpropagated errors from vanishing or exploding.[20] Instead errors can flow backwards through unlimited numbers of virtual layers in space-unfolded LSTM. That is, LSTM can learn "very deep learning" tasks[9] that require memories of events that happened thousands or even millions of discrete time steps ago. Problem-specific LSTM-like topologies can be evolved.[106] LSTM can handle long delays and signals that have a mix of low and high frequency components.
Stacks of LSTM RNNs[107] trained by Connectionist Temporal Classification (CTC)[108] can find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.
In 2003, LSTM started to become competitive with traditional speech recognizers.[109] In 2007, the combination with CTC achieved first good results on speech data.[110] In 2009, a CTC-trained LSTM was the first RNN to win pattern recognition contests, when it won several competitions in connected handwriting recognition.[9][35] In 2014, Baidu used CTC-trained RNNs to break the Switchboard Hub5'00 speech recognition benchmark, without traditional speech processing methods.[111] LSTM also improved large-vocabulary speech recognition,[112][113] text-to-speech synthesis,[114] for Google Android,[54][115] and photo-real talking heads.[116] In 2015, Google's speech recognition experienced a 49% improvement through CTC-trained LSTM.[117]
LSTM became popular in Natural Language Processing. Unlike previous models based on HMMs and similar concepts, LSTM can learn to recognise context-sensitive languages.[118] LSTM improved machine translation,[119][120] language modeling[121] and multilingual language processing.[122] LSTM combined with CNNs improved automatic image captioning.[123]

Deep reservoir computing[edit]
Main article: Reservoir computingDeep Reservoir Computing and Deep Echo State Networks (deepESNs)[124][125] provide a framework for efficiently trained models for hierarchical processing of temporal data, while enabling the investigation of the inherent role of RNN layered composition.[clarification needed]
Deep belief networks[edit]
Main article: Deep belief network
 A restricted Boltzmann machine (RBM) with fully connected visible and hidden units. Note there are no hidden-hidden or visible-visible connections.
A deep belief network (DBN) is a probabilistic, generative model made up of multiple layers of hidden units. It can be considered a composition of simple learning modules that make up each layer.[126]
A DBN can be used to generatively pre-train a DNN by using the learned DBN weights as the initial DNN weights. Backpropagation or other discriminative algorithms can then tune these weights. This is particularly helpful when training data are limited, because poorly initialized weights can significantly hinder model performance. These pre-trained weights are in a region of the weight space that is closer to the optimal weights than were they randomly chosen. This allows for both improved modeling and faster convergence of the fine-tuning phase.[127]

Large memory storage and retrieval neural networks[edit]
Large memory storage and retrieval neural networks (LAMSTAR)[128][129] are fast deep learning neural networks of many layers that can use many filters simultaneously. These filters may be nonlinear, stochastic, logic, non-stationary, or even non-analytical. They are biologically motivated and learn continuously.
A LAMSTAR neural network may serve as a dynamic neural network in spatial or time domains or both. Its speed is provided by Hebbian link-weights[130] that integrate the various and usually different filters (preprocessing functions) into its many layers and to dynamically rank the significance of the various layers and functions relative to a given learning task. This grossly imitates biological learning which integrates various preprocessors (cochlea, retina, etc.) and cortexes (auditory, visual, etc.) and their various regions. Its deep learning capability is further enhanced by using inhibition, correlation and its ability to cope with incomplete data, or "lost" neurons or layers even amidst a task. It is fully transparent due to its link weights. The link-weights allow dynamic determination of innovation and redundancy, and facilitate the ranking of layers, of filters or of individual neurons relative to a task.
LAMSTAR has been applied to many domains, including medical[131][132][133] and financial predictions,[134] adaptive filtering of noisy speech in unknown noise,[135] still-image recognition,[136] video image recognition,[137] software security[138] and adaptive control of non-linear systems.[139] LAMSTAR had a much faster learning speed and somewhat lower error rate than a CNN based on ReLU-function filters and max pooling, in 20 comparative studies.[140]
These applications demonstrate delving into aspects of the data that are hidden from shallow learning networks and the human senses, such as in the cases of predicting onset of sleep apnea events,[132] of an electrocardiogram of a fetus as recorded from skin-surface electrodes placed on the mother's abdomen early in pregnancy,[133] of financial prediction[128] or in blind filtering of noisy speech.[135]
LAMSTAR was proposed in 1996 (A U.S. Patent 5,920,852 A) and was further developed Graupe and Kordylewski from 1997–2002.[141][142][143] A modified version, known as LAMSTAR 2, was developed by Schneider and Graupe in 2008.[144][145]

Stacked (de-noising) auto-encoders[edit]
The auto encoder idea is motivated by the concept of a good representation. For example, for a classifier, a good representation can be defined as one that yields a better-performing classifier.
An encoder is a deterministic mapping 




f

θ




{\displaystyle f_{\theta }}

 that transforms an input vector x into hidden representation y, where 



θ
=
{

W

,
b
}


{\displaystyle \theta =\{{\boldsymbol {W}},b\}}

, 




W



{\displaystyle {\boldsymbol {W}}}

 is the weight matrix and b is an offset vector (bias). A decoder maps back the hidden representation y to the reconstructed input z via 




g

θ




{\displaystyle g_{\theta }}

. The whole process of auto encoding is to compare this reconstructed input to the original and try to minimize the error to make the reconstructed value as close as possible to the original.
In stacked denoising auto encoders, the partially corrupted output is cleaned (de-noised). This idea was introduced in 2010 by Vincent et al.[146] with a specific approach to good representation, a good representation is one that can be obtained robustly from a corrupted input and that will be useful for recovering the corresponding clean input. Implicit in this definition are the following ideas:

The higher level representations are relatively stable and robust to input corruption;
It is necessary to extract features that are useful for representation of the input distribution.
The algorithm starts by a stochastic mapping of 




x



{\displaystyle {\boldsymbol {x}}}

 to 






x
~





{\displaystyle {\tilde {\boldsymbol {x}}}}

 through 




q

D


(



x
~




|


x

)


{\displaystyle q_{D}({\tilde {\boldsymbol {x}}}|{\boldsymbol {x}})}

, this is the corrupting step. Then the corrupted input 






x
~





{\displaystyle {\tilde {\boldsymbol {x}}}}

 passes through a basic auto-encoder process and is mapped to a hidden representation 




y

=

f

θ


(



x
~



)
=
s
(

W




x
~



+
b
)


{\displaystyle {\boldsymbol {y}}=f_{\theta }({\tilde {\boldsymbol {x}}})=s({\boldsymbol {W}}{\tilde {\boldsymbol {x}}}+b)}

. From this hidden representation, we can reconstruct 




z

=

g

θ


(

y

)


{\displaystyle {\boldsymbol {z}}=g_{\theta }({\boldsymbol {y}})}

. In the last stage, a minimization algorithm runs in order to have z as close as possible to uncorrupted input 




x



{\displaystyle {\boldsymbol {x}}}

. The reconstruction error 




L

H


(

x

,

z

)


{\displaystyle L_{H}({\boldsymbol {x}},{\boldsymbol {z}})}

 might be either the cross-entropy loss with an affine-sigmoid decoder, or the squared error loss with an affine decoder.[146]
In order to make a deep architecture, auto encoders stack.[147] Once the encoding function 




f

θ




{\displaystyle f_{\theta }}

 of the first denoising auto encoder is learned and used to uncorrupt the input (corrupted input), the second level can be trained.[146]
Once the stacked auto encoder is trained, its output can be used as the input to a supervised learning algorithm such as support vector machine classifier or a multi-class logistic regression.[146]

Deep stacking networks[edit]
A deep stacking network (DSN)[148] (deep convex network) is based on a hierarchy of blocks of simplified neural network modules. It was introduced in 2011 by Deng and Dong.[149] It formulates the learning as a convex optimization problem with a closed-form solution, emphasizing the mechanism's similarity to stacked generalization.[150] Each DSN block is a simple module that is easy to train by itself in a supervised fashion without backpropagation for the entire blocks.[151]
Each block consists of a simplified multi-layer perceptron (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units, and the output layer has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T, and the input data vectors x form the columns of matrix X. The matrix of hidden units is 




H

=
σ
(


W


T



X

)


{\displaystyle {\boldsymbol {H}}=\sigma ({\boldsymbol {W}}^{T}{\boldsymbol {X}})}

. Modules are trained in order, so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class y, and its estimate is concatenated with original input X to form the expanded input for the next block. Thus, the input to the first block contains the original data only, while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem:






min


U

T




f
=

|


|



U


T



H

−

T


|



|


F


2


,


{\displaystyle \min _{U^{T}}f=||{\boldsymbol {U}}^{T}{\boldsymbol {H}}-{\boldsymbol {T}}||_{F}^{2},}


which has a closed-form solution.
Unlike other deep architectures, such as DBNs, the goal is not to discover the transformed feature representation. The structure of the hierarchy of this kind of architecture makes parallel learning straightforward, as a batch-mode optimization problem. In purely discriminative tasks, DSNs perform better than conventional DBNs.[148]

Tensor deep stacking networks[edit]
This architecture is a DSN extension. It offers two important improvements: it uses higher-order information from covariance statistics, and it transforms the non-convex problem of a lower-layer to a convex sub-problem of an upper-layer.[152] TDSNs use covariance statistics in a bilinear mapping from each of two distinct sets of hidden units in the same layer to predictions, via a third-order tensor.
While parallelization and scalability are not considered seriously in conventional DNNs,[153][154][155] all learning for DSNs and TDSNs is done in batch mode, to allow parallelization.[149][148] Parallelization allows scaling the design to larger (deeper) architectures and data sets.
The basic architecture is suitable for diverse tasks such as classification and regression.

Spike-and-slab RBMs[edit]
The need for deep learning with real-valued inputs, as in Gaussian restricted Boltzmann machines, led to the spike-and-slab RBM (ssRBM), which models continuous-valued inputs with strictly binary latent variables.[156] Similar to basic RBMs and its variants, a spike-and-slab RBM is a bipartite graph, while like GRBMs, the visible units (input) are real-valued. The difference is in the hidden layer, where each hidden unit has a binary spike variable and a real-valued slab variable. A spike is a discrete probability mass at zero, while a slab is a density over continuous domain;[157] their mixture forms a prior.[158]
An extension of ssRBM called µ-ssRBM provides extra modeling capacity using additional terms in the energy function. One of these terms enables the model to form a conditional distribution of the spike variables by marginalizing out the slab variables given an observation.

Compound hierarchical-deep models[edit]
Compound hierarchical-deep models compose deep networks with non-parametric Bayesian models. Features can be learned using deep architectures such as DBNs,[25] DBMs,[159] deep auto encoders,[160] convolutional variants,[161][162] ssRBMs,[157] deep coding networks,[163] DBNs with sparse feature learning,[164] RNNs,[165] conditional DBNs,[166] de-noising auto encoders.[167] This provides a better representation, allowing faster learning and more accurate classification with high-dimensional data. However, these architectures are poor at learning novel classes with few examples, because all network units are involved in representing the input (a distributed representation) and must be adjusted together (high degree of freedom). Limiting the degree of freedom reduces the number of parameters to learn, facilitating learning of new classes from few examples. Hierarchical Bayesian (HB) models allow learning from few examples, for example[168][169][170][171][172] for computer vision, statistics and cognitive science.
Compound HD architectures aim to integrate characteristics of both HB and deep networks. The compound HDP-DBM architecture is a hierarchical Dirichlet process (HDP) as a hierarchical model, incorporated with DBM architecture. It is a full generative model, generalized from abstract concepts flowing through the layers of the model, which is able to synthesize new examples in novel classes that look "reasonably" natural. All the levels are learned jointly by maximizing a joint log-probability score.[173]
In a DBM with three hidden layers, the probability of a visible input ν is:





p
(

ν

,
ψ
)
=


1
Z



∑

h



e


∑

i
j



W

i
j


(
1
)



ν

i



h

j


1


+

∑

j
l



W

j
l


(
2
)



h

j


1



h

l


2


+

∑

l
m



W

l
m


(
3
)



h

l


2



h

m


3




,


{\displaystyle p({\boldsymbol {\nu }},\psi )={\frac {1}{Z}}\sum _{h}e^{\sum _{ij}W_{ij}^{(1)}\nu _{i}h_{j}^{1}+\sum _{jl}W_{jl}^{(2)}h_{j}^{1}h_{l}^{2}+\sum _{lm}W_{lm}^{(3)}h_{l}^{2}h_{m}^{3}},}


where 




h

=
{


h


(
1
)


,


h


(
2
)


,


h


(
3
)


}


{\displaystyle {\boldsymbol {h}}=\{{\boldsymbol {h}}^{(1)},{\boldsymbol {h}}^{(2)},{\boldsymbol {h}}^{(3)}\}}

 is the set of hidden units, and 



ψ
=
{


W


(
1
)


,


W


(
2
)


,


W


(
3
)


}


{\displaystyle \psi =\{{\boldsymbol {W}}^{(1)},{\boldsymbol {W}}^{(2)},{\boldsymbol {W}}^{(3)}\}}

 are the model parameters, representing visible-hidden and hidden-hidden symmetric interaction terms.
A learned DBM model is an undirected model that defines the joint distribution 



P
(
ν
,

h

1


,

h

2


,

h

3


)


{\displaystyle P(\nu ,h^{1},h^{2},h^{3})}

. One way to express what has been learned is the conditional model 



P
(
ν
,

h

1


,

h

2



|


h

3


)


{\displaystyle P(\nu ,h^{1},h^{2}|h^{3})}

 and a prior term 



P
(

h

3


)


{\displaystyle P(h^{3})}

.
Here 



P
(
ν
,

h

1


,

h

2



|


h

3


)


{\displaystyle P(\nu ,h^{1},h^{2}|h^{3})}

 represents a conditional DBM model, which can be viewed as a two-layer DBM but with bias terms given by the states of 




h

3




{\displaystyle h^{3}}

:





P
(
ν
,

h

1


,

h

2



|


h

3


)
=


1

Z
(
ψ
,

h

3


)




e


∑

i
j



W

i
j


(
1
)



ν

i



h

j


1


+

∑

j
l



W

j
l


(
2
)



h

j


1



h

l


2


+

∑

l
m



W

l
m


(
3
)



h

l


2



h

m


3




.


{\displaystyle P(\nu ,h^{1},h^{2}|h^{3})={\frac {1}{Z(\psi ,h^{3})}}e^{\sum _{ij}W_{ij}^{(1)}\nu _{i}h_{j}^{1}+\sum _{jl}W_{jl}^{(2)}h_{j}^{1}h_{l}^{2}+\sum _{lm}W_{lm}^{(3)}h_{l}^{2}h_{m}^{3}}.}


Deep predictive coding networks[edit]
A deep predictive coding network (DPCN) is a predictive coding scheme that uses top-down information to empirically adjust the priors needed for a bottom-up inference procedure by means of a deep, locally connected, generative model. This works by extracting sparse features from time-varying observations using a linear dynamical model. Then, a pooling strategy is used to learn invariant feature representations. These units compose to form a deep architecture and are trained by greedy layer-wise unsupervised learning. The layers constitute a kind of Markov chain such that the states at any layer depend only on the preceding and succeeding layers.
DPCNs predict the representation of the layer, by using a top-down approach using the information in upper layer and temporal dependencies from previous states.[174]
DPCNs can be extended to form a convolutional network.[174]

Networks with separate memory structures[edit]
Integrating external memory with Artificial neural networks dates to early research in distributed representations[175] and Kohonen's self-organizing maps. For example, in sparse distributed memory or hierarchical temporal memory, the patterns encoded by neural networks are used as addresses for content-addressable memory, with "neurons" essentially serving as address encoders and decoders. However, the early controllers of such memories were not differentiable.

LSTM-related differentiable memory structures[edit]
Apart from long short-term memory (LSTM), other approaches also added differentiable memory to recurrent functions. For example:

Differentiable push and pop actions for alternative memory networks called neural stack machines[176][177]
Memory networks where the control network's external differentiable storage is in the fast weights of another network[178]
LSTM forget gates[179]
Self-referential RNNs with special output units for addressing and rapidly manipulating the RNN's own weights in differentiable fashion (internal storage)[180][181]
Learning to transduce with unbounded memory[182]
Neural Turing machines[edit]
Main article: Neural Turing machineNeural Turing machines[183] couple LSTM networks to external memory resources, with which they can interact by attentional processes. The combined system is analogous to a Turing machine but is differentiable end-to-end, allowing it to be efficiently trained by gradient descent. Preliminary results demonstrate that neural Turing machines can infer simple algorithms such as copying, sorting and associative recall from input and output examples.
Differentiable neural computers (DNC) are an NTM extension. They out-performed Neural turing machines, long short-term memory systems and memory networks on sequence-processing tasks.[184][185][186][187][188]

Semantic hashing[edit]
Approaches that represent previous experiences directly and use a similar experience to form a local model are often called nearest neighbour or k-nearest neighbors methods.[189] Deep learning is useful in semantic hashing[190] where a deep graphical model the word-count vectors[191] obtained from a large set of documents.[clarification needed] Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by accessing all the addresses that differ by only a few bits from the address of the query document. Unlike sparse distributed memory that operates on 1000-bit addresses, semantic hashing works on 32 or 64-bit addresses found in a conventional computer architecture.

Memory networks[edit]
Memory networks[192][193] are another extension to neural networks incorporating long-term memory. The long-term memory can be read and written to, with the goal of using it for prediction. These models have been applied in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base and the output is a textual response.[194] A team of electrical and computer engineers from UCLA Samueli School of Engineering has created a physical artificial neural network that can analyze large volumes of data and identify objects at the actual speed of light.[195]

Pointer networks[edit]
Deep neural networks can be potentially improved by deepening and parameter reduction, while maintaining trainability. While training extremely deep (e.g., 1 million layers) neural networks might not be practical, CPU-like architectures such as pointer networks[196] and neural random-access machines[197] overcome this limitation by using external random-access memory and other components that typically belong to a computer architecture such as registers, ALU and pointers. Such systems operate on probability distribution vectors stored in memory cells and registers. Thus, the model is fully differentiable and trains end-to-end. The key characteristic of these models is that their depth, the size of their short-term memory, and the number of parameters can be altered independently – unlike models like LSTM, whose number of parameters grows quadratically with memory size.

Encoder–decoder networks[edit]
Encoder–decoder frameworks are based on neural networks that map highly structured input to highly structured output. The approach arose in the context of machine translation,[198][199][200] where the input and output are written sentences in two natural languages. In that work, an LSTM RNN or CNN was used as an encoder to summarize a source sentence, and the summary was decoded using a conditional RNN language model to produce the translation.[201] These systems share building blocks: gated RNNs and CNNs and trained attention mechanisms.

Multilayer kernel machine[edit]
Multilayer kernel machines (MKM) are a way of learning highly nonlinear functions by iterative application of weakly nonlinear kernels. They use the kernel principal component analysis (KPCA),[202] as a method for the unsupervised greedy layer-wise pre-training step of deep learning.[203]
Layer 



l
+
1


{\displaystyle l+1}

 learns the representation of the previous layer 



l


{\displaystyle l}

, extracting the 




n

l




{\displaystyle n_{l}}

 principal component (PC) of the projection layer 



l


{\displaystyle l}

 output in the feature domain induced by the kernel. For the sake of dimensionality reduction of the updated representation in each layer, a supervised strategy selects the best informative features among features extracted by KPCA. The process is:

rank the 




n

l




{\displaystyle n_{l}}

 features according to their mutual information with the class labels;
for different values of K and 




m

l


∈
{
1
,
…
,

n

l


}


{\displaystyle m_{l}\in \{1,\ldots ,n_{l}\}}

, compute the classification error rate of a K-nearest neighbor (K-NN) classifier using only the 




m

l




{\displaystyle m_{l}}

 most informative features on a validation set;
the value of 




m

l




{\displaystyle m_{l}}

 with which the classifier has reached the lowest error rate determines the number of features to retain.
Some drawbacks accompany the KPCA method as the building cells of an MKM.
A more straightforward way to use kernel machines for deep learning was developed for spoken language understanding.[204] The main idea is to use a kernel machine to approximate a shallow neural net with an infinite number of hidden units, then use stacking to splice the output of the kernel machine and the raw input in building the next, higher level of the kernel machine. The number of levels in the deep convex network is a hyper-parameter of the overall system, to be determined by cross validation.

Neural architecture search[edit]
Main article: Neural architecture search
Neural architecture search (NAS) uses machine learning to automate the design of Artificial neural networks. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset and use the results as feedback to teach the NAS network.[205]

Use[edit]
Using Artificial neural networks requires an understanding of their characteristics.

Choice of model: This depends on the data representation and the application. Overly complex models slow learning.
Learning algorithm: Numerous trade-offs exist between learning algorithms. Almost any algorithm will work well with the correct hyperparameters for training on a particular data set. However, selecting and tuning an algorithm for training on unseen data requires significant experimentation.
Robustness: If the model, cost function and learning algorithm are selected appropriately, the resulting ANN can become robust.
ANN capabilities fall within the following broad categories:[citation needed]

Function approximation, or regression analysis, including time series prediction, fitness approximation and modeling.
Classification, including pattern and sequence recognition, novelty detection and sequential decision making.
Data processing, including filtering, clustering, blind source separation and compression.
Robotics, including directing manipulators and prostheses.
Control, including computer numerical control.
Applications[edit]
Because of their ability to reproduce and model nonlinear processes, Artificial neural networks have found many applications in a wide range of disciplines.
Application areas include system identification and control (vehicle control, trajectory prediction,[206] process control, natural resource management), quantum chemistry,[207] general game playing,[208] pattern recognition (radar systems, face identification, signal classification,[209] 3D reconstruction,[210] object recognition and more), sequence recognition (gesture, speech, handwritten and printed text recognition), medical diagnosis, finance[211] (e.g. automated trading systems), data mining, visualization, machine translation, social network filtering[212] and e-mail spam filtering.
Artificial neural networks have been used to diagnose cancers, including lung cancer,[213] prostate cancer, colorectal cancer[214] and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.[215][216]
Artificial neural networks have been used to accelerate reliability analysis of infrastructures subject to natural disasters[217][218] and to predict foundation settlements.[219]
Artificial neural networks have also been used for building black-box models in geoscience: hydrology,[220][221] ocean modelling and coastal engineering,[222][223] and geomorphology.[224]
Artificial neural networks have been employed with some success also in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying android malware,[225] for identifying domains belonging to threat actors[226] and for detecting URLs posing a security risk.[227] Research is being carried out also on ANN systems designed for penetration testing,[228] for detecting botnets,[229] credit cards frauds,[230] network intrusions and, more in general, potentially infected machines.

Types of models[edit]
Many types of models are used, defined at different levels of abstraction and modeling different aspects of neural systems. They range from models of the short-term behavior of individual neurons,[231] models of how the dynamics of neural circuitry arise from interactions between individual neurons and finally to models of how behavior can arise from abstract neural modules that represent complete subsystems. These include models of the long-term, and short-term plasticity, of neural systems and their relations to learning and memory from the individual neuron to the system level.

Theoretical properties[edit]
Computational power[edit]
The multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.
A specific recurrent architecture with rational valued weights (as opposed to full precision real number-valued weights) has the full power of a universal Turing machine,[232] using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.[233]

Capacity[edit]
Models' "capacity" property roughly corresponds to their ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.[citation needed]

Convergence[edit]
Models may not consistently converge on a single solution, firstly because many local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical. However, for CMAC neural network, a recursive least squares algorithm was introduced to train it, and this algorithm can be guaranteed to converge in one step.[90]

Generalization and statistics[edit]
Applications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the capacity of the network significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and optimally select hyperparameters to minimize the generalization error. The second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.

 Confidence analysis of a neural network
Supervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of the output of the network, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.
By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based neural network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is very useful in classification as it gives a certainty measure on classifications.
The softmax activation function is:






y

i


=



e


x

i






∑

j
=
1


c



e


x

j









{\displaystyle y_{i}={\frac {e^{x_{i}}}{\sum _{j=1}^{c}e^{x_{j}}}}}




Criticism[edit]
Training issues[edit]
A common criticism of neural networks, particularly in robotics, is that they require too much training for real-world operation.[citation needed] Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example and by grouping examples in so-called mini-batches. Improving the training efficiency and convergence capability has always been an ongoing research area for neural network. For example, by introducing a recursive least squares algorithm for CMAC neural network, the training process only takes one step to converge.[90]

Theoretical issues[edit]
A fundamental objection is that they do not reflect how real neurons function. Back propagation is a critical part of most artificial neural networks, although no such mechanism exists in biological neural networks.[234] How information is coded by real neurons is not known. Sensor neurons fire action potentials more frequently with sensor activation and muscle cells pull more strongly when their associated motor neurons receive action potentials more frequently.[235] Other than the case of relaying information from a sensor neuron to a motor neuron, almost nothing of the principles of how information is handled by biological neural networks is known. This is a subject of active research in neural coding.
The motivation behind artificial neural networks is not necessarily to strictly replicate neural function, but to use biological neural networks as an inspiration. A central claim of artificial neural networks is therefore that it embodies some new and powerful general principle for processing information. Unfortunately, these general principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. Alexander Dewdney commented that, as a result, artificial neural networks have a "something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything".[236]
Biological brains use both shallow and deep circuits as reported by brain anatomy,[237] displaying a wide variety of invariance. Weng[238] argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.

Hardware issues[edit]
Large and effective neural networks require considerable computing resources.[239] While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may compel a neural network designer to fill many millions of database rows for its connections –  which can consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons –  which must often be matched with enormous CPU processing power and time.
Schmidhuber notes that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before.[240] The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.[241][239]
Neuromorphic engineering addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.[242]

Practical counterexamples to criticisms[edit]
Arguments against Dewdney's position are that neural networks have been successfully used to solve many complex and diverse tasks, ranging from autonomously flying aircraft[243] to detecting credit card fraud to mastering the game of Go.
Technology writer Roger Bridgman commented:

.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be "an opaque, unreadable table...valueless as a scientific resource".
In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.[244]


Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs non-local learning and shallow vs deep architecture.[245]

Hybrid approaches[edit]
Advocates of hybrid models (combining neural networks and symbolic approaches), claim that such a mixture can better capture the mechanisms of the human mind.[246][247]

Types[edit]
Main article: Types of artificial neural networks
Artificial neural networks have many variations. The simplest, static types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to change during the learning process. The latter are much more complicated, but can shorten learning periods and produce better results. Some types allow/require learning to be "supervised" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.

Gallery[edit]




A single-layer feedforward artificial neural network. Arrows originating from 





x

2





{\displaystyle \scriptstyle x_{2}}

 are omitted for clarity. There are p inputs to this network and q outputs. In this system, the value of the qth output, 





y

q





{\displaystyle \scriptstyle y_{q}}

 would be calculated as 





y

q


=
K
∗
(
∑
(

x

i


∗

w

i
q


)
−

b

q


)



{\displaystyle \scriptstyle y_{q}=K*(\sum (x_{i}*w_{iq})-b_{q})}








A two-layer feedforward artificial neural network.






An artificial neural network.






An ANN dependency graph.






A single-layer feedforward artificial neural network with 4 inputs, 6 hidden and 2 outputs. Given position state and direction outputs wheel based control values.






A two-layer feedforward artificial neural network with 8 inputs, 2x8 hidden and 2 outputs. Given position state, direction and other environment values outputs thruster based control values.






Parallel pipeline structure of CMAC neural network. This learning algorithm can converge in one step.




See also[edit]
This "see also" section may contain an excessive number of suggestions. Please ensure that only the most relevant links are given, that they are not red links, and that any links are not already in this article. (March 2018) (Learn how and when to remove this template message)

Hierarchical temporal memory
20Q
ADALINE
Adaptive resonance theory
Artificial life
Associative memory
Autoencoder
BEAM robotics
Biological cybernetics
Biologically inspired computing
Blue Brain Project
Catastrophic interference
Cerebellar Model Articulation Controller (CMAC)
Cognitive architecture
Cognitive science
Convolutional neural network (CNN)
Connectionist expert system
Connectomics
Cultured neuronal networks
Deep learning
Encog
Fuzzy logic
Gene expression programming
Genetic algorithm
Genetic programming
Group method of data handling
Habituation
In Situ Adaptive Tabulation
Machine learning concepts
Models of neural computation
Neuroevolution
Neural coding
Neural gas
Neural machine translation
Neural network software
Neuroscience
Nonlinear system identification
Optical neural network
Parallel Constraint Satisfaction Processes
Parallel distributed processing
Radial basis function network
Recurrent neural networks
Self-organizing map
Spiking neural network
Systolic array
Tensor product network
Time delay neural network (TDNN)

References[edit]


^ McCulloch, Warren; Walter Pitts (1943). "A Logical Calculus of Ideas Immanent in Nervous Activity". Bulletin of Mathematical Biophysics. 5 (4): 115–133. doi:10.1007/BF02478259..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ Kleene, S.C. (1956). "Representation of Events in Nerve Nets and Finite Automata". Annals of Mathematics Studies (34). Princeton University Press. pp. 3–41. Retrieved 17 June 2017.

^ Hebb, Donald (1949). The Organization of Behavior. New York: Wiley. ISBN 978-1-135-63190-1.

^ Farley, B.G.; W.A. Clark (1954). "Simulation of Self-Organizing Systems by Digital Computer". IRE Transactions on Information Theory. 4 (4): 76–84. doi:10.1109/TIT.1954.1057468.

^ Rochester, N.; J.H. Holland; L.H. Habit; W.L. Duda (1956). "Tests on a cell assembly theory of the action of the brain, using a large digital computer". IRE Transactions on Information Theory. 2 (3): 80–93. doi:10.1109/TIT.1956.1056810.

^ Rosenblatt, F. (1958). "The Perceptron: A Probabilistic Model For Information Storage And Organization In The Brain". Psychological Review. 65 (6): 386–408. CiteSeerX 10.1.1.588.3775. doi:10.1037/h0042519. PMID 13602029.

^ a b Werbos, P.J. (1975). Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences.

^ David H. Hubel and Torsten N. Wiesel (2005). Brain and visual perception: the story of a 25-year collaboration. Oxford University Press US. p. 106. ISBN 978-0-19-517618-6.

^ a b c d e f Schmidhuber, J. (2015). "Deep Learning in Neural Networks: An Overview". Neural Networks. 61: 85–117. arXiv:1404.7828. doi:10.1016/j.neunet.2014.09.003. PMID 25462637.

^ Ivakhnenko, A. G. (1973). Cybernetic Predicting Devices. CCM Information Corporation.

^ Ivakhnenko, A. G.; Grigorʹevich Lapa, Valentin (1967). Cybernetics and forecasting techniques. American Elsevier Pub. Co.

^ Minsky, Marvin; Papert, Seymour (1969). Perceptrons: An Introduction to Computational Geometry. MIT Press. ISBN 978-0-262-63022-1.

^ Rumelhart, D.E; McClelland, James (1986). Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Cambridge: MIT Press. ISBN 978-0-262-63110-5.

^ Qian, N.; Sejnowski, T.J. (1988). "Predicting the secondary structure of globular proteins using neural network models" (PDF). Journal of Molecular Biology. 202. pp. 865–884. Qian1988.

^ Rost, B.; Sander, C. (1993). "Prediction of protein secondary structure at better than 70% accuracy" (PDF). Journal of Molecular Biology. 232. pp. 584–599. Rost1993.

^ J. Weng, N. Ahuja and T. S. Huang, "Cresceptron: a self-organizing neural network which grows adaptively," Proc. International Joint Conference on Neural Networks, Baltimore, Maryland, vol I, pp. 576–581, June, 1992.

^ a b J. Weng, N. Ahuja and T. S. Huang, "Learning recognition and segmentation of 3-D objects from 2-D images," Proc. 4th International Conf. Computer Vision, Berlin, Germany, pp. 121–128, May, 1993.

^ J. Weng, N. Ahuja and T. S. Huang, "Learning recognition and segmentation using the Cresceptron," International Journal of Computer Vision, vol. 25, no. 2, pp. 105–139, Nov. 1997.

^ Dominik Scherer, Andreas C. Müller, and Sven Behnke: "Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition," In 20th International Conference Artificial Neural Networks (ICANN), pp. 92–101, 2010. doi:10.1007/978-3-642-15825-4_10.

^ a b S. Hochreiter., "Untersuchungen zu dynamischen neuronalen Netzen," Diploma thesis. Institut f. Informatik, Technische Univ. Munich. Advisor: J. Schmidhuber, 1991.

^ Hochreiter, S.;  et al. (15 January 2001). "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies".  In Kolen, John F.; Kremer, Stefan C. (eds.). A Field Guide to Dynamical Recurrent Networks. John Wiley & Sons. ISBN 978-0-7803-5369-5.

^ J. Schmidhuber., "Learning complex, extended sequences using the principle of history compression," Neural Computation, 4, pp. 234–242, 1992.

^ Sven Behnke (2003). Hierarchical Neural Networks for Image Interpretation (PDF). Lecture Notes in Computer Science. 2766. Springer.

^ Smolensky, P. (1986). "Information processing in dynamical systems: Foundations of harmony theory.".  In D. E. Rumelhart; J. L. McClelland; PDP Research Group (eds.). Parallel Distributed Processing: Explorations in the Microstructure of Cognition. 1. pp. 194–281. ISBN 9780262680530.

^ a b c Hinton, G. E.; Osindero, S.; Teh, Y. (2006). "A fast learning algorithm for deep belief nets" (PDF). Neural Computation. 18 (7): 1527–1554. CiteSeerX 10.1.1.76.1541. doi:10.1162/neco.2006.18.7.1527. PMID 16764513.

^ Hinton, G. (2009). "Deep belief networks". Scholarpedia. 4 (5): 5947. Bibcode:2009SchpJ...4.5947H. doi:10.4249/scholarpedia.5947.

^ Ng, Andrew; Dean, Jeff (2012). "Building High-level Features Using Large Scale Unsupervised Learning". arXiv:1112.6209 [cs.LG].

^ Yang, J. J.; Pickett, M. D.; Li, X. M.; Ohlberg, D. A. A.; Stewart, D. R.; Williams, R. S. (2008). "Memristive switching mechanism for metal/oxide/metal nanodevices". Nat. Nanotechnol. 3 (7): 429–433. doi:10.1038/nnano.2008.160. PMID 18654568.

^ Strukov, D. B.; Snider, G. S.; Stewart, D. R.; Williams, R. S. (2008). "The missing memristor found". Nature. 453 (7191): 80–83. Bibcode:2008Natur.453...80S. doi:10.1038/nature06932. PMID 18451858.

^ Cireşan, Dan Claudiu; Meier, Ueli; Gambardella, Luca Maria; Schmidhuber, Jürgen (21 September 2010). "Deep, Big, Simple Neural Nets for Handwritten Digit Recognition". Neural Computation. 22 (12): 3207–3220. arXiv:1003.0358. doi:10.1162/neco_a_00052. ISSN 0899-7667. PMID 20858131.

^ 2012 Kurzweil AI Interview with Jürgen Schmidhuber on the eight competitions won by his Deep Learning team 2009–2012

^ "How bio-inspired deep learning keeps winning competitions | KurzweilAI". www.kurzweilai.net. Retrieved 16 June 2017.

^ Graves, Alex; and Schmidhuber, Jürgen; Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks, in Bengio, Yoshua; Schuurmans, Dale; Lafferty, John; Williams, Chris K. I.; and Culotta, Aron (eds.), Advances in Neural Information Processing Systems 22 (NIPS'22), 7–10 December 2009, Vancouver, BC, Neural Information Processing Systems (NIPS) Foundation, 2009, pp. 545–552.

^ a b Graves, A.; Liwicki, M.; Fernandez, S.; Bertolami, R.; Bunke, H.; Schmidhuber, J. (2009). "A Novel Connectionist System for Improved Unconstrained Handwriting Recognition" (PDF). IEEE Transactions on Pattern Analysis and Machine Intelligence. 31 (5): 855–868. CiteSeerX 10.1.1.139.4502. doi:10.1109/tpami.2008.137. PMID 19299860.

^ a b c Graves, Alex; Schmidhuber, Jürgen (2009).  Bengio, Yoshua; Schuurmans, Dale; Lafferty, John; Williams, Chris editor-K. I.; Culotta, Aron (eds.). "Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks". Neural Information Processing Systems (NIPS) Foundation. Curran Associates, Inc: 545–552.

^ Graves, A.; Liwicki, M.; Fernández, S.; Bertolami, R.; Bunke, H.; Schmidhuber, J. (May 2009). "A Novel Connectionist System for Unconstrained Handwriting Recognition". IEEE Transactions on Pattern Analysis and Machine Intelligence. 31 (5): 855–868. CiteSeerX 10.1.1.139.4502. doi:10.1109/tpami.2008.137. ISSN 0162-8828. PMID 19299860.

^ a b Cireşan, Dan; Meier, Ueli; Masci, Jonathan; Schmidhuber, Jürgen (August 2012). "Multi-column deep neural network for traffic sign classification". Neural Networks. Selected Papers from IJCNN 2011. 32: 333–338. CiteSeerX 10.1.1.226.8219. doi:10.1016/j.neunet.2012.02.023. PMID 22386783.

^ a b Ciresan, Dan; Giusti, Alessandro; Gambardella, Luca M.; Schmidhuber, Juergen (2012).  Pereira, F.; Burges, C. J. C.; Bottou, L.; Weinberger, K. Q. (eds.). Advances in Neural Information Processing Systems 25 (PDF). Curran Associates, Inc. pp. 2843–2851.

^ a b Ciresan, Dan; Meier, U.; Schmidhuber, J. (June 2012). Multi-column deep neural networks for image classification. 2012 IEEE Conference on Computer Vision and Pattern Recognition. pp. 3642–3649. arXiv:1202.2745. CiteSeerX 10.1.1.300.3283. doi:10.1109/cvpr.2012.6248110. ISBN 978-1-4673-1228-8.

^ a b Ciresan, D. C.; Meier, U.; Masci, J.; Gambardella, L. M.; Schmidhuber, J. (2011). "Flexible, High Performance Convolutional Neural Networks for Image Classification" (PDF). International Joint Conference on Artificial Intelligence. doi:10.5591/978-1-57735-516-8/ijcai11-210.

^ Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffry (2012). "ImageNet Classification with Deep Convolutional Neural Networks" (PDF). NIPS 2012: Neural Information Processing Systems, Lake Tahoe, Nevada.

^ Fukushima, K. (1980). "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position". Biological Cybernetics. 36 (4): 93–202. doi:10.1007/BF00344251. PMID 7370364.

^ Riesenhuber, M; Poggio, T (1999). "Hierarchical models of object recognition in cortex". Nature Neuroscience. 2 (11): 1019–1025. doi:10.1038/14819. PMID 10526343.

^ Hinton, Geoffrey (31 May 2009). "Deep belief networks". Scholarpedia. 4 (5): 5947. Bibcode:2009SchpJ...4.5947H. doi:10.4249/scholarpedia.5947. ISSN 1941-6016.

^ Markoff, John (23 November 2012). "Scientists See Promise in Deep-Learning Programs". New York Times.

^ Martines, H.; Bengio, Y.; Yannakakis, G. N. (2013). "Learning Deep Physiological Models of Affect". IEEE Computational Intelligence Magazine (Submitted manuscript). 8 (2): 20–33. doi:10.1109/mci.2013.2247823.

^ J. Weng, "Why Have We Passed 'Neural Networks Do not Abstract Well'?," Natural Intelligence: the INNS Magazine, vol. 1, no.1, pp. 13–22, 2011.

^ Z. Ji, J. Weng, and D. Prokhorov, "Where-What Network 1: Where and What Assist Each Other Through Top-down Connections," Proc. 7th International Conference on Development and Learning (ICDL'08), Monterey, CA, Aug. 9–12, pp. 1–6, 2008.

^ X. Wu, G. Guo, and J. Weng, "Skull-closed Autonomous Development: WWN-7 Dealing with Scales," Proc. International Conference on Brain-Mind, July 27–28, East Lansing, Michigan, pp. 1–9, 2013.

^ a b c d e Zell, Andreas (1994). "chapter 5.2". Simulation Neuronaler Netze [Simulation of Neural Networks] (in German) (1st ed.). Addison-Wesley. ISBN 978-3-89319-554-1.

^ Abbod, Maysam F (2007). "Application of Artificial Intelligence to the Management of Urological Cancer". The Journal of Urology. 178 (4): 1150–1156. doi:10.1016/j.juro.2007.05.122. PMID 17698099.

^ DAWSON, CHRISTIAN W (1998). "An artificial neural network approach to rainfall-runoff modelling". Hydrological Sciences Journal. 43 (1): 47–66. doi:10.1080/02626669809492102.

^ "The Machine Learning Dictionary".

^ a b c d Schmidhuber, Jürgen (2015). "Deep Learning". Scholarpedia. 10 (11): 32832. Bibcode:2015SchpJ..1032832S. doi:10.4249/scholarpedia.32832.

^ Dreyfus, Stuart E. (1 September 1990). "Artificial neural networks, back propagation, and the Kelley-Bryson gradient procedure". Journal of Guidance, Control, and Dynamics. 13 (5): 926–928. Bibcode:1990JGCD...13..926D. doi:10.2514/3.25422. ISSN 0731-5090.

^ Eiji Mizutani, Stuart Dreyfus, Kenichi Nishio (2000). On derivation of MLP backpropagation from the Kelley-Bryson optimal-control gradient formula and its application. Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN 2000), Como Italy, July 2000. Online

^ Kelley, Henry J. (1960). "Gradient theory of optimal flight paths". ARS Journal. 30 (10): 947–954. doi:10.2514/8.5282.

^ Arthur E. Bryson (1961, April). A gradient method for optimizing multi-stage allocation processes. In Proceedings of the Harvard Univ. Symposium on digital computers and their applications.

^ Dreyfus, Stuart (1962). "The numerical solution of variational problems". Journal of Mathematical Analysis and Applications. 5 (1): 30–45. doi:10.1016/0022-247x(62)90004-5.

^ Russell, Stuart J.; Norvig, Peter (2010). Artificial Intelligence A Modern Approach. Prentice Hall. p. 578. ISBN 978-0-13-604259-4. The most popular method for learning in multilayer networks is called Back-propagation.

^ Bryson, Arthur Earl (1969). Applied Optimal Control: Optimization, Estimation and Control. Blaisdell Publishing Company or Xerox College Publishing. p. 481.

^ Seppo Linnainmaa (1970). The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors. Master's Thesis (in Finnish), Univ. Helsinki, 6–7.

^ Linnainmaa, Seppo (1976). "Taylor expansion of the accumulated rounding error". BIT Numerical Mathematics. 16 (2): 146–160. doi:10.1007/bf01931367.

^ Griewank, Andreas (2012). "Who Invented the Reverse Mode of Differentiation?" (PDF). Documenta Matematica, Extra Volume ISMP: 389–400. Archived from the original (PDF) on 21 July 2017. Retrieved 27 June 2017.

^ Griewank, Andreas; Walther, Andrea (2008). Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation, Second Edition. SIAM. ISBN 978-0-89871-776-1.

^ Dreyfus, Stuart (1973). "The computational solution of optimal control problems with time lag". IEEE Transactions on Automatic Control. 18 (4): 383–385. doi:10.1109/tac.1973.1100330.

^ Paul Werbos (1974). Beyond regression: New tools for prediction and analysis in the behavioral sciences. PhD thesis, Harvard University.

^ Werbos, Paul (1982). "Applications of advances in nonlinear sensitivity analysis" (PDF). System modeling and optimization. Springer. pp. 762–770.

^ Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J. (1986). "Learning representations by back-propagating errors". Nature. 323 (6088): 533–536. Bibcode:1986Natur.323..533R. doi:10.1038/323533a0.

^ Eric A. Wan (1993). "Time series prediction by using a connectionist network with internal delay lines." In Proceedings of the Santa Fe Institute Studies in the Sciences of Complexity, 15: p. 195. Addison-Wesley Publishing Co.

^ Hinton, G.; Deng, L.; Yu, D.; Dahl, G. E.; Mohamed, A. r; Jaitly, N.; Senior, A.; Vanhoucke, V.; Nguyen, P. (November 2012). "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups". IEEE Signal Processing Magazine. 29 (6): 82–97. Bibcode:2012ISPM...29...82H. doi:10.1109/msp.2012.2205597. ISSN 1053-5888.

^ Huang, Guang-Bin; Zhu, Qin-Yu; Siew, Chee-Kheong (2006). "Extreme learning machine: theory and applications". Neurocomputing. 70 (1): 489–501. CiteSeerX 10.1.1.217.3692. doi:10.1016/j.neucom.2005.12.126.

^ Widrow, Bernard;  et al. (2013). "The no-prop algorithm: A new learning algorithm for multilayer neural networks". Neural Networks. 37: 182–188. doi:10.1016/j.neunet.2012.09.020. PMID 23140797.

^ Ollivier, Yann; Charpiat, Guillaume (2015). "Training recurrent networks without backtracking". arXiv:1507.07680 [cs.NE].

^ ESANN. 2009

^ Hinton, G. E. (2010). "A Practical Guide to Training Restricted Boltzmann Machines". Tech. Rep. UTML TR 2010-003.

^ Ojha, Varun Kumar; Abraham, Ajith; Snášel, Václav (1 April 2017). "Metaheuristic design of feedforward neural networks: A review of two decades of research". Engineering Applications of Artificial Intelligence. 60: 97–116. arXiv:1705.05584. doi:10.1016/j.engappai.2017.01.013.

^ Dominic, S.; Das, R.; Whitley, D.; Anderson, C. (July 1991). "Genetic reinforcement learning for neural networks". IJCNN-91-Seattle International Joint Conference on Neural Networks. IJCNN-91-Seattle International Joint Conference on Neural Networks. Seattle, Washington, USA: IEEE. doi:10.1109/IJCNN.1991.155315. ISBN 0-7803-0164-1.

^ Hoskins, J.C.; Himmelblau, D.M. (1992). "Process control via artificial neural networks and reinforcement learning". Computers & Chemical Engineering. 16 (4): 241–251. doi:10.1016/0098-1354(92)80045-B.

^ Bertsekas, D.P.; Tsitsiklis, J.N. (1996). Neuro-dynamic programming. Athena Scientific. p. 512. ISBN 978-1-886529-10-6.

^ Secomandi, Nicola (2000). "Comparing neuro-dynamic programming algorithms for the vehicle routing problem with stochastic demands". Computers & Operations Research. 27 (11–12): 1201–1225. CiteSeerX 10.1.1.392.4034. doi:10.1016/S0305-0548(99)00146-X.

^ de Rigo, D.; Rizzoli, A. E.; Soncini-Sessa, R.; Weber, E.; Zenesi, P. (2001). "Neuro-dynamic programming for the efficient management of reservoir networks". Proceedings of MODSIM 2001, International Congress on Modelling and Simulation. MODSIM 2001, International Congress on Modelling and Simulation. Canberra, Australia: Modelling and Simulation Society of Australia and New Zealand. doi:10.5281/zenodo.7481. ISBN 0-867405252. |access-date= requires |url= (help)

^ Damas, M.; Salmeron, M.; Diaz, A.; Ortega, J.; Prieto, A.; Olivares, G. (2000). "Genetic algorithms and neuro-dynamic programming: application to water supply networks". Proceedings of 2000 Congress on Evolutionary Computation. 2000 Congress on Evolutionary Computation. La Jolla, California, USA: IEEE. doi:10.1109/CEC.2000.870269. ISBN 0-7803-6375-2.

^ Deng, Geng; Ferris, M.C. (2008). Neuro-dynamic programming for fractionated radiotherapy planning. Springer Optimization and Its Applications. 12. pp. 47–70. CiteSeerX 10.1.1.137.8288. doi:10.1007/978-0-387-73299-2_3. ISBN 978-0-387-73298-5.

^ M. Forouzanfar; H. R. Dajani; V. Z. Groza; M. Bolic & S. Rajan (July 2010). Comparison of Feed-Forward Neural Network Training Algorithms for Oscillometric Blood Pressure Estimation (PDF). 4th Int. Workshop Soft Computing Applications. Arad, Romania: IEEE.

^ de Rigo, D.; Castelletti, A.; Rizzoli, A. E.; Soncini-Sessa, R.; Weber, E. (January 2005). "A selective improvement technique for fastening Neuro-Dynamic Programming in Water Resources Network Management".  In Pavel Zítek (ed.). Proceedings of the 16th IFAC World Congress – IFAC-PapersOnLine. 16th IFAC World Congress. 16. Prague, Czech Republic: IFAC. doi:10.3182/20050703-6-CZ-1902.02172. ISBN 978-3-902661-75-3. Retrieved 30 December 2011.

^ Ferreira, C. (2006). "Designing Neural Networks Using Gene Expression Programming" (PDF). In A. Abraham, B. de Baets, M. Köppen, and B. Nickolay, eds., Applied Soft Computing Technologies: The Challenge of Complexity, pages 517–536, Springer-Verlag.

^ Da, Y.; Xiurun, G. (July 2005).  T. Villmann (ed.). An improved PSO-based ANN with simulated annealing technique. New Aspects in Neurocomputing: 11th European Symposium on Artificial Neural Networks. Elsevier. doi:10.1016/j.neucom.2004.07.002.

^ Wu, J.; Chen, E. (May 2009).  Wang, H.; Shen, Y.; Huang, T.; Zeng, Z. (eds.). A Novel Nonparametric Regression Ensemble for Rainfall Forecasting Using Particle Swarm Optimization Technique Coupled with Artificial Neural Network. 6th International Symposium on Neural Networks, ISNN 2009. Springer. doi:10.1007/978-3-642-01513-7-6. ISBN 978-3-642-01215-0.

^ a b c Ting Qin, et al. "A learning algorithm of CMAC based on RLS." Neural Processing Letters 19.1 (2004): 49–61.

^ Ting Qin, et al. "Continuous CMAC-QRLS and its systolic array." Neural Processing Letters 22.1 (2005): 1–16.

^ Werbos, Paul J. (1994). The Roots of Backpropagation. From Ordered Derivatives to Neural Networks and Political Forecasting. New York, NY: John Wiley & Sons, Inc.

^ Li, Y.; Fu, Y.; Li, H.; Zhang, S. W. (1 June 2009). The Improved Training Algorithm of Back Propagation Neural Network with Self-adaptive Learning Rate. 2009 International Conference on Computational Intelligence and Natural Computing. 1. pp. 73–76. doi:10.1109/CINC.2009.111. ISBN 978-0-7695-3645-3.

^ Ivakhnenko, Alexey Grigorevich (1968). "The group method of data handling – a rival of the method of stochastic approximation". Soviet Automatic Control. 13 (3): 43–55.

^ Ivakhnenko, Alexey (1971). "Polynomial theory of complex systems". IEEE Transactions on Systems, Man and Cybernetics (4) (4): 364–378. doi:10.1109/TSMC.1971.4308320.

^ Kondo, T.; Ueno, J. (2008). "Multi-layered GMDH-type neural network self-selecting optimum neural network architecture and its application to 3-dimensional medical image recognition of blood vessels". International Journal of Innovative Computing, Information and Control. 4 (1): 175–187.

^ Fukushima, K. (1980). "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position". Biol. Cybern. 36 (4): 193–202. doi:10.1007/bf00344251. PMID 7370364.

^ LeCun et al., "Backpropagation Applied to Handwritten Zip Code Recognition," Neural Computation, 1, pp. 541–551, 1989.

^ Yann LeCun (2016). Slides on Deep Learning Online

^ "Unsupervised Feature Learning and Deep Learning Tutorial".

^ Szegedy, Christian; Liu, Wei; Jia, Yangqing; Sermanet, Pierre; Reed, Scott; Anguelov, Dragomir; Erhan, Dumitru; Vanhoucke, Vincent; Rabinovich, Andrew (2014). Going Deeper with Convolutions. Computing Research Repository. p. 1. arXiv:1409.4842. doi:10.1109/CVPR.2015.7298594. ISBN 978-1-4673-6964-0.

^ Ran, Lingyan; Zhang, Yanning; Zhang, Qilin; Yang, Tao (12 June 2017). "Convolutional Neural Network-Based Robot Navigation Using Uncalibrated Spherical Images" (PDF). Sensors. 17 (6): 1341. doi:10.3390/s17061341. ISSN 1424-8220. PMC 5492478. PMID 28604624.

^ Hinton, Geoffrey E.; Krizhevsky, Alex; Wang, Sida D. (2011), "Transforming Auto-Encoders", Lecture Notes in Computer Science, Springer Berlin Heidelberg, pp. 44–51, CiteSeerX 10.1.1.220.5099, doi:10.1007/978-3-642-21735-7_6, ISBN 9783642217340

^ Hochreiter, Sepp; Schmidhuber, Jürgen (1 November 1997). "Long Short-Term Memory". Neural Computation. 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735. ISSN 0899-7667.

^ "Learning Precise Timing with LSTM Recurrent Networks (PDF Download Available)". Crossref Listing of Deleted Dois. 1: 115–143. 2000. doi:10.1162/153244303768966139. Retrieved 13 June 2017.

^ Bayer, Justin; Wierstra, Daan; Togelius, Julian; Schmidhuber, Jürgen (14 September 2009). Evolving Memory Cell Structures for Sequence Learning (PDF). Artificial Neural Networks – ICANN 2009. Lecture Notes in Computer Science. 5769. Springer, Berlin, Heidelberg. pp. 755–764. doi:10.1007/978-3-642-04277-5_76. ISBN 978-3-642-04276-8.

^ Fernández, Santiago; Graves, Alex; Schmidhuber, Jürgen (2007). "Sequence labelling in structured domains with hierarchical recurrent neural networks". In Proc. 20th Int. Joint Conf. On Artificial In℡ligence, Ijcai 2007: 774–779. CiteSeerX 10.1.1.79.1887.

^ Graves, Alex; Fernández, Santiago; Gomez, Faustino (2006). "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks". In Proceedings of the International Conference on Machine Learning, ICML 2006: 369–376. CiteSeerX 10.1.1.75.6306.

^ Graves, Alex; Eck, Douglas; Beringer, Nicole; Schmidhuber, Jürgen (2003). "Biologically Plausible Speech Recognition with LSTM Neural Nets" (PDF). 1st Intl. Workshop on Biologically Inspired Approaches to Advanced Information Technology, Bio-ADIT 2004, Lausanne, Switzerland. pp. 175–184.

^ Fernández, Santiago; Graves, Alex; Schmidhuber, Jürgen (2007). An Application of Recurrent Neural Networks to Discriminative Keyword Spotting. Proceedings of the 17th International Conference on Artificial Neural Networks. ICANN'07. Berlin, Heidelberg: Springer-Verlag. pp. 220–229. ISBN 978-3540746935.

^ Hannun, Awni; Case, Carl; Casper, Jared; Catanzaro, Bryan; Diamos, Greg; Elsen, Erich; Prenger, Ryan; Satheesh, Sanjeev; Sengupta, Shubho (17 December 2014). "Deep Speech: Scaling up end-to-end speech recognition". arXiv:1412.5567 [cs.CL].

^ Sak, Hasim; Senior, Andrew; Beaufays, Francoise (2014). "Long Short-Term Memory recurrent neural network architectures for large scale acoustic modeling" (PDF).

^ Li, Xiangang; Wu, Xihong (15 October 2014). "Constructing Long Short-Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition". arXiv:1410.4281 [cs.CL].

^ Fan, Y.; Qian, Y.; Xie, F.; Soong, F. K. (2014). "TTS synthesis with bidirectional LSTM based Recurrent Neural Networks". Proceedings of the Annual Conference of the International Speech Communication Association, Interspeech: 1964–1968. Retrieved 13 June 2017.

^ Zen, Heiga; Sak, Hasim (2015). "Unidirectional Long Short-Term Memory Recurrent Neural Network with Recurrent Output Layer for Low-Latency Speech Synthesis" (PDF). Google.com. ICASSP. pp. 4470–4474.

^ Fan, Bo; Wang, Lijuan; Soong, Frank K.; Xie, Lei (2015). "Photo-Real Talking Head with Deep Bidirectional LSTM" (PDF). Proceedings of ICASSP.

^ Sak, Haşim; Senior, Andrew; Rao, Kanishka; Beaufays, Françoise; Schalkwyk, Johan (September 2015). "Google voice search: faster and more accurate".

^ Gers, Felix A.; Schmidhuber, Jürgen (2001). "LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages". IEEE Transactions on Neural Networks. 12 (6): 1333–1340. doi:10.1109/72.963769. PMID 18249962.

^ Schmidhuber, Juergen (2018). "Video-based Sign Language Recognition without Temporal Segmentation". arXiv:1801.10111 [cs.CV].

^ Sutskever, L.; Vinyals, O.; Le, Q. (2014). "Sequence to Sequence Learning with Neural Networks" (PDF). NIPS'14 Proceedings of the 27th International Conference on Neural Information Processing Systems. 2: 3104–3112. arXiv:1409.3215. Bibcode:2014arXiv1409.3215S.

^ Jozefowicz, Rafal; Vinyals, Oriol; Schuster, Mike; Shazeer, Noam; Wu, Yonghui (7 February 2016). "Exploring the Limits of Language Modeling". arXiv:1602.02410 [cs.CL].

^ Gillick, Dan; Brunk, Cliff; Vinyals, Oriol; Subramanya, Amarnag (30 November 2015). "Multilingual Language Processing From Bytes". arXiv:1512.00103 [cs.CL].

^ Vinyals, Oriol; Toshev, Alexander; Bengio, Samy; Erhan, Dumitru (17 November 2014). "Show and Tell: A Neural Image Caption Generator". arXiv:1411.4555 [cs.CV].

^ Gallicchio, Claudio; Micheli, Alessio; Pedrelli, Luca (2017). "Deep reservoir computing: A critical experimental analysis". Neurocomputing. 268: 87–99. doi:10.1016/j.neucom.2016.12.089.

^ Gallicchio, Claudio; Micheli, Alessio (2017). "Echo State Property of Deep Reservoir Computing Networks". Cognitive Computation. 9 (3): 337–350. doi:10.1007/s12559-017-9461-9. ISSN 1866-9956.

^ Hinton, G.E. (2009). "Deep belief networks". Scholarpedia. 4 (5): 5947. Bibcode:2009SchpJ...4.5947H. doi:10.4249/scholarpedia.5947.

^ Larochelle, Hugo; Erhan, Dumitru; Courville, Aaron; Bergstra, James; Bengio, Yoshua (2007). An Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation. Proceedings of the 24th International Conference on Machine Learning. ICML '07. New York, NY, USA: ACM. pp. 473–480. CiteSeerX 10.1.1.77.3242. doi:10.1145/1273496.1273556. ISBN 9781595937933.

^ a b Graupe, Daniel (2013). Principles of Artificial Neural Networks. World Scientific. pp. 1–. ISBN 978-981-4522-74-8.

^ A US 5920852 A  D. Graupe," Large memory storage and retrieval (LAMSTAR) network, April 1996

^ D. Graupe, "Principles of Artificial Neural Networks.3rd Edition", World Scientific Publishers, 2013, pp. 203–274.

^ Nigam, Vivek Prakash; Graupe, Daniel (1 January 2004). "A neural-network-based detection of epilepsy". Neurological Research. 26 (1): 55–60. doi:10.1179/016164104773026534. ISSN 0161-6412. PMID 14977058.

^ a b Waxman, Jonathan A.; Graupe, Daniel; Carley, David W. (1 April 2010). "Automated Prediction of Apnea and Hypopnea, Using a LAMSTAR Artificial Neural Network". American Journal of Respiratory and Critical Care Medicine. 181 (7): 727–733. doi:10.1164/rccm.200907-1146oc. ISSN 1073-449X. PMID 20019342.

^ a b Graupe, D.; Graupe, M. H.; Zhong, Y.; Jackson, R. K. (2008). "Blind adaptive filtering for non-invasive extraction of the fetal electrocardiogram and its non-stationarities". Proc. Inst. Mech. Eng. H. 222 (8): 1221–1234. doi:10.1243/09544119jeim417. PMID 19143416.

^ Graupe 2013, pp. 240–253

^ a b Graupe, D.; Abon, J. (2002). "A Neural Network for Blind Adaptive Filtering of Unknown Noise from Speech". Intelligent Engineering Systems Through Artificial Neural Networks. 12: 683–688. Retrieved 14 June 2017.

^ D. Graupe, "Principles of Artificial Neural Networks.3rd Edition", World Scientific Publishers", 2013, pp. 253–274.

^ Girado, J. I.; Sandin, D. J.; DeFanti, T. A. (2003). "Real-time camera-based face detection using a modified LAMSTAR neural network system". Proc. SPIE 5015, Applications of Artificial Neural Networks in Image Processing VIII. Applications of Artificial Neural Networks in Image Processing VIII. 5015: 36–46. Bibcode:2003SPIE.5015...36G. doi:10.1117/12.477405.

^ Venkatachalam, V; Selvan, S. (2007). "Intrusion Detection using an Improved Competitive Learning Lamstar Network". International Journal of Computer Science and Network Security. 7 (2): 255–263.

^ Graupe, D.; Smollack, M. (2007). "Control of unstable nonlinear and nonstationary systems using LAMSTAR neural networks". ResearchGate. Proceedings of 10th IASTED on Intelligent Control, Sect.592. pp. 141–144. Retrieved 14 June 2017.

^ Graupe, Daniel (7 July 2016). Deep Learning Neural Networks: Design and Case Studies. World Scientific Publishing Co Inc. pp. 57–110. ISBN 978-981-314-647-1.

^ Graupe, D.; Kordylewski, H. (August 1996). Network based on SOM (Self-Organizing-Map) modules combined with statistical decision tools. Proceedings of the 39th Midwest Symposium on Circuits and Systems. 1. pp. 471–474 vol.1. doi:10.1109/mwscas.1996.594203. ISBN 978-0-7803-3636-0.

^ Graupe, D.; Kordylewski, H. (1 March 1998). "A Large Memory Storage and Retrieval Neural Network for Adaptive Retrieval and Diagnosis". International Journal of Software Engineering and Knowledge Engineering. 08 (1): 115–138. doi:10.1142/s0218194098000091. ISSN 0218-1940.

^ Kordylewski, H.; Graupe, D; Liu, K. (2001). "A novel large-memory neural network as an aid in medical diagnosis applications". IEEE Transactions on Information Technology in Biomedicine. 5 (3): 202–209. doi:10.1109/4233.945291.

^ Schneider, N.C.; Graupe (2008). "A modified LAMSTAR neural network and its applications". International Journal of Neural Systems. 18 (4): 331–337. doi:10.1142/s0129065708001634. PMID 18763732.

^ Graupe 2013, p. 217

^ a b c d Vincent, Pascal; Larochelle, Hugo; Lajoie, Isabelle; Bengio, Yoshua; Manzagol, Pierre-Antoine (2010). "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion". The Journal of Machine Learning Research. 11: 3371–3408.

^ Ballard, Dana H. (1987). "Modular learning in neural networks" (PDF). Proceedings of AAAI. pp. 279–284. Archived from the original (PDF) on 16 October 2015.

^ a b c Deng, Li; Yu, Dong; Platt, John (2012). "Scalable stacking and learning for building deep architectures" (PDF). 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP): 2133–2136.

^ a b Deng, Li; Yu, Dong (2011). "Deep Convex Net: A Scalable Architecture for Speech Pattern Classification" (PDF). Proceedings of the Interspeech: 2285–2288.

^ David, Wolpert (1992). "Stacked generalization". Neural Networks. 5 (2): 241–259. CiteSeerX 10.1.1.133.8090. doi:10.1016/S0893-6080(05)80023-1.

^ Bengio, Y. (15 November 2009). "Learning Deep Architectures for AI". Foundations and Trends in Machine Learning. 2 (1): 1–127. CiteSeerX 10.1.1.701.9550. doi:10.1561/2200000006. ISSN 1935-8237.

^ Hutchinson, Brian; Deng, Li; Yu, Dong (2012). "Tensor deep stacking networks". IEEE Transactions on Pattern Analysis and Machine Intelligence. 1–15 (8): 1944–1957. doi:10.1109/tpami.2012.268.

^ Hinton, Geoffrey; Salakhutdinov, Ruslan (2006). "Reducing the Dimensionality of Data with Neural Networks". Science. 313 (5786): 504–507. Bibcode:2006Sci...313..504H. doi:10.1126/science.1127647. PMID 16873662.

^ Dahl, G.; Yu, D.; Deng, L.; Acero, A. (2012). "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition". IEEE Transactions on Audio, Speech, and Language Processing. 20 (1): 30–42. CiteSeerX 10.1.1.227.8990. doi:10.1109/tasl.2011.2134090.

^ Mohamed, Abdel-rahman; Dahl, George; Hinton, Geoffrey (2012). "Acoustic Modeling Using Deep Belief Networks". IEEE Transactions on Audio, Speech, and Language Processing. 20 (1): 14–22. CiteSeerX 10.1.1.338.2670. doi:10.1109/tasl.2011.2109382.

^ Courville, Aaron; Bergstra, James; Bengio, Yoshua (2011). "A Spike and Slab Restricted Boltzmann Machine" (PDF). JMLR: Workshop and Conference Proceeding. 15: 233–241.

^ a b Courville, Aaron; Bergstra, James; Bengio, Yoshua (2011). "Unsupervised Models of Images by Spike-and-Slab RBMs". Proceedings of the 28th International Conference on Machine Learning (PDF). 10. pp. 1–8.

^ Mitchell, T; Beauchamp, J (1988). "Bayesian Variable Selection in Linear Regression". Journal of the American Statistical Association. 83 (404): 1023–1032. doi:10.1080/01621459.1988.10478694.

^ Hinton, Geoffrey; Salakhutdinov, Ruslan (2009). "Efficient Learning of Deep Boltzmann Machines" (PDF). 3: 448–455.

^ Larochelle, Hugo; Bengio, Yoshua; Louradour, Jerdme; Lamblin, Pascal (2009). "Exploring Strategies for Training Deep Neural Networks". The Journal of Machine Learning Research. 10: 1–40.

^ Coates, Adam; Carpenter, Blake (2011). "Text Detection and Character Recognition in Scene Images with Unsupervised Feature Learning" (PDF): 440–445.

^ Lee, Honglak; Grosse, Roger (2009). Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. Proceedings of the 26th Annual International Conference on Machine Learning. pp. 1–8. CiteSeerX 10.1.1.149.6800. doi:10.1145/1553374.1553453. ISBN 9781605585161.

^ Lin, Yuanqing; Zhang, Tong (2010). "Deep Coding Network" (PDF). Advances in Neural . . .: 1–9.

^ Ranzato, Marc Aurelio; Boureau, Y-Lan (2007). "Sparse Feature Learning for Deep Belief Networks" (PDF). Advances in Neural Information Processing Systems. 23: 1–8.

^ Socher, Richard; Lin, Clif (2011). "Parsing Natural Scenes and Natural Language with Recursive Neural Networks" (PDF). Proceedings of the 26th International Conference on Machine Learning.

^ Taylor, Graham; Hinton, Geoffrey (2006). "Modeling Human Motion Using Binary Latent Variables" (PDF). Advances in Neural Information Processing Systems.

^ Vincent, Pascal; Larochelle, Hugo (2008). Extracting and composing robust features with denoising autoencoders. Proceedings of the 25th International Conference on Machine Learning – ICML '08. pp. 1096–1103. CiteSeerX 10.1.1.298.4083. doi:10.1145/1390156.1390294. ISBN 9781605582054.

^ Kemp, Charles; Perfors, Amy; Tenenbaum, Joshua (2007). "Learning overhypotheses with hierarchical Bayesian models". Developmental Science. 10 (3): 307–21. CiteSeerX 10.1.1.141.5560. doi:10.1111/j.1467-7687.2007.00585.x. PMID 17444972.

^ Xu, Fei; Tenenbaum, Joshua (2007). "Word learning as Bayesian inference". Psychol. Rev. 114 (2): 245–72. CiteSeerX 10.1.1.57.9649. doi:10.1037/0033-295X.114.2.245. PMID 17500627.

^ Chen, Bo; Polatkan, Gungor (2011). "The Hierarchical Beta Process for Convolutional Factor Analysis and Deep Learning" (PDF). Proceedings of the 28th International Conference on International Conference on Machine Learning. Omnipress. pp. 361–368. ISBN 978-1-4503-0619-5.

^ Fei-Fei, Li; Fergus, Rob (2006). "One-shot learning of object categories". IEEE Transactions on Pattern Analysis and Machine Intelligence. 28 (4): 594–611. CiteSeerX 10.1.1.110.9024. doi:10.1109/TPAMI.2006.79. PMID 16566508.

^ Rodriguez, Abel; Dunson, David (2008). "The Nested Dirichlet Process". Journal of the American Statistical Association. 103 (483): 1131–1154. CiteSeerX 10.1.1.70.9873. doi:10.1198/016214508000000553.

^ Ruslan, Salakhutdinov; Joshua, Tenenbaum (2012). "Learning with Hierarchical-Deep Models". IEEE Transactions on Pattern Analysis and Machine Intelligence. 35 (8): 1958–71. CiteSeerX 10.1.1.372.909. doi:10.1109/TPAMI.2012.269. PMID 23787346.

^ a b Chalasani, Rakesh; Principe, Jose (2013). "Deep Predictive Coding Networks". arXiv:1301.3541 [cs.LG].

^ Hinton, Geoffrey E. (1984). "Distributed representations". Archived from the original on 2 May 2016.

^ S. Das, C.L. Giles, G.Z. Sun, "Learning Context Free Grammars: Limitations of a Recurrent Neural Network with an External Stack Memory," Proc. 14th Annual Conf. of the Cog. Sci. Soc., p. 79, 1992.

^ Mozer, M. C.; Das, S. (1993). "A connectionist symbol manipulator that discovers the structure of context-free languages". NIPS 5. pp. 863–870.

^ Schmidhuber, J. (1992). "Learning to control fast-weight memories: An alternative to recurrent nets". Neural Computation. 4 (1): 131–139. doi:10.1162/neco.1992.4.1.131.

^ Gers, F.; Schraudolph, N.; Schmidhuber, J. (2002). "Learning precise timing with LSTM recurrent networks" (PDF). JMLR. 3: 115–143.

^ Jürgen Schmidhuber (1993). "An introspective network that can learn to run its own weight change algorithm". In Proc. of the Intl. Conf. on Artificial Neural Networks, Brighton. IEE. pp. 191–195.

^ Hochreiter, Sepp; Younger, A. Steven; Conwell, Peter R. (2001). "Learning to Learn Using Gradient Descent". ICANN. 2130: 87–94. CiteSeerX 10.1.1.5.323.

^ Schmidhuber, Juergen (2015). "Learning to Transduce with Unbounded Memory". arXiv:1506.02516 [cs.NE].

^ Schmidhuber, Juergen (2014). "Neural Turing Machines". arXiv:1410.5401 [cs.NE].

^ Burgess, Matt. "DeepMind's AI learned to ride the London Underground using human-like reason and memory". WIRED UK. Retrieved 19 October 2016.

^ "DeepMind AI 'Learns' to Navigate London Tube". PCMAG. Retrieved 19 October 2016.

^ Mannes, John. "DeepMind's differentiable neural computer helps you navigate the subway with its memory". TechCrunch. Retrieved 19 October 2016.

^ Graves, Alex; Wayne, Greg; Reynolds, Malcolm; Harley, Tim; Danihelka, Ivo; Grabska-Barwińska, Agnieszka; Colmenarejo, Sergio Gómez; Grefenstette, Edward; Ramalho, Tiago (12 October 2016). "Hybrid computing using a neural network with dynamic external memory". Nature. 538 (7626): 471–476. Bibcode:2016Natur.538..471G. doi:10.1038/nature20101. ISSN 1476-4687. PMID 27732574.

^ "Differentiable neural computers | DeepMind". DeepMind. Retrieved 19 October 2016.

^ Atkeson, Christopher G.; Schaal, Stefan (1995). "Memory-based neural networks for robot learning". Neurocomputing. 9 (3): 243–269. doi:10.1016/0925-2312(95)00033-6.

^ Salakhutdinov, Ruslan, and Geoffrey Hinton. "Semantic hashing." International Journal of Approximate Reasoning 50.7 (2009): 969–978.

^ Le, Quoc V.; Mikolov, Tomas (2014). "Distributed representations of sentences and documents". arXiv:1405.4053 [cs.CL].

^ Schmidhuber, Juergen (2014). "Memory Networks". arXiv:1410.3916 [cs.AI].

^ Schmidhuber, Juergen (2015). "End-To-End Memory Networks". arXiv:1503.08895 [cs.NE].

^ Schmidhuber, Juergen (2015). "Large-scale Simple Question Answering with Memory Networks". arXiv:1506.02075 [cs.LG].

^ "AI device identifies objects at the speed of light: The 3D-printed artificial neural network can be used in medicine, robotics and security". ScienceDaily. Retrieved 8 August 2018.

^ Schmidhuber, Juergen (2015). "Pointer Networks". arXiv:1506.03134 [stat.ML].

^ Schmidhuber, Juergen (2015). "Neural Random-Access Machines". arXiv:1511.06392 [cs.LG].

^ Kalchbrenner, N.; Blunsom, P. (2013). "Recurrent continuous translation models". EMNLP'2013.

^ Sutskever, I.; Vinyals, O.; Le, Q. V. (2014). "Sequence to sequence learning with neural networks" (PDF). NIPS'2014.

^ Schmidhuber, Juergen (2014). "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation". arXiv:1406.1078 [cs.CL].

^ Schmidhuber, Juergen; Courville, Aaron; Bengio, Yoshua (2015). "Describing Multimedia Content using Attention-based Encoder--Decoder Networks". IEEE Transactions on Multimedia. 17 (11): 1875–1886. arXiv:1507.01053. doi:10.1109/TMM.2015.2477044.

^ Scholkopf, B; Smola, Alexander (1998). "Nonlinear component analysis as a kernel eigenvalue problem". Neural Computation. (44) (5): 1299–1319. CiteSeerX 10.1.1.53.8911. doi:10.1162/089976698300017467.

^ Cho, Youngmin (2012). "Kernel Methods for Deep Learning" (PDF): 1–9.

^ Deng, Li; Tur, Gokhan; He, Xiaodong; Hakkani-Tür, Dilek (1 December 2012). "Use of Kernel Deep Convex Networks and End-To-End Learning for Spoken Language Understanding". Microsoft Research.

^ Zoph, Barret; Le, Quoc V. (4 November 2016). "Neural Architecture Search with Reinforcement Learning". arXiv:1611.01578 [cs.LG].

^ Zissis, Dimitrios (October 2015). "A cloud based architecture capable of perceiving and predicting multiple vessel behaviour". Applied Soft Computing. 35: 652–661. doi:10.1016/j.asoc.2015.07.002.

^ Roman M. Balabin; Ekaterina I. Lomakina (2009). "Neural network approach to quantum-chemistry data: Accurate prediction of density functional theory energies". J. Chem. Phys. 131 (7): 074104. Bibcode:2009JChPh.131g4104B. doi:10.1063/1.3206326. PMID 19708729.

^ Silver, David, et al. "Mastering the game of Go with deep neural networks and tree search." nature 529.7587 (2016): 484.

^ Sengupta, Nandini; Sahidullah, Md; Saha, Goutam (August 2016). "Lung sound classification using cepstral-based statistical features". Computers in Biology and Medicine. 75 (1): 118–129. doi:10.1016/j.compbiomed.2016.05.013. PMID 27286184.

^ Choy, Christopher B., et al. "3d-r2n2: A unified approach for single and multi-view 3d object reconstruction." European conference on computer vision. Springer, Cham, 2016.

^ French, Jordan (2016). "The time traveller's CAPM". Investment Analysts Journal. 46 (2): 81–96. doi:10.1080/10293523.2016.1255469.

^ Schechner, Sam (15 June 2017). "Facebook Boosts A.I. to Block Terrorist Propaganda". Wall Street Journal. ISSN 0099-9660. Retrieved 16 June 2017.

^ Ganesan, N. "Application of Neural Networks in Diagnosing Cancer Disease Using Demographic Data" (PDF). International Journal of Computer Applications.

^ Bottaci, Leonardo. "Artificial Neural Networks Applied to Outcome Prediction for Colorectal Cancer Patients in Separate Institutions" (PDF). The Lancet.

^ Alizadeh, Elaheh; Lyons, Samanthe M; Castle, Jordan M; Prasad, Ashok (2016). "Measuring systematic changes in invasive cancer cell shape using Zernike moments". Integrative Biology. 8 (11): 1183–1193. doi:10.1039/C6IB00100A. PMID 27735002.

^ Lyons, Samanthe (2016). "Changes in cell shape are correlated with metastatic potential in murine". Biology Open. 5 (3): 289–299. doi:10.1242/bio.013409. PMC 4810736. PMID 26873952.

^ Nabian, Mohammad Amin; Meidani, Hadi (28 August 2017). "Deep Learning for Accelerated Reliability Analysis of Infrastructure Networks". Computer-Aided Civil and Infrastructure Engineering. 33 (6): 443–458. arXiv:1708.08551. doi:10.1111/mice.12359.

^ Nabian, Mohammad Amin; Meidani, Hadi (2018). "Accelerating Stochastic Assessment of Post-Earthquake Transportation Network Connectivity via Machine-Learning-Based Surrogates". Transportation Research Board 97th Annual Meeting.

^ Díaz, E.; Brotons, V.; Tomás, R. (September 2018). "Use of artificial neural networks to predict 3-D elastic settlement of foundations on soils with inclined bedrock". Soils and Foundations. 58 (6): 1414–1422. doi:10.1016/j.sandf.2018.08.001. ISSN 0038-0806.

^ null null (1 April 2000). "Artificial Neural Networks in Hydrology. I: Preliminary Concepts". Journal of Hydrologic Engineering. 5 (2): 115–123. CiteSeerX 10.1.1.127.3861. doi:10.1061/(ASCE)1084-0699(2000)5:2(115).

^ null null (1 April 2000). "Artificial Neural Networks in Hydrology. II: Hydrologic Applications". Journal of Hydrologic Engineering. 5 (2): 124–137. doi:10.1061/(ASCE)1084-0699(2000)5:2(124).

^ Peres, D. J.; Iuppa, C.; Cavallaro, L.; Cancelliere, A.; Foti, E. (1 October 2015). "Significant wave height record extension by neural networks and reanalysis wind data". Ocean Modelling. 94: 128–140. Bibcode:2015OcMod..94..128P. doi:10.1016/j.ocemod.2015.08.002.

^ Dwarakish, G. S.; Rakshith, Shetty; Natesan, Usha (2013). "Review on Applications of Neural Network in Coastal Engineering". Artificial Intelligent Systems and Machine Learning. 5 (7): 324–331.

^ Ermini, Leonardo; Catani, Filippo; Casagli, Nicola (1 March 2005). "Artificial Neural Networks applied to landslide susceptibility assessment". Geomorphology. Geomorphological hazard and human impact in mountain environments. 66 (1): 327–343. Bibcode:2005Geomo..66..327E. doi:10.1016/j.geomorph.2004.09.025.

^ Nix, R.; Zhang, J. (May 2017). "Classification of Android apps and malware using deep neural networks". 2017 International Joint Conference on Neural Networks (IJCNN): 1871–1878. doi:10.1109/IJCNN.2017.7966078. ISBN 978-1-5090-6182-2.

^ "Machine Learning: Detecting malicious domains with Tensorflow". The Coruscan Project.

^ "Detecting Malicious URLs". The systems and networking group at UCSD.

^ "DeepExploit: a fully automated penetration test tool". Isao Takaesu | GitHub.

^ Homayoun, Sajad; Ahmadzadeh, Marzieh; Hashemi, Sattar; Dehghantanha, Ali; Khayami, Raouf (2018),  Dehghantanha, Ali; Conti, Mauro; Dargahi, Tooska (eds.), "BoTShark: A Deep Learning Approach for Botnet Traffic Detection", Cyber Threat Intelligence, Advances in Information Security, Springer International Publishing, pp. 137–153, doi:10.1007/978-3-319-73951-9_7, ISBN 9783319739519

^ and (January 1994). "Credit card fraud detection with a neural-network". 1994 Proceedings of the Twenty-Seventh Hawaii International Conference on System Sciences. 3: 621–630. doi:10.1109/HICSS.1994.323314. ISBN 978-0-8186-5090-1.

^ Forrest MD (April 2015). "Simulation of alcohol action upon a detailed Purkinje neuron model and a simpler surrogate model that runs >400 times faster". BMC Neuroscience. 16 (27): 27. doi:10.1186/s12868-015-0162-6. PMC 4417229. PMID 25928094.

^ Siegelmann, H.T.; Sontag, E.D. (1991). "Turing computability with neural nets" (PDF). Appl. Math. Lett. 4 (6): 77–80. doi:10.1016/0893-9659(91)90080-F.

^ Balcázar, José (July 1997). "Computational Power of Neural Networks: A Kolmogorov Complexity Characterization". IEEE Transactions on Information Theory. 43 (4): 1175–1183. CiteSeerX 10.1.1.411.7782. doi:10.1109/18.605580. Retrieved 3 November 2014.

^ Crick, Francis (1989). "The recent excitement about neural networks". Nature. 337 (6203): 129–132. Bibcode:1989Natur.337..129C. doi:10.1038/337129a0. PMID 2911347.

^ Adrian, Edward D. (1926). "The impulses produced by sensory nerve endings". The Journal of Physiology. 61 (1): 49–72. doi:10.1113/jphysiol.1926.sp002273. PMC 1514809. PMID 16993776.

^ Dewdney, A. K. (1 April 1997). Yes, we have no neutrons: an eye-opening tour through the twists and turns of bad science. Wiley. p. 82. ISBN 978-0-471-10806-1.

^ D. J. Felleman and D. C. Van Essen, "Distributed hierarchical processing in the primate cerebral cortex," Cerebral Cortex, 1, pp. 1–47, 1991.

^ J. Weng, "Natural and Artificial Intelligence: Introduction to Computational Brain-Mind," BMI Press, ISBN 978-0985875725, 2012.

^ a b Edwards, Chris (25 June 2015). "Growing pains for deep learning". Communications of the ACM. 58 (7): 14–16. doi:10.1145/2771283.

^ Schmidhuber, Jürgen (2015). "Deep learning in neural networks: An overview". Neural Networks. 61: 85–117. arXiv:1404.7828. doi:10.1016/j.neunet.2014.09.003. PMID 25462637.

^ "A Survey of FPGA-based Accelerators for Convolutional Neural Networks", NCAA, 2018

^ Cade Metz (18 May 2016). "Google Built Its Very Own Chips to Power Its AI Bots". Wired.

^ NASA – Dryden Flight Research Center – News Room: News Releases: NASA NEURAL NETWORK PROJECT PASSES MILESTONE. Nasa.gov. Retrieved on 2013-11-20.

^ "Roger Bridgman's defence of neural networks". Archived from the original on 19 March 2012. Retrieved 12 July 2010.

^ "Scaling Learning Algorithms towards {AI} – LISA – Publications – Aigaion 2.0".

^ Sun and Bookman (1990)

^ Tahmasebi; Hezarkhani (2012). "A hybrid neural networks-fuzzy logic-genetic algorithm for grade estimation". Computers & Geosciences. 42: 18–27. Bibcode:2012CG.....42...18T. doi:10.1016/j.cageo.2012.02.004. PMC 4268588. PMID 25540468.


Bibliography[edit]

Bhadeshia H. K. D. H. (1999). "Neural Networks in Materials Science" (PDF). ISIJ International. 39 (10): 966–979. doi:10.2355/isijinternational.39.966.
M., Bishop, Christopher (1995). Neural networks for pattern recognition. Clarendon Press. ISBN 978-0198538493. OCLC 33101074.
Borgelt, Christian (2003). Neuro-Fuzzy-Systeme : von den Grundlagen künstlicher Neuronaler Netze zur Kopplung mit Fuzzy-Systemen. Vieweg. ISBN 9783528252656. OCLC 76538146.
Cybenko, G.V. (2006). "Approximation by Superpositions of a Sigmoidal function".  In van Schuppen, Jan H. (ed.). Mathematics of Control, Signals, and Systems. Springer International. pp. 303–314. PDF
Dewdney, A. K. (1997). Yes, we have no neutrons : an eye-opening tour through the twists and turns of bad science. New York: Wiley. ISBN 9780471108061. OCLC 35558945.
Duda, Richard O.; Hart, Peter Elliot; Stork, David G. (2001). Pattern classification (2 ed.). Wiley. ISBN 978-0471056690. OCLC 41347061.
Egmont-Petersen, M.; de Ridder, D.; Handels, H. (2002). "Image processing with neural networks – a review". Pattern Recognition. 35 (10): 2279–2301. CiteSeerX 10.1.1.21.5444. doi:10.1016/S0031-3203(01)00178-9.
Fahlman, S.; Lebiere, C (1991). "The Cascade-Correlation Learning Architecture" (PDF).
created for National Science Foundation, Contract Number EET-8716324, and Defense Advanced Research Projects Agency (DOD), ARPA Order No. 4976 under Contract F33615-87-C-1499.
Gurney, Kevin (1997). An introduction to neural networks. UCL Press. ISBN 978-1857286731. OCLC 37875698.
Haykin, Simon S. (1999). Neural networks : a comprehensive foundation. Prentice Hall. ISBN 978-0132733502. OCLC 38908586.
Hertz, J.; Palmer, Richard G.; Krogh, Anders S. (1991). Introduction to the theory of neural computation. Addison-Wesley. ISBN 978-0201515602. OCLC 21522159.
Information theory, inference, and learning algorithms. Cambridge University Press. 25 September 2003. ISBN 9780521642989. OCLC 52377690.
Kruse, Rudolf; Borgelt, Christian; Klawonn, F.; Moewes, Christian; Steinbrecher, Matthias; Held, Pascal (2013). Computational intelligence : a methodological introduction. Springer. ISBN 9781447150121. OCLC 837524179.
Lawrence, Jeanette (1994). Introduction to neural networks : design, theory and applications. California Scientific Software. ISBN 978-1883157005. OCLC 32179420.
MacKay, David, J.C. (2003). Information Theory, Inference, and Learning Algorithms (PDF). Cambridge University Press. ISBN 9780521642989.
Masters, Timothy (1994). Signal and image processing with neural networks : a C++ sourcebook. J. Wiley. ISBN 978-0471049630. OCLC 29877717.
Ripley, Brian D. (2007). Pattern Recognition and Neural Networks. Cambridge University Press. ISBN 978-0-521-71770-0.
Siegelmann, H.T.; Sontag, Eduardo D. (1994). "Analog computation via neural networks" (PDF). Theoretical Computer Science. 131 (2): 331–360. doi:10.1016/0304-3975(94)90178-3.
Smith, Murray (1993). Neural networks for statistical modeling. Van Nostrand Reinhold. ISBN 978-0442013103. OCLC 27145760.
Wasserman, Philip D. (1993). Advanced methods in neural computing. Van Nostrand Reinhold. ISBN 978-0442004613. OCLC 27429729.

vteProcessor technologiesModels
Turing machine
Universal
Post–Turing
Quantum
Belt machine
Stack machine
Finite-state machine
with datapath
Hierarchical
Queue automaton
Register machines
Counter
Pointer
Random-access
Random-access stored program
Architecture
Von Neumann
Harvard
modified
Dataflow
Transport-triggered
Cellular
Endianness
Memory access
NUMA
HUMA
Load/store
Register/memory
Cache hierarchy
Memory hierarchy
Virtual memory
Secondary storage
Heterogeneous
Fabric
Multiprocessing
Cognitive
Neuromorphic
Instruction setarchitecturesTypes
CISC
RISC
Application-specific
EDGE
TRIPS
VLIW
EPIC
MISC
OISC
NISC
ZISC
comparison
addressing modes

x86
ARM
MIPS
Power ISA
SPARC
Itanium
Unicore
MicroBlaze
RISC-V
othersExecutionInstruction pipelining
Pipeline stall
Operand forwarding
Classic RISC pipeline
Hazards
Data dependency
Structural
Control
False sharing
Out-of-order
Tomasulo algorithm
Reservation station
Re-order buffer
Register renaming
Speculative
Branch prediction
Memory dependence prediction
ParallelismLevel
Bit
Bit-serial
Word
Instruction
Pipelining
Scalar
Superscalar
Task
Thread
Process
Data
Vector
Memory
Distributed
Multithreading
Temporal
Simultaneous
Hyperthreading
Speculative
Preemptive
Cooperative
Flynn's taxonomy
SISD
SIMD
SWAR
SIMT
MISD
MIMD
SPMD
Processorperformance
Transistor count
Instructions per cycle (IPC)
Cycles per instruction (CPI)
Instructions per second (IPS)
Floating-point operations per second (FLOPS)
Transactions per second (TPS)
Synaptic updates per second (SUPS)
Performance per watt (PPW)
Cache performance metrics
Computer performance by orders of magnitude
Types
Central processing unit (CPU)
Graphics processing unit (GPU)
GPGPU
Vector
Barrel
Stream
Coprocessor
ASIC
FPGA
CPLD
Multi-chip module (MCM)
System in package (SiP)
By application
Microprocessor
Microcontroller
Mobile
Notebook
Ultra-low-voltage
ASIP
Systemson chip
System on a chip (SoC)
Multiprocessor (MPSoC)
Programmable (PSoC)
Network on a chip (NoC)
Hardwareaccelerators
AI accelerator
Vision processing unit (VPU)
Physics processing unit (PPU)
Digital signal processor (DSP)
Tensor processing unit (TPU)
Secure cryptoprocessor
Network processor
Baseband processor

Word size
1-bit
2-bit
4-bit
8-bit
16-bit
32-bit
48-bit
64-bit
128-bit
256-bit
512-bit
others
variable
Core count
Single-core
Multi-core
Manycore
Heterogeneous architecture
Components
Core
Cache
CPU cache
replacement policies
coherence
Bus
Clock rate
FIFO
Functional units
Arithmetic logic unit (ALU)
Address generation unit (AGU)
Floating-point unit (FPU)
Memory management unit (MMU)
Load–store unit
Translation lookaside buffer (TLB)
Logic
Combinational
Sequential
Glue
Logic gate
Quantum
Array
Registers
Processor register
Register file
Memory buffer
Program counter
Stack
Control unit
Instruction unit
Data buffer
Write buffer
Microcode ROM
Counter
Datapath
Multiplexer
Demultiplexer
Adder
Multiplier
CPU
Binary decoder
Address decoder
Sum addressed decoder
Barrel shifter
Circuitry
Integrated circuit
3D
Mixed-signal
Power management
Boolean
Digital
Analog
Quantum
Switch

Powermanagement
PMU
APM
ACPI
Dynamic frequency scaling
Dynamic voltage scaling
Clock gating
Performance per watt (PPW)
Related
History of general-purpose CPUs
Microprocessor chronology
Processor design
Digital electronics
Hardware security module




NervanaTypeSubsidiary of IntelIndustryArtificial intelligenceFounded2014HeadquartersSan Diego, California, U.S. and Palo Alto, California, U.S.
Nervana Systems is an artificial intelligence software company based in San Diego, California, and Palo Alto, California.[1]  The company provides a full-stack software-as-a-service platform called Nervana Cloud that enables businesses to develop custom deep learning software.[2] On August 9, 2016, it was acquired by Intel, for an estimated $408 million.[3][4]

Contents

1 Deep learning framework
2 Nervana Cloud
3 History
4 References


Deep learning framework[edit]
The company’s (now discontinued) open source deep learning framework is called neon.[5] Neon –  which the company says outperforms rival frameworks such as Caffe, Theano, Torch, and TensorFlow[5] – achieves its performance advantage through assembler-level optimization, multi-GPU support, and use of an algorithm called Winograd for computing convolutions, which are common mathematical operations in the deep learning process.[6]

Nervana Cloud[edit]
Nervana Cloud,[7] announced in February 2016, is based on Neon, and runs on Nvidia Titan X GPUs today, but Nervana is developing a custom application-specific integrated circuit (ASIC) called the Nervana Engine that is optimized for deep learning and that Nervana says will perform 10x better than Nvidia Maxwell architecture GPUs.[8] The Nervana Engine will achieve greater compute density by implementing only those design elements that are necessary to support deep learning algorithms and ignoring legacy elements specific to graphics processing.[9]

History[edit]
Nervana was founded in 2014 by CEO Naveen Rao, CTO Amir Khosrowshahi (Dara Khosrowshahi's cousin[10]), and VP Algorithms Arjun Bansal.[11] Nervana has raised $28 million in funding.[1] In June 2015, Nervana raised $20.5 million in series A funding led by Data Collective with participation from Allen & Company, AME Cloud Ventures, Playground Global, the CME Group, Draper Fisher Jurvetson, Fuel Capital, Lux Capital, and Omidyar Network.[12] It was estimated to have only 48 employees when it was acquired by Intel in 2016.[3]

References[edit]


^ a b "Nervana Systems Puts Deep Learning AI in the Cloud". IEEE Spectrum: Technology, Engineering, and Science News. Retrieved 2016-06-22..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ "Nervana Systems Brings Deep Learning to the Masses". Fortune. 2016-02-29. Retrieved 2016-06-22.

^ a b Ina Fried (August 9, 2016). "Intel is paying more than $400 million to buy deep-learning startup Nervana Systems". Recode. Retrieved August 12, 2016.

^ Jordan Novet (August 9, 2016). "Intel acquires deep learning startup Nervana for more than $350 million". Venture Beat. Missing or empty |url= (help); |access-date= requires |url= (help)

^ a b "Nervana open-sources its deep-learning software, says it outperforms Facebook, Nvidia tools". VentureBeat. Retrieved 2016-06-22.

^ ""Not so fast, FFT": Winograd - Nervana". Nervana. 2016-03-03. Retrieved 2016-06-22.

^ "Nervana's cloud platform makes deep learning more widely available". Retrieved 2016-06-24.

^ "Nervana Engine delivers deep learning at ludicrous speed! - Nervana". Nervana. 2016-05-18. Retrieved 2016-06-22.

^ Patterson, Steven Max. "Startup Nervana joins Google in building hardware tailored for neural networks". Network World. Retrieved 2016-06-22.

^ Hackett, Robert (November 17, 2017). "Uber's CEO Comes From What May Be the World's Most Techie Family". Fortune.

^ "Deep Learning at Scale: Q&A with Naveen Rao, Nervana Systems". re-work.co. Retrieved 2016-06-22.

^ "Deep learning startup Nervana raises $20.5M". VentureBeat. Retrieved 2016-06-22.





Machine learning anddata mining
Problems
Classification
Clustering
Regression
Anomaly detection
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction


Supervised learning.mw-parser-output .nobold{font-weight:normal}(classification • regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)


Clustering
BIRCH
CURE
Hierarchical
k-means
Expectation–maximization (EM)
DBSCAN
OPTICS
Mean-shift


Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
t-SNE


Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov


Anomaly detection
k-NN
Local outlier factor


Artificial neural networks
Autoencoder
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net


Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)


Theory
Bias–variance dilemma
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory


Machine-learning venues
NIPS
ICML
ML
JMLR
ArXiv:cs.LG


Glossary of artificial intelligence
Glossary of artificial intelligence


Related articles
List of datasets for machine-learning research
Outline of machine learning


 Machine learning portalvte
DeepDream is a computer vision program created by Google engineer Alexander Mordvintsev which uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dream-like hallucinogenic appearance in the deliberately over-processed images.[1][2][3]
Google's program popularized the term (deep) "dreaming" to refer to the generation of images that produce desired activations in a trained deep network, and the term now refers to a collection of related approaches.

Contents

1 History
2 Process
3 Usage
4 See also
5 References
6 External links


History[edit]
The DeepDream software originated in a deep convolutional network codenamed "Inception" after the film of the same name,[1][2][3] was developed for the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) in 2014[3] and released in July 2015.
The dreaming idea and name became popular on the internet in 2015 thanks to Google's DeepDream program.  The idea dates from early in the history of neural networks,[4] and similar methods have been used to synthesize visual textures.[5]
Related visualization ideas were developed (prior to Google's work) by several research groups.[6][7]
After Google published their techniques and made their code open source,[8] a number of tools in the form of web services, mobile applications, and desktop software appeared on the market to enable users to transform their own photos.[9]

Process[edit]
.mw-parser-output .tmulti .thumbinner{display:flex;flex-direction:column}.mw-parser-output .tmulti .trow{display:flex;flex-direction:row;clear:left;flex-wrap:wrap;width:100%;box-sizing:border-box}.mw-parser-output .tmulti .tsingle{margin:1px;float:left}.mw-parser-output .tmulti .theader{clear:both;font-weight:bold;text-align:center;align-self:center;background-color:transparent;width:100%}.mw-parser-output .tmulti .thumbcaption{text-align:left;background-color:transparent}.mw-parser-output .tmulti .text-align-left{text-align:left}.mw-parser-output .tmulti .text-align-right{text-align:right}.mw-parser-output .tmulti .text-align-center{text-align:center}@media all and (max-width:720px){.mw-parser-output .tmulti .thumbinner{width:100%!important;box-sizing:border-box;max-width:none!important;align-items:center}.mw-parser-output .tmulti .trow{justify-content:center}.mw-parser-output .tmulti .tsingle{float:none!important;max-width:100%!important;box-sizing:border-box;text-align:center}.mw-parser-output .tmulti .thumbcaption{text-align:center}}The original image (top) after applying ten (middle) and fifty (bottom) iterations DeepDream, the network having been trained to perceive dogs
The software is designed to detect faces and other patterns in images, with the aim of automatically classifying images.[10] However, once trained, the network can also be run in reverse, being asked to adjust the original image slightly so that a given output neuron (e.g. the one for faces or certain animals) yields a higher confidence score. This can be used for visualizations to understand the emergent structure of the neural network better, and is the basis for the DeepDream concept. However, after enough reiterations, even imagery initially devoid of the sought features will be adjusted enough that a form of pareidolia results, by which psychedelic and surreal images are generated algorithmically. The optimization resembles Backpropagation, however instead of adjusting the network weights, the weights are held fixed and the input is adjusted.
For example, an existing image can be altered so that it is "more cat-like", and the resulting enhanced image can be again input to the procedure.[2] This usage resembles the activity of looking for animals or other patterns in clouds.
Applying gradient descent independently to each pixel of the input produces images in which
adjacent pixels have little relation and thus the image has too much high frequency information.
The generated images can be greatly improved by including a prior or regularizer that prefers inputs
that have natural image statistics (without a preference for any particular image), or are simply smooth.[7][11][12]
For example, Mahendran et al.[11] used the total variation regularizer that prefers images that are piecewise constant. Various regularizers are discussed further in.[12] An in-depth, visual exploration of feature visualization and regularization techniques was published more recently.[13]
The cited resemblance of the imagery to LSD- and psilocybin-induced hallucinations is suggestive of a functional resemblance between artificial neural networks and particular layers of the visual cortex.[14]

Usage[edit]
 A late-stage DeepDream processed photograph of three men in a pool
The dreaming idea can be applied to hidden (internal) neurons other than those in the output, 
which allows exploration of the roles and representations of various parts of the network.[12]
It is also possible to optimize the input to satisfy either a single neuron (this usage is sometimes called Activity Maximization)[15] or an entire layer of neurons.
While dreaming is most often used for visualizing networks or producing computer art, it has recently been proposed that adding "dreamed" inputs to the training set can improve training times for abstractions in Computer Science.[16]
The DeepDream model has also been demonstrated to have application in the field of art history.[17]
DeepDream was used for Foster the People's music video for the song "Doing It for the Money".[18]
Recently, a research group out of the University of Sussex created a Hallucination Machine, applying the DeepDream algorithm to a pre-recorded panoramic video, allowing users to explore virtual reality environments to mimic the experience of psychoactive substances and/or psychopathological conditions.[19]  They were able to demonstrate that the subjective experiences induced by the Hallucination Machine differed significantly from control (non-‘hallucinogenic’) videos, while bearing phenomenological similarities to the psychedelic state (following administration of psilocybin).

See also[edit]


Artificial Intelligence portal
Art portal
Feature detection (computer vision)
Procedural textures
Texture synthesis
References[edit]


^ a b Mordvintsev, Alexander; Olah, Christopher; Tyka, Mike (2015). "DeepDream - a code example for visualizing Neural Networks". Google Research. Archived from the original on 2015-07-08..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ a b c Mordvintsev, Alexander; Olah, Christopher; Tyka, Mike (2015). "Inceptionism: Going Deeper into Neural Networks". Google Research. Archived from the original on 2015-07-03.

^ a b c Szegedy, Christian; Liu, Wei; Jia, Yangqing; Sermanet, Pierre; Reed, Scott; Anguelov, Dragomir; Erhan, Dumitru; Vanhoucke, Vincent; Rabinovich, Andrew (2014). "Going Deeper with Convolutions". Computing Research Repository. arXiv:1409.4842. Bibcode:2014arXiv1409.4842S.

^ Lewis, J.P. (1988). Creation by refinement: a creativity paradigm for gradient descent learning networks. IEEE International Conference on Neural Networks. doi:10.1109/ICNN.1988.23933.

^ Portilla, J; Simoncelli, Eero (2000). "A parametric texture model based on joint statistics of complex wavelet coefficients". International Journal of Computer Vision. 40: 49–70. doi:10.1023/A:1026553619983.

^ Erhan, Dumitru. (2009). Visualizing Higher-Layer Features of a Deep Network (PDF). International Conference on Machine Learning Workshop on Learning Feature Hierarchies.

^ a b Simonyan, Karen; Vedaldi, Andrea; Zisserman, Andrew (2014). Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. International Conference on Learning Representations Workshop.

^ deepdream on GitHub

^ Daniel Culpan (2015-07-03). "These Google "Deep Dream" Images Are Weirdly Mesmerising". Wired. Retrieved 2015-07-25.

^ Rich McCormick (7 July 2015). "Fear and Loathing in Las Vegas is terrifying through the eyes of a computer". The Verge. Retrieved 2015-07-25.

^ a b Mahendran, Aravindh; Vedaldi, Andrea (2015). Understanding Deep Image Representations by Inverting Them. IEEE Conference on Computer Vision and Pattern Recognition. arXiv:1412.0035. doi:10.1109/CVPR.2015.7299155.

^ a b c Yosinski, Jason; Clune, Jeff; Nguyen, Anh; Fuchs, Thomas (2015). Understanding Neural Networks Through Deep Visualization. Deep Learning Workshop, International Conference on Machine Learning (ICML) Deep Learning Workshop. arXiv:1506.06579.

^ Olah, Chris; Mordvintsev, Alexander; Schubert, Ludwig (2017-11-07). "Feature Visualization". Distill. 2 (11). doi:10.23915/distill.00007. ISSN 2476-0757.

^ LaFrance, Adrienne (2015-09-03). "When Robots Hallucinate". The Atlantic. Retrieved 24 September 2015.

^ Nguyen, Anh; Dosovitskiy, Alexey; Yosinski, Jason; Brox, Thomas (2016). Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. arxiv. arXiv:1605.09304. Bibcode:2016arXiv160509304N.

^ Arora, Sanjeev; Liang, Yingyu; Tengyu, Ma (2016). Why are deep nets reversible: A simple theory, with implications for training. arxiv. arXiv:1511.05653. Bibcode:2015arXiv151105653A.

^ Spratt, Emily L. (2017). "Dream Formulations and Deep Neural Networks: Humanistic Themes in the Iconology of the Machine-Learned Image" (PDF). Kunsttexte. Humboldt-Universität zu Berlin. 4. arXiv:1802.01274. Bibcode:2018arXiv180201274S.

^ fosterthepeopleVEVO (2017-08-11), Foster The People - Doing It for the Money, retrieved 2017-08-15

^ Suzuki, Keisuke (22 November 2017). "A Deep-Dream Virtual Reality Platform for Studying Altered Perceptual Phenomenology". Sci Rep. 7 (1): 15982. Bibcode:2017NatSR...715982S. doi:10.1038/s41598-017-16316-2. PMC 5700081. PMID 29167538.


External links[edit]



Wikimedia Commons has media related to Deep Dream images.

Deep Dream, python notebook on GitHub
Mordvintsev, Alexander; Olah, Christopher; Tyka, Mike (June 17, 2015). "Inceptionism: Going Deeper into Neural Networks". Archived from the original on 2015-07-03.



 Schematic overview of a deep belief net. Arrows represent directed connections in the graphical model that the net represents.
Machine learning anddata mining
Problems
Classification
Clustering
Regression
Anomaly detection
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction


Supervised learning.mw-parser-output .nobold{font-weight:normal}(classification • regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)


Clustering
BIRCH
CURE
Hierarchical
k-means
Expectation–maximization (EM)
DBSCAN
OPTICS
Mean-shift


Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
t-SNE


Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov


Anomaly detection
k-NN
Local outlier factor


Artificial neural networks
Autoencoder
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net


Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)


Theory
Bias–variance dilemma
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory


Machine-learning venues
NIPS
ICML
ML
JMLR
ArXiv:cs.LG


Glossary of artificial intelligence
Glossary of artificial intelligence


Related articles
List of datasets for machine-learning research
Outline of machine learning


 Machine learning portalvte
In machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a class of deep neural network, composed of multiple layers of latent variables ("hidden units"), with connections between the layers but not between units within each layer.[1]
When trained on a set of examples without supervision, a DBN can learn to probabilistically reconstruct its inputs. The layers then act as feature detectors.[1] After this learning step, a DBN can be further trained with supervision to perform classification.[2]
DBNs can be viewed as a composition of simple, unsupervised networks such as restricted Boltzmann machines (RBMs)[1] or autoencoders,[3] where each sub-network's hidden layer serves as the visible layer for the next. An RBM is an undirected, generative energy-based model with a "visible" input layer and a hidden layer and connections between but not within layers. This composition leads to a fast, layer-by-layer unsupervised training procedure, where contrastive divergence is applied to each sub-network in turn, starting from the "lowest" pair of layers (the lowest visible layer is a training set).
The observation[2] that DBNs can be trained greedily, one layer at a time, led to one of the first effective deep learning algorithms.[4]:6 Overall, there are many attractive implementations and uses of DBNs in real-life applications and scenarios (e.g., electroencephalography,[5] drug discovery[6][7][8]).

Contents

1 Training
2 See also
3 References
4 External links


Training[edit]
 A restricted Boltzmann machine (RBM) with fully connected visible and hidden units. Note there are no hidden-hidden or visible-visible connections.
The training method for RBMs proposed by Geoffrey Hinton for use with training "Product of Expert" models is called contrastive divergence (CD).[9] CD provides an approximation to the maximum likelihood method that would ideally be applied for learning the weights.[10][11] In training a single RBM, weight updates are performed with gradient descent via the following equation: 




w

i
j


(
t
+
1
)
=

w

i
j


(
t
)
+
η



∂
log
⁡
(
p
(
v
)
)


∂

w

i
j







{\displaystyle w_{ij}(t+1)=w_{ij}(t)+\eta {\frac {\partial \log(p(v))}{\partial w_{ij}}}}


where, 



p
(
v
)


{\displaystyle p(v)}

 is the probability of a visible vector, which is given by 



p
(
v
)
=


1
Z



∑

h



e

−
E
(
v
,
h
)




{\displaystyle p(v)={\frac {1}{Z}}\sum _{h}e^{-E(v,h)}}

. 



Z


{\displaystyle Z}

 is the partition function (used for normalizing) and 



E
(
v
,
h
)


{\displaystyle E(v,h)}

 is the energy function assigned to the state of the network. A lower energy indicates the network is in a more "desirable" configuration. The gradient 






∂
log
⁡
(
p
(
v
)
)


∂

w

i
j







{\displaystyle {\frac {\partial \log(p(v))}{\partial w_{ij}}}}

 has the simple form 



⟨

v

i



h

j



⟩

data


−
⟨

v

i



h

j



⟩

model




{\displaystyle \langle v_{i}h_{j}\rangle _{\text{data}}-\langle v_{i}h_{j}\rangle _{\text{model}}}

 where 



⟨
⋯

⟩

p




{\displaystyle \langle \cdots \rangle _{p}}

 represent averages with respect to distribution 



p


{\displaystyle p}

. The issue arises in sampling 



⟨

v

i



h

j



⟩

model




{\displaystyle \langle v_{i}h_{j}\rangle _{\text{model}}}

 because this requires extended alternating Gibbs sampling. CD replaces this step by running alternating Gibbs sampling for 



n


{\displaystyle n}

 steps (values of 



n
=
1


{\displaystyle n=1}

 perform well). After 



n


{\displaystyle n}

 steps, the data are sampled and that sample is used in place of 



⟨

v

i



h

j



⟩

model




{\displaystyle \langle v_{i}h_{j}\rangle _{\text{model}}}

. The CD procedure works as follows:[10]

Initialize the visible units to a training vector.
Update the hidden units in parallel given the visible units: 



p
(

h

j


=
1
∣


V


)
=
σ
(

b

j


+

∑

i



v

i



w

i
j


)


{\displaystyle p(h_{j}=1\mid {\textbf {V}})=\sigma (b_{j}+\sum _{i}v_{i}w_{ij})}

. 



σ


{\displaystyle \sigma }

 is the sigmoid function and 




b

j




{\displaystyle b_{j}}

 is the bias of 




h

j




{\displaystyle h_{j}}

.
Update the visible units in parallel given the hidden units: 



p
(

v

i


=
1
∣


H


)
=
σ
(

a

i


+

∑

j



h

j



w

i
j


)


{\displaystyle p(v_{i}=1\mid {\textbf {H}})=\sigma (a_{i}+\sum _{j}h_{j}w_{ij})}

. 




a

i




{\displaystyle a_{i}}

 is the bias of 




v

i




{\displaystyle v_{i}}

. This is called the "reconstruction" step.
Re-update the hidden units in parallel given the reconstructed visible units using the same equation as in step 2.
Perform the weight update: 



Δ

w

i
j


∝
⟨

v

i



h

j



⟩

data


−
⟨

v

i



h

j



⟩

reconstruction




{\displaystyle \Delta w_{ij}\propto \langle v_{i}h_{j}\rangle _{\text{data}}-\langle v_{i}h_{j}\rangle _{\text{reconstruction}}}

.
Once an RBM is trained, another RBM is "stacked" atop it, taking its input from the final trained layer. The new visible layer is initialized to a training vector, and values for the units in the already-trained layers are assigned using the current weights and biases. The new RBM is then trained with the procedure above. This whole process is repeated until the desired stopping criterion is met.[12]
Although the approximation of CD to maximum likelihood is crude (does not follow the gradient of any function), it is empirically effective.[10]

See also[edit]
Bayesian network
Deep learning
References[edit]


^ a b c Hinton G (2009). "Deep belief networks". Scholarpedia. 4 (5): 5947. doi:10.4249/scholarpedia.5947..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ a b Hinton GE, Osindero S, Teh YW (July 2006). "A fast learning algorithm for deep belief nets" (PDF). Neural Computation. 18 (7): 1527–54. CiteSeerX 10.1.1.76.1541. doi:10.1162/neco.2006.18.7.1527. PMID 16764513.

^ Bengio Y, Lamblin P, Popovici D, Larochelle H (2007). Greedy Layer-Wise Training of Deep Networks (PDF). NIPS.

^ Bengio, Y. (2009). "Learning Deep Architectures for AI" (PDF). Foundations and Trends in Machine Learning. 2: 1–127. CiteSeerX 10.1.1.701.9550. doi:10.1561/2200000006.

^ Movahedi F, Coyle JL, Sejdic E (May 2018). "Deep Belief Networks for Electroencephalography: A Review of Recent Contributions and Future Outlooks". IEEE Journal of Biomedical and Health Informatics. 22 (3): 642–652. doi:10.1109/jbhi.2017.2727218. PMC 5967386. PMID 28715343.

^ Ghasemi, Pérez-Sánchez; Mehri, Pérez-Garrido (2018). "Neural network and deep-learning algorithms used in QSAR studies: merits and drawbacks". Drug Discovery Today. 23 (10): 1784–1790. doi:10.1016/j.drudis.2018.06.016. PMID 29936244.

^ Ghasemi, Pérez-Sánchez; Mehri, fassihi (2016). "The Role of Different Sampling Methods in Improving Biological Activity Prediction Using Deep Belief Network". Journal of Computational Chemistry. 38 (10): 1–8. doi:10.1002/jcc.24671. PMID 27862046.

^ Gawehn E, Hiss JA, Schneider G (January 2016). "Deep Learning in Drug Discovery". Molecular Informatics. 35 (1): 3–14. doi:10.1002/minf.201501008. PMID 27491648.

^ Hinton GE (2002). "Training Product of Experts by Minimizing Contrastive Divergence" (PDF). Neural Computation. 14 (8): 1771–1800. CiteSeerX 10.1.1.35.8613. doi:10.1162/089976602760128018. PMID 12180402.

^ a b c Hinton GE (2010). "A Practical Guide to Training Restricted Boltzmann Machines". Tech. Rep. UTML TR 2010-003.

^ Fischer A, Igel C (2014). "Training Restricted Boltzmann Machines: An Introduction" (PDF). Pattern Recognition. 47: 25–39. CiteSeerX 10.1.1.716.8647. doi:10.1016/j.patcog.2013.05.025.

^ Bengio Y (2009). "Learning Deep Architectures for AI" (PDF). Foundations and Trends in Machine Learning. 2 (1): 1–127. CiteSeerX 10.1.1.701.9550. doi:10.1561/2200000006.


External links[edit]
"Deep Belief Networks". Deep Learning Tutorials.
"Deep Belief Network Example". Deeplearning4j Tutorials.





DeepMind Technologies LimitedType of businessSubsidiaryFounded23 September 2010; 8 years ago (2010-09-23) [1]Headquarters6 Pancras Square,[2]London N1C 4AG, UK
Founder(s)
Demis Hassabis,
Shane Legg,
Mustafa Suleyman
CEODemis HassabisGeneral managerLila IbrahimIndustryArtificial IntelligenceEmployees700 (as of Dec 2017)[3]ParentIndependent (2010–2014)  Google Inc. (2014–2015)  Alphabet Inc. (2015–present)Websitewww.deepmind.com
 Entrance of building where Google and DeepMind are located at 6 Pancras Square, London, UK.
DeepMind Technologies is a British artificial intelligence company founded in September 2010, currently owned by Alphabet Inc. The company is based in London, with research centres in Canada,[4] France,[5] and the United States.
Acquired by Google in 2014, the company has created a neural network that learns how to play video games in a fashion similar to that of humans,[6] as well as a Neural Turing machine,[7] or a neural network that may be able to access an external memory like a conventional Turing machine, resulting in a computer that mimics the short-term memory of the human brain.[8][9]
The company made headlines in 2016 after its AlphaGo program beat a human professional Go player Lee Sedol, the world champion, in a five-game match, which was the subject of a documentary film.[10]
A more general program, AlphaZero, beat the most powerful programs playing go, chess and shogi (Japanese chess) after a few days of play against itself using reinforcement learning.[11]

.mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}Contents

1 History
2 Machine learning

2.1 Deep reinforcement learning
2.2 AlphaGo and successors

2.2.1 Technology


2.3 AlphaFold
2.4 AlphaStar
2.5 WaveNet
2.6 Miscellaneous contributions to Google


3 DeepMind Health

3.1 NHS data-sharing controversy


4 DeepMind Ethics and Society
5 See also
6 References
7 External links



History[edit]
The start-up was founded by Demis Hassabis, Shane Legg and Mustafa Suleyman in 2010.[12][13] Hassabis and Legg first met at University College London's Gatsby Computational Neuroscience Unit.[14]
During one of the interviews, Demis Hassabis said that the start-up began working on artificial intelligence technology by teaching it how to play old games from the seventies and eighties, which are relatively primitive compared to the ones that are available today. Some of those games included Breakout, Pong and Space Invaders.  AI was introduced to one game at a time, without any prior knowledge of its rules. After spending some time on learning the game, AI would eventually become an expert in it. “The cognitive processes which the AI goes through are said to be very like those a human who had never seen the game would use to understand and attempt to master it.”[15] The goal of the founders is to create a general-purpose AI that can be useful and effective for almost anything.
Major venture capital firms Horizons Ventures and Founders Fund invested in the company,[16] as well as entrepreneurs Scott Banister[17] and Elon Musk.[18] Jaan Tallinn was an early investor and an adviser to the company.[19] On 26 January 2014, Google announced the company had acquired DeepMind for $500 million,[20][21][22][23][24][25] and that it had agreed to take over DeepMind Technologies. The sale to Google took place after Facebook reportedly ended negotiations with DeepMind Technologies in 2013.[26] The company was afterwards renamed Google DeepMind and kept that name for about two years.[2]
In 2014, DeepMind received the "Company of the Year" award from Cambridge Computer Laboratory.[27]
In September 2015, DeepMind and the Royal Free NHS Trust signed their initial Information Sharing Agreement (ISA) to co-develop a clinical task management app, Streams.[28]
After Google's acquisition the company established an artificial intelligence ethics board.[29] The ethics board for AI research remains a mystery, with both Google and DeepMind declining to reveal who sits on the board.[30]  DeepMind, together with Amazon, Google, Facebook, IBM and Microsoft, is a founding member of Partnership on AI, an organization devoted to the society-AI interface.[31] DeepMind has opened a new unit called DeepMind Ethics and Society and focused on the ethical and societal questions raised by artificial intelligence featuring prominent philosopher Nick Bostrom as advisor.[32] In October 2017, DeepMind launched a new research team to investigate AI ethics.[33][34]

Machine learning[edit]
DeepMind Technologies' goal is to "solve intelligence",[35] which they are trying to achieve by combining "the best techniques from machine learning and systems neuroscience to build powerful general-purpose learning algorithms".[35]
They are trying to formalize intelligence[36] in order to not only implement it into machines, but also understand the human brain, as Demis Hassabis explains:

.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}[...] attempting to distil intelligence into an algorithmic construct may prove to be the best path to understanding some of the enduring mysteries of our minds.[37]

Google Research has released a paper in 2016 regarding AI Safety and avoiding undesirable behaviour during the AI learning process.[38] Deepmind has also released several publications via its website.[39] In 2017 DeepMind released GridWorld, an open-source testbed for evaluating whether an algorithm learns to disable its kill switch or otherwise exhibits certain undesirable behaviours.[40][41]
To date, the company has published research on computer systems that are able to play games, and developing these systems, ranging from strategy games such as Go[42] to arcade games. According to Shane Legg, human-level machine intelligence can be achieved "when a machine can learn to play a really wide range of games from perceptual stream input and output, and transfer understanding across games[...]."[43]
Research describing an AI playing seven different Atari 2600 video games (the Pong game in Video Olympics, Breakout, Space Invaders, Seaquest, Beamrider, Enduro, and Q*bert) reportedly led to the company's acquisition by Google.[6] Hassabis has mentioned the popular e-sport game StarCraft as a possible future challenge, since it requires a high level of strategic thinking and handling imperfect information.[44] The first demonstration of the DeepMind progress in StarCraft II occurred on January 24, 2019 on StarCrafts Twitch channel and DeepMind’s YouTube channel.[45]
In July 2018, researchers from DeepMind trained one of its systems to play the famous computer game Quake III Arena.[46]

Deep reinforcement learning[edit]
As opposed to other AIs, such as IBM's Deep Blue or Watson, which were developed for a pre-defined purpose and only function within its scope, DeepMind claims that its system is not pre-programmed: it learns from experience, using only raw pixels as data input. Technically it uses deep learning on a convolutional neural network, with a novel form of Q-learning, a form of model-free reinforcement learning.[2][47]  They test the system on video games, notably early arcade games, such as Space Invaders or Breakout.[47][48] Without altering the code, the AI begins to understand how to play the game, and after some time plays, for a few games (most notably Breakout), a more efficient game than any human ever could.[48]
In 2013, DeepMind demonstrated an AI system could surpass human abilities in games such as Pong, Breakout, Space Invaders, Seaquest, Beamrider, Enduro and Q*bert.[49] DeepMind's AI had been applied to video games made in the 1970s and 1980s; work was ongoing for more complex 3D games such as Doom, which first appeared in the early 1990s.[48]

AlphaGo and successors[edit]
Main articles: AlphaGo, Master (software), AlphaGo Zero, and AlphaZero
In October 2015, a computer Go program called AlphaGo, developed by DeepMind, beat the European Go champion Fan Hui, a 2 dan (out of 9 dan possible) professional, five to zero.[50] This is the first time an artificial intelligence (AI) defeated a professional Go player.[51] Previously, computers were only known to have played Go at "amateur" level.[50][52] Go is considered much more difficult for computers to win compared to other games like chess, due to the much larger number of possibilities, making it prohibitively difficult for traditional AI methods such as brute-force.[50][52]
In March 2016 it beat Lee Sedol—a 9th dan Go player and one of the highest ranked players in the world—with a score of 4-1 in a five-game match. 
In the 2017 Future of Go Summit, AlphaGo won a three-game match with Ke Jie, who at the time continuously held the world No. 1 ranking for two years.[53][54] It used a supervised learning protocol, studying large numbers of games played by humans against each other.[55]
In 2017, an improved version, AlphaGo Zero, defeated AlphaGo 100 games to 0. AlphaGo Zero's strategies were self-taught. AlphaGo Zero was able to beat its predecessor after just three days with less processing power than AlphaGo; in comparison, the original AlphaGo needed months to learn how to play.[56]
Later that year, AlphaZero, a modified version of AlphaGo Zero but for handling any two-player game of perfect information, gained superhuman abilities at chess and shogi. Like AlphaGo Zero, AlphaZero learned solely through self-play.

Technology[edit]
AlphaGo technology was developed based on the deep reinforcement learning approach. This makes AlphaGo different from the rest of AI technologies on the market. With that said, AlphaGo's ‘brain’ was introduced to various moves based on the historical tournament data. The number of moves was increased gradually until it eventually processed over 30 million of them. The aim was to have the system mimic the human player and eventually become better. It played against itself and learned not only from its own defeats but wins as well; thus, it learned to improve itself over the time and increased its winning rate as a result.[57]
AlphaGo used two deep neural networks: a policy network to evaluate move probabilities and a value network to assess positions. The policy network trained via supervised learning, and was subsequently refined by policy-gradient reinforcement learning. The value network learned to predict winners of games played by the policy network against itself. After training these networks employed a lookahead Monte Carlo tree search (MCTS), using the policy network to identify candidate high-probability moves, while the value network (in conjunction with Monte Carlo rollouts using a fast rollout policy) evaluated tree positions.[58]
Zero trained using reinforcement learning in which the system played millions of games against itself. Its only guide was to increase its win rate. It did so without learning from games played by humans. Its only input features are the black and white stones from the board. It uses a single neural network, rather than separate policy and value networks. Its simplified tree search relies upon this neural network to evaluate positions and sample moves, without Monte Carlo rollouts. A new reinforcement learning algorithm incorporates lookahead search inside the training loop.[58] AlphaGo Zero employed around 15 people and millions in computing resources.[59] Ultimately, it needed much less computing power than AlphaGo, running on four specialized AI processors (Google TPUs), instead of AlphaGo's 48.[60]

AlphaFold[edit]
In 2016 DeepMind turned its artificial intelligence to protein folding, one of the toughest problems in science. In December 2018, DeepMind's tool AlphaFold won the 13th Critical Assessment of Techniques for Protein Structure Prediction (CASP) by successfully predicting the most accurate structure for 25 out of 43 proteins. “This is a lighthouse project, our first major investment in terms of people and resources into a fundamental, very important, real-world scientific problem,” Demis Hassabis said to The Guardian.[61] This demonstration to the power of AI, specifically DeepMind's application, bodes well to understanding the causes and cures to diseases[62] and serves as a component to potential Nobel-prize worth breakthroughs in the next decade.[63]

AlphaStar[edit]
In January 2019, DeepMind introduced AlphaStar, a program playing the real-time strategy game StarCraft II. AlphaStar uses a reinforced learning to learn the basics of the Protoss race based on replays from human players, and later played against itself to enhance its skills. At the time of the presentation, AlphaStar had knowledge equivalent to 200 years of playing time; it won 10 consecutive matches against professional players, and lost just one.[64]

WaveNet[edit]
Main article: WaveNet
WaveNet is DeepMind's deep generative model of raw audio waveforms. WaveNet was originally too computationally intensive for use in consumer products when it debuted in 2016; however, in late 2017, it became ready for use in consumer applications such as Google Assistant.[65][66] In 2018 Google launched a commercial a text-to-speech product, Cloud Text-to-Speech, based on WaveNet.[67][68]

Miscellaneous contributions to Google[edit]
Google has stated that DeepMind algorithms have greatly increased the efficiency of cooling its data centers.[69] In addition, DeepMind (alongside other Alphabet AI researchers) assists Google Play's personalized app recommendations.[67] DeepMind has also collaborated with the Android team at Google for the creation of two new features which will be available to people with devices running Android Pie, the ninth installment of Google's mobile operating system. These features, Adaptive Battery and Adaptive Brightness, use machine learning to conserve energy and make devices running the operating system easier to use. It is the first time DeepMind has used these techniques on such a small scale, with typical machine learning applications requiring orders of magnitude more compute power.[70]

DeepMind Health[edit]
In July 2016, a collaboration between DeepMind and Moorfields Eye Hospital was announced to develop AI applications for healthcare.[71] DeepMind would be applied to the analysis of anonymised eye scans, searching for early signs of diseases leading to blindness.
In August 2016, a research programme with University College London Hospital was announced with the aim of developing an algorithm that can automatically differentiate between healthy and cancerous tissues in head and neck areas.[72]
There are also projects with the Royal Free London NHS Foundation Trust and Imperial College Healthcare NHS Trust to develop new clinical mobile apps linked to electronic patient records.[73]  Staff at the Royal Free Hospital were reported as saying in December 2017 that access to patient data through the app had saved a ‘huge amount of time’ and made a ‘phenomenal’ difference to the management of patients with acute kidney injury.  Test result data is sent to staff's mobile phones and alerts them to change in the patient's condition.  It also enables staff to see if someone else has responded, and to show patients their results in visual form.[74][unreliable source?]
In November 2017, DeepMind announced a research partnership with the Cancer Research UK Centre at Imperial College London with the goal of improving breast cancer detection by applying machine learning to mammography.[75] Additionally, in February 2018, DeepMind announced it was working with the U.S. Department of Veterans Affairs in an attempt to use machine learning to predict the onset of acute kidney injury in patients, and also more broadly the general deterioration of patients during a hospital stay so that doctors and nurses can more quickly treat patients in need.[76]
DeepMind developed an app called Streams, which sends alerts to doctors about patients at risk of acute risk injury.[77] On 13 November 2018, DeepMind announced that its health division and the Streams app would be absorbed into Google Health.[78] Privacy advocates said the announcement betrayed patient trust and appeared to contradict previous statements by DeepMind that patient data would not be connected to Google accounts or services.[79][80] A spokesman for DeepMind said that patient data would still be kept separate from Google services or projects.[81]

NHS data-sharing controversy[edit]
In April 2016, New Scientist obtained a copy of a data-sharing agreement between DeepMind and the Royal Free London NHS Foundation Trust.  The latter operates three London hospitals where an estimated 1.6 million patients are treated annually. The agreement shows DeepMind Health had access to admissions, discharge and transfer data, accident and emergency, pathology and radiology, and critical care at these hospitals. This included personal details such as whether patients had been diagnosed with HIV, suffered from depression or had ever undergone an abortion in order to conduct research to seek better outcomes in various health conditions.[82][83]
A complaint was filed to the Information Commissioner's Office (ICO), arguing that the data should be pseudonymised and encrypted.[84] In May 2016, New Scientist published a further article claiming that the project had failed to secure approval from the Confidentiality Advisory Group of the Medicines and Healthcare Products Regulatory Agency.[85]
In May 2017, Sky News published a leaked letter from the National Data Guardian, Dame Fiona Caldicott, revealing that in her "considered opinion" the data-sharing agreement between DeepMind and the Royal Free took place on an "inappropriate legal basis".[86] The Information Commissioner's Office ruled in July 2017 that the Royal Free hospital failed to comply with the Data Protection Act when it handed over personal data of 1.6 million patients to DeepMind.[87]

DeepMind Ethics and Society[edit]
As of October 2017, DeepMind has expanded its focus to also include AI ethics. With the former Google UK and EU policy manager Verity Harding co-leading this new team with Sean Legassick,[88] their goal is to fund external research of the following themes: privacy transparency and fairness; economic impacts; governance and accountability; managing AI risk; AI morality and values; and how AI can address the world's challenges. As a result, the team hopes to further understand the ethical implications of AI and aid society to seeing AI can be beneficial.[89]
This new subdivision of DeepMind is a completely separate unit from the partnership of leading companies using AI, academia, civil society organizations and nonprofits of the name Partnership on Artificial Intelligence to Benefit People and Society which DeepMind is also a part of.[90]

See also[edit]
Artificial intelligence
Glossary of artificial intelligence
References[edit]


^ "DEEPMIND TECHNOLOGIES LIMITED – Overview (free company information from Companies House)". Companies House. Retrieved 13 March 2016..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ a b c Mnih, Volodymyr; Kavukcuoglu, Koray; Silver, David (26 February 2015). "Human-level control through deep reinforcement learning". Nature. 518 (7540): 529–33. Bibcode:2015Natur.518..529M. doi:10.1038/nature14236. PMID 25719670.

^ "DeepMind's CEO told Prince Harry his AI lab now employs 700 staff".

^ "About Us | DeepMind". DeepMind.

^ "A return to Paris | DeepMind". DeepMind.

^ a b "The Last AI Breakthrough DeepMind Made Before Google Bought It". The Physics arXiv Blog. 29 January 2014. Retrieved 12 October 2014.

^ Graves, Alex; Wayne, Greg; Danihelka, Ivo (2014). "Neural Turing Machines". arXiv:1410.5401 [cs.NE].

^ Best of 2014: Google's Secretive DeepMind Startup Unveils a "Neural Turing Machine", MIT Technology Review

^ Graves, Alex; Wayne, Greg; Reynolds, Malcolm; Harley, Tim; Danihelka, Ivo; Grabska-Barwińska, Agnieszka; Colmenarejo, Sergio Gómez; Grefenstette, Edward; Ramalho, Tiago (12 October 2016). "Hybrid computing using a neural network with dynamic external memory". Nature. 538 (7626): 471–476. Bibcode:2016Natur.538..471G. doi:10.1038/nature20101. ISSN 1476-4687. PMID 27732574.

^ Kohs, Greg (29 September 2017), AlphaGo, Ioannis Antonoglou, Lucas Baker, Nick Bostrom, retrieved 9 January 2018

^ Silver, David; Hubert, Thomas; Schrittwieser, Julian; Antonoglou, Ioannis; Lai, Matthew; Guez, Arthur; Lanctot, Marc; Sifre, Laurent; Kumaran, Dharshan; Graepel, Thore; Lillicrap, Timothy; Simonyan, Karen; Hassabis, Demis (5 December 2017). "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm". arXiv:1712.01815 [cs.AI].

^ "Google Buys U.K. Artificial Intelligence Company DeepMind". Bloomberg. 27 January 2014. Retrieved 13 November 2014.

^ "Google makes £400m move in quest for artificial intelligence". Financial Times. 27 January 2014. Retrieved 13 November 2014.

^ "Demis Hassabis: 15 facts about the DeepMind Technologies founder". The Guardian. Retrieved 12 October 2014.

^ Marr, Bernard. "How Google's Amazing AI Start-Up 'DeepMind' Is Making Our World A Smarter Place". Forbes. Retrieved 30 June 2018.

^ "DeepMind buy heralds rise of the machines". Financial Times. Retrieved 14 October 2014.

^ "DeepMind Technologies Investors". Retrieved 12 October 2014.

^ Cuthbertson, Anthony (4 August 2014). "Elon Musk: Artificial Intelligence 'Potentially More Dangerous Than Nukes'". International Business Times UK.

^ "Recode.net – DeepMind Technologies Acquisition". 26 January 2014. Retrieved 27 January 2014.

^ "Google to buy artificial intelligence company DeepMind". Reuters. 26 January 2014. Retrieved 12 October 2014.

^ "Google Acquires UK AI startup Deepmind". The Guardian. Retrieved 27 January 2014.

^ "Report of Acquisition, TechCrunch". TechCrunch. Retrieved 27 January 2014.

^ Oreskovic, Alexei (27 January 2014). "Reuters Report". Reuters. Retrieved 27 January 2014.

^ "Google Acquires Artificial Intelligence Start-Up DeepMind". The Verge. Retrieved 27 January 2014.

^ "Google acquires AI pioneer DeepMind Technologies". Ars Technica. 27 January 2014. Retrieved 27 January 2014.

^ "Google beats Facebook for Acquisition of DeepMind Technologies". Retrieved 27 January 2014.

^ "Hall of Fame Awards: To celebrate the success of companies founded by Computer Laboratory graduates". University of Cambridge. Retrieved 12 October 2014.

^ Lomas, Natasha. "Documents detail DeepMind's plan to apply AI to NHS data in 2015". TechCrunch. Retrieved 26 September 2017.

^ "Inside Google's Mysterious Ethics Board". Forbes. 3 February 2014. Retrieved 12 October 2014.

^ Ramesh, Randeep (4 May 2016). "Google's DeepMind shouldn't suck up our NHS records in secret". The Guardian. Archived from the original on 13 October 2016. Retrieved 19 October 2016.

^ "Home/ Partnership on Artificial Intelligence to Benefit People and Society". 2016. Retrieved 15 October 2016.

^ Hern, Alex (4 October 2017). "DeepMind announces ethics group to focus on problems of AI". The Guardian  – via www.theguardian.com.

^ "DeepMind has launched a new 'ethics and society' research team". Business Insider. Retrieved 25 October 2017.

^ "DeepMind launches new research team to investigate AI ethics". The Verge. Retrieved 25 October 2017.

^ a b "DeepMind Technologies Website". DeepMind Technologies. Retrieved 11 October 2014.

^ Legg, Shane; Veness, Joel (29 September 2011). "An Approximation of the Universal Intelligence Measure". arXiv:1109.5951 [cs.AI].

^ Hassabis, Demis (23 February 2012). "Model the brain's algorithms" (PDF). Nature. Retrieved 12 October 2014.

^ Amodei, Dario; Olah, Chris; Steinhardt, Jacob; Christiano, Paul; Schulman, John; Mané, Dan (21 June 2016). "Concrete Problems in AI Safety". arXiv:1606.06565 [cs.AI].

^ "Publications". DeepMind. Retrieved 11 September 2016.

^ "DeepMind Has Simple Tests That Might Prevent Elon Musk's AI Apocalypse". Bloomberg.com. 11 December 2017. Retrieved 8 January 2018.

^ "Alphabet's DeepMind Is Using Games to Discover If Artificial Intelligence Can Break Free and Kill Us All". Fortune. Retrieved 8 January 2018.

^ Huang, Shih-Chieh; Müller, Martin (12 July 2014). Investigating the Limits of Monte-Carlo Tree Search Methods in Computer Go. Lecture Notes in Computer Science. 8427. Springer. pp. 39–48. CiteSeerX 10.1.1.500.1701. doi:10.1007/978-3-319-09165-5_4. ISBN 978-3-319-09164-8.

^ "Q&A with Shane Legg on risks from AI". 17 June 2011. Retrieved 12 October 2014.

^ "DeepMind founder Demis Hassabis on how AI will shape the future". The Verge. 10 March 2016.

^ DeepMind - StarCraft II demonstation in StarCraft II Oficial Website

^ "DeepMind AI’s new trick is playing ‘Quake III Arena’ like a human". Engadget. 3 July 2018.

^ a b Mnih, Volodymyr; Kavukcuoglu, Koray; Silver, David; Graves, Alex; Antonoglou, Ioannis; Wierstra, Daan; Riedmiller, Martin (12 December 2013). "Playing Atari with Deep Reinforcement Learning". arXiv:1312.5602 [cs.LG].

^ a b c Deepmind artificial intelligence @ FDOT14. 19 April 2014.

^ "A look back at some of AI's biggest video game wins in 2018". VentureBeat. 29 December 2018. Retrieved 19 April 2019.

^ a b c "Google achieves AI 'breakthrough' by beating Go champion". BBC News. 27 January 2016.

^ "Première défaite d'un professionnel du go contre une intelligence artificielle". Le Monde (in French). 27 January 2016.

^ a b "Research Blog: AlphaGo: Mastering the ancient game of Go with Machine Learning". Google Research Blog. 27 January 2016.

^ "World's Go Player Ratings". May 2017.

^ "柯洁迎19岁生日 雄踞人类世界排名第一已两年" (in Chinese). May 2017.

^ "The latest AI can work things out without being taught". The Economist. Retrieved 19 October 2017.

^ Cellan-Jones, Rory (18 October 2017). "Google DeepMind: AI becomes more alien". BBC News. Retrieved 3 December 2017.

^ "Penn State WebAccess Secure Login". ebookcentral.proquest.com. Retrieved 30 June 2018.

^ a b Silver, David; Schrittwieser, Julian; Simonyan, Karen; Antonoglou, Ioannis; Huang, Aja; Guez, Arthur; Hubert, Thomas; Baker, Lucas; Lai, Matthew; Bolton, Adrian; Chen, Yutian; Lillicrap, Timothy; Fan, Hui; Sifre, Laurent; Driessche, George van den; Graepel, Thore; Hassabis, Demis (19 October 2017). "Mastering the game of Go without human knowledge". Nature. 550 (7676): 354–359. Bibcode:2017Natur.550..354S. doi:10.1038/nature24270. ISSN 0028-0836. PMID 29052630.

^ Knight, Will. "The world's smartest game-playing AI—DeepMind's AlphaGo—just got way smarter". MIT Technology Review. Retrieved 19 October 2017.

^ Vincent, James (18 October 2017). "DeepMind's Go-playing AI doesn't need human help to beat us anymore". The Verge. Retrieved 19 October 2017.

^ Sample, Ian (2 December 2018). "Google's DeepMind predicts 3D shapes of proteins". The Guardian. Retrieved 3 December 2018.

^ "DeepMind's AI will accelerate drug discovery by predicting how proteins fold". MIT Technology Review. Retrieved 3 December 2018.

^ "Google DeepMind founder Demis Hassabis: Three truths about AI". TechRepublic. Retrieved 3 December 2018.

^ "DeepMind AI Challenges Pro StarCraft II Players, Wins Almost Every Match". Extreme Tech. 24 January 2019. Retrieved 24 January 2019.

^ "Here's Why Google's Assistant Sounds More Realistic Than Ever Before". Fortune. 5 October 2017. Retrieved 20 January 2018.

^ Gershgorn, Dave. "Google's voice-generating AI is now indistinguishable from humans". Quartz. Retrieved 20 January 2018.

^ a b Novet, Jordan (31 March 2018). "Google is finding ways to make money from Alphabet's DeepMind A.I. technology". CNBC. Retrieved 3 April 2018.

^ "Introducing Cloud Text-to-Speech powered by DeepMind WaveNet technology". Google Cloud Platform Blog. Retrieved 5 April 2018.

^ "DeepMind AI Reduces Google Data Centre Cooling Bill by 40%". DeepMind Blog. 20 July 2016.

^ "DeepMind, meet Android | DeepMind". DeepMind Blog. 8 May 2018.

^ Baraniuk, Chris (6 July 2016). "Google's DeepMind to peek at NHS eye scans for disease analysis". BBC. Retrieved 6 July 2016.

^ Baraniuk, Chris (31 August 2016). "Google DeepMind targets NHS head and neck cancer treatment". BBC. Retrieved 5 September 2016.

^ "DeepMind announces second NHS partnership". IT Pro. 23 December 2016. Retrieved 23 December 2016.

^ "Google DeepMind's Streams technology branded 'phenomenal'". Digital Health. 4 December 2017. Retrieved 23 December 2017.

^ "Google DeepMind announces new research partnership to fight breast cancer with AI". Silicon Angle. 24 November 2017.

^ "Google's DeepMind wants AI to spot kidney injuries". Venture Beat. 22 February 2018.

^ Evenstad, Lis (15 June 2018). "DeepMind Health must be transparent to gain public trust, review finds". ComputerWeekly.com. Retrieved 14 November 2018.

^ Vincent, James (13 November 2018). "Google is absorbing DeepMind's health care unit to create an 'AI assistant for nurses and doctors'". The Verge. Retrieved 14 November 2018.

^ Hern, Alex (14 November 2018). "Google 'betrays patient trust' with DeepMind Health move". the Guardian. Retrieved 14 November 2018.

^ Stokel-Walker, Chris (14 November 2018). "Why Google consuming DeepMind Health is scaring privacy experts". Wired. Retrieved 15 November 2018.

^ Murphy, Margi (14 November 2018). "DeepMind boss defends controversial Google health deal". The Telegraph. Retrieved 14 November 2018.

^ Hodson, Hal (29 April 2016). "Revealed: Google AI has access to huge haul of NHS patient data". New Scientist.

^ "Leader: If Google has nothing to hide about NHS data, why so secretive?". New Scientist. 4 May 2016.

^ Donnelly, Caroline (12 May 2016). "ICO probes Google DeepMind patient data-sharing deal with NHS Hospital Trust". Computer Weekly.

^ Hodson, Hal (25 May 2016). "Did Google's NHS patient data deal need ethical approval?". New Scientist. Retrieved 28 May 2016.

^ Martin, Alexander J (15 May 2017). "Google received 1.6 million NHS patients' data on an 'inappropriate legal basis'". Sky News. Retrieved 16 May 2017.

^ Hern, Alex (3 July 2017). "Royal Free breached UK data law in 1.6m patient deal with Google's DeepMind". The Guardian.

^ "Why we launched DeepMind Ethics & Society". DeepMind Blog. Retrieved 25 March 2018.

^ Temperton, James. "DeepMind's new AI ethics unit is the company's next big move". Wired (UK). Retrieved 3 December 2017.

^ Hern, Alex (4 October 2017). "DeepMind announces ethics group to focus on problems of AI". The Guardian. Retrieved 8 December 2017.


External links[edit]
Official website 
vteAlphabet Inc.Subsidiaries
Google
Google Fiber
Calico
CapitalG
Chronicle
DeepMind
GV
Jigsaw
Loon
Makani
Sidewalk Labs
Verily
X
Waymo
Wing
People
Arthur D. Levinson
Astro Teller
David Krane
Eric Schmidt
Ruth Porat
Sundar Pichai
Tony Fadell
Andrew Conrad
Founders
Larry Page
Sergey Brin


 Category
 Portal




Not to be confused with recursive neural network.
Machine learning anddata mining
Problems
Classification
Clustering
Regression
Anomaly detection
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction


Supervised learning.mw-parser-output .nobold{font-weight:normal}(classification • regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)


Clustering
BIRCH
CURE
Hierarchical
k-means
Expectation–maximization (EM)
DBSCAN
OPTICS
Mean-shift


Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
t-SNE


Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov


Anomaly detection
k-NN
Local outlier factor


Artificial neural networks
Autoencoder
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net


Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)


Theory
Bias–variance dilemma
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory


Machine-learning venues
NIPS
ICML
ML
JMLR
ArXiv:cs.LG


Glossary of artificial intelligence
Glossary of artificial intelligence


Related articles
List of datasets for machine-learning research
Outline of machine learning


 Machine learning portalvte
A recurrent neural network (RNN) is a class of artificial neural network where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition[1] or speech recognition.[2][3]
The term "recurrent neural network" is used indiscriminately to refer to two broad classes of networks with a similar general structure, where one is finite impulse and the other is infinite impulse. Both classes of networks exhibit temporal dynamic behavior.[4] A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.
Both finite impulse and infinite impulse recurrent networks can have additional stored state, and the storage can be under direct control by the neural network. The storage can also be replaced by another network or graph, if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated state or gated memory, and are part of long short-term memory networks (LSTMs) and gated recurrent units.

.mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}Contents

1 History

1.1 LSTM


2 Architectures

2.1 Fully recurrent
2.2 Elman networks and Jordan networks
2.3 Hopfield

2.3.1 Bidirectional associative memory


2.4 Echo state
2.5 Independent RNN (IndRNN)
2.6 Recursive
2.7 Neural history compressor
2.8 Second order RNNs
2.9 Long short-term memory
2.10 Gated recurrent unit
2.11 Bi-directional
2.12 Continuous-time
2.13 Hierarchical
2.14 Recurrent multilayer perceptron network
2.15 Multiple timescales model
2.16 Neural Turing machines
2.17 Differentiable neural computer
2.18 Neural network pushdown automata


3 Training

3.1 Gradient descent
3.2 Global optimization methods


4 Related fields and models
5 Libraries
6 Applications
7 References
8 Further reading
9 External links



History[edit]
Recurrent neural networks were based on David Rumelhart's work in 1986.[5]  Hopfield networks - a special kind of RNN, were discovered by John Hopfield in 1982. In 1993, a neural history compressor system solved a "Very Deep Learning" task that required more than 1000 subsequent layers in an RNN unfolded in time.[6]

LSTM[edit]
Long short-term memory (LSTM) networks were discovered by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.[7]
Around 2007, LSTM started to revolutionize speech recognition, outperforming traditional models in certain speech applications.[8] In 2009, a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition.[9][10] In 2014, the Chinese search giant Baidu used CTC-trained RNNs to break the Switchboard Hub5'00 speech recognition benchmark without using any traditional speech processing methods.[11]
LSTM also improved large-vocabulary speech recognition[2][3] and text-to-speech synthesis[12] and was used in Google Android.[9][13] In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49%[citation needed] through CTC-trained LSTM, which was used by Google voice search.[14]
LSTM broke records for improved machine translation,[15] Language Modeling[16] and Multilingual Language Processing.[17] LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning.[18]

Architectures[edit]
RNNs come in many variants.

Fully recurrent[edit]
 Unfolded basic recurrent neural network
Basic RNNs are a network of neuron-like nodes organized into successive "layers", each node in a given layer is connected with a directed (one-way) connection to every other node in the next successive layer.[citation needed] Each node (neuron) has a time-varying real-valued activation. Each connection (synapse) has a modifiable real-valued weight. Nodes are either input nodes (receiving data from outside the network), output nodes (yielding results), or hidden nodes (that modify the data en route from input to output).
For supervised learning in discrete time settings, sequences of real-valued input vectors arrive at the input nodes, one vector at a time. At any given time step, each non-input unit computes its current activation (result) as a nonlinear function of the weighted sum of the activations of all units that connect to it. Supervisor-given target activations can be supplied for some output units at certain time steps. For example, if the input sequence is a speech signal corresponding to a spoken digit, the final target output at the end of the sequence may be a label classifying the digit.
In reinforcement learning settings, no teacher provides target signals. Instead a fitness function or reward function is occasionally used to evaluate the RNN's performance, which influences its input stream through output units connected to actuators that affect the environment. This might be used to play a game in which progress is measured with the number of points won.
Each sequence produces an error as the sum of the deviations of all target signals from the corresponding activations computed by the network. For a training set of numerous sequences, the total error is the sum of the errors of all individual sequences.

Elman networks and Jordan networks[edit]
 The Elman network
An Elman network is a three-layer network (arranged horizontally as x, y, and z in the illustration) with the addition of a set of "context units" (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one.[19] At each time step, the input is feed-forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state, allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron.
Jordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also referred to as the state layer. They have a recurrent connection to themselves.[19]
Elman and Jordan networks are also known as "simple recurrent networks" (SRN).

Elman network[20]









h

t





=

σ

h


(

W

h



x

t


+

U

h



h

t
−
1


+

b

h


)





y

t





=

σ

y


(

W

y



h

t


+

b

y


)






{\displaystyle {\begin{aligned}h_{t}&=\sigma _{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})\\y_{t}&=\sigma _{y}(W_{y}h_{t}+b_{y})\end{aligned}}}


Jordan network[21]









h

t





=

σ

h


(

W

h



x

t


+

U

h



y

t
−
1


+

b

h


)





y

t





=

σ

y


(

W

y



h

t


+

b

y


)






{\displaystyle {\begin{aligned}h_{t}&=\sigma _{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})\\y_{t}&=\sigma _{y}(W_{y}h_{t}+b_{y})\end{aligned}}}


Variables and functions






x

t




{\displaystyle x_{t}}

: input vector





h

t




{\displaystyle h_{t}}

: hidden layer vector





y

t




{\displaystyle y_{t}}

: output vector




W


{\displaystyle W}

, 



U


{\displaystyle U}

 and 



b


{\displaystyle b}

: parameter matrices and vector





σ

h




{\displaystyle \sigma _{h}}

 and 




σ

y




{\displaystyle \sigma _{y}}

: Activation functions
Hopfield[edit]
Main article: Hopfield network
The Hopfield network is an RNN in which all connections are symmetric. It requires stationary inputs and is thus not a general RNN, as it does not process sequences of patterns. It guarantees that it will converge. If the connections are trained using Hebbian learning then the Hopfield network can perform as robust content-addressable memory, resistant to connection alteration.

Bidirectional associative memory[edit]
Main article: Bidirectional associative memory
Introduced by Bart Kosko,[22] a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bi-directionality comes from passing information through a matrix and its transpose. Typically, bipolar encoding is preferred to binary encoding of the associative pairs. Recently, stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.[23]
A BAM network has two layers, either of which can be driven as an input to recall an association and produce an output on the other layer.[24]

Echo state[edit]
Main article: Echo state network
The echo state network (ESN) has a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series.[25] A variant for spiking neurons is known as a liquid state machine.[26]

Independent RNN (IndRNN)[edit]
The Independently recurrent neural network (IndRNN)[27] addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other's history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with the non-saturated nonlinear functions such as ReLU. Using skip connections, deep networks can be trained.

Recursive[edit]
Main article: Recursive neural network
A recursive neural network[28] is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation.[29][30] They can process distributed representations of structure, such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing.[31] The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree.[32]

Neural history compressor[edit]
The neural history compressor is an unsupervised stack of RNNs.[33] At the input level, it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN, which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level.
The system effectively minimises the description length or the negative logarithm of the probability of the data.[34] Given a lot of learnable predictability in the incoming data sequence, the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events.
It is possible to distill the RNN hierarchy into two RNNs: the "conscious" chunker (higher level) and the "subconscious" automatizer (lower level).[33] Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer, then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate, rarely changing memories across long intervals. In turn this helps the automatizer to make many of its once unpredictable inputs predictable, such that the chunker can focus on the remaining unpredictable events.[33]
A generative model partially overcame the vanishing gradient problem[35] of automatic differentiation or backpropagation in neural networks in 1992. In 1993, such a system solved a "Very Deep Learning" task that required more than 1000 subsequent layers in an RNN unfolded in time.[6]

Second order RNNs[edit]
Second order RNNs use higher order weights 



w




i
j
k




{\displaystyle w{}_{ijk}}

 instead of the standard 



w




i
j




{\displaystyle w{}_{ij}}

 weights, and states can be a product. This allows a direct mapping to a finite state machine both in training, stability, and representation.[36][37] Long short-term memory is an example of this but has no such formal mappings or proof of stability.

Long short-term memory[edit]
Main article: Long short-term memory
 Long short-term memory unit
Long short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem. LSTM is normally augmented by recurrent gates called "forget" gates.[38] LSTM prevents backpropagated errors from vanishing or exploding.[35] Instead, errors can flow backwards through unlimited numbers of virtual layers unfolded in space. That is, LSTM can learn tasks[9] that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved.[39] LSTM works even given long delays between significant events and can handle signals that mix low and high frequency components.
Many applications use stacks of LSTM RNNs[40] and train them by Connectionist Temporal Classification (CTC)[41] to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.
LSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts.[42]

Gated recurrent unit[edit]
Main article: Gated recurrent unit
 Gated recurrent unit
Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014. They are used in the full form and several simplified variants.[43][44] Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory.[45] They have fewer parameters than LSTM, as they lack an output gate.[46]

Bi-directional[edit]
Bi-directional RNNs use a finite sequence to predict or label each element of the sequence based on the element's past and future contexts. This is done by concatenating the outputs of two RNNs, one processing the sequence from left to right, the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique proved to be especially useful when combined with LSTM RNNs.[47][48]

Continuous-time[edit]
A continuous time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming spike train.
For a neuron 



i


{\displaystyle i}

 in the network with action potential 




y

i




{\displaystyle y_{i}}

, the rate of change of activation is given by:






τ

i






y
˙




i


=
−

y

i


+

∑

j
=
1


n



w

j
i


σ
(

y

j


−

Θ

j


)
+

I

i


(
t
)


{\displaystyle \tau _{i}{\dot {y}}_{i}=-y_{i}+\sum _{j=1}^{n}w_{ji}\sigma (y_{j}-\Theta _{j})+I_{i}(t)}


Where:






τ

i




{\displaystyle \tau _{i}}

 : Time constant of postsynaptic node





y

i




{\displaystyle y_{i}}

 : Activation of postsynaptic node








y
˙




i




{\displaystyle {\dot {y}}_{i}}

 : Rate of change of activation of postsynaptic node




w




j
i




{\displaystyle w{}_{ji}}

 : Weight of connection from pre to postsynaptic node




σ
(
x
)


{\displaystyle \sigma (x)}

 : Sigmoid of x e.g. 



σ
(
x
)
=
1

/

(
1
+

e

−
x


)


{\displaystyle \sigma (x)=1/(1+e^{-x})}

.





y

j




{\displaystyle y_{j}}

 : Activation of presynaptic node





Θ

j




{\displaystyle \Theta _{j}}

 : Bias of presynaptic node





I

i


(
t
)


{\displaystyle I_{i}(t)}

 : Input (if any) to node
CTRNNs have been applied to evolutionary robotics where they have been used to address vision,[49] co-operation,[50] and minimal cognitive behaviour.[51]
Note that, by the Shannon sampling theorem, discrete time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations. This transformation can be thought of as occurring after the post-synaptic node activation functions 




y

i


(
t
)


{\displaystyle y_{i}(t)}

 have been low-pass filtered but prior to sampling.

Hierarchical[edit]
Hierarchical RNNs connect their neurons in various ways to decompose hierarchical behavior into useful subprograms.[33][52]

Recurrent multilayer perceptron network[edit]
Generally, a Recurrent Multi-Layer Perceptron (RMLP) network consists of cascaded subnetworks, each of which contains multiple layers of nodes. Each of these subnetworks is feed-forward except for the last layer, which can have feedback connections. Each of these subnets is connected only by feed forward connections.[53]

Multiple timescales model[edit]
A multiple timescales recurrent neural network (MTRNN) is a neural-based computational model that can simulate the functional hierarchy of the brain through self-organization that depends on spatial connection between neurons and on distinct types of neuron activities, each with distinct time properties.[54][55] With such varied neuronal activities, continuous sequences of any set of behaviors are segmented into reusable primitives, which in turn are flexibly integrated into diverse sequential behaviors. The biological approval of such a type of hierarchy was discussed in the memory-prediction theory of brain function by Hawkins in his book On Intelligence.[citation needed]

Neural Turing machines[edit]
Main article: Neural Turing machine
Neural Turing machines (NTMs) are a method of extending recurrent neural networks by coupling them to external memory resources which they can interact with by attentional processes. The combined system is analogous to a Turing machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent.[56]

Differentiable neural computer[edit]
Main article: Differentiable neural computer
Differentiable neural computers (DNCs) are an extension of Neural Turing machines, allowing for usage of fuzzy amounts of each memory address and a record of chronology.

Neural network pushdown automata[edit]
Neural network pushdown automata (NNPDA) are similar to NTMs, but tapes are replaced by analogue stacks that are differentiable and that are trained. In this way, they are similar in complexity to recognizers of context free grammars (CFGs).[57]

Training[edit]
Gradient descent[edit]
Main article: Gradient descent
Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. In neural networks, it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight, provided the non-linear activation functions are differentiable. Various methods for doing so were developed in the 1980s and early 1990s by Werbos, Williams, Robinson, Schmidhuber, Hochreiter, Pearlmutter and others.
The standard method is called "backpropagation through time" or BPTT, and is a generalization of back-propagation for feed-forward networks.[58][59] Like that method, it is an instance of automatic differentiation in the reverse accumulation mode of Pontryagin's minimum principle. A more computationally expensive online variant is called "Real-Time Recurrent Learning" or RTRL,[60][61] which is an instance of automatic differentiation in the forward accumulation mode with stacked tangent vectors. Unlike BPTT, this algorithm is local in time but not local in space.
In this context, local in space means that a unit's weight vector can be updated using only information stored in the connected units and the unit itself such that update complexity of a single unit is linear in the dimensionality of the weight vector. Local in time means that the updates take place continually (on-line) and depend only on the most recent time step rather than on multiple time steps within a given time horizon as in BPTT. Biological neural networks appear to be local with respect to both time and space.[62][63]
For recursively computing the partial derivatives, RTRL has a time-complexity of O(number of hidden x number of weights) per time step for computing the Jacobian matrices, while BPTT only takes O(number of weights) per time step, at the cost of storing all forward activations within the given time horizon.[64] An online hybrid between BPTT and RTRL with intermediate complexity exists,[65][66] along with variants for continuous time.[67]
A major problem with gradient descent for standard RNN architectures is that error gradients vanish exponentially quickly with the size of the time lag between important events.[35][68] LSTM combined with a BPTT/RTRL hybrid learning method attempts to overcome these problems.[7] This problem is also solved in the independently recurrent neural network (IndRNN)[27] by reducing the context of a neuron to its own past state and the cross-neuron information can then be explored in the following layers. Memories of different range including long-term memory can be learned without the gradient vanishing and exploding problem.
The on-line algorithm called causal recursive backpropagation (CRBP), implements and combines BPTT and RTRL paradigms for locally recurrent networks.[69] It works with the most general locally recurrent networks. The CRBP algorithm can minimize the global error term. This fact improves stability of the algorithm, providing a unifying view on gradient calculation techniques for recurrent networks with local feedback.
One approach to the computation of gradient information in RNNs with arbitrary architectures is based on signal-flow graphs diagrammatic derivation.[70] It uses the BPTT batch algorithm, based on Lee's theorem for network sensitivity calculations.[71] It was proposed by Wan and Beaufays, while its fast online version was proposed by Campolucci, Uncini and Piazza.[71]

Global optimization methods[edit]
Training the weights in a neural network can be modeled as a non-linear global optimization problem. A target function can be formed to evaluate the fitness or error of a particular weight vector as follows: First, the weights in the network are set according to the weight vector. Next, the network is evaluated against the training sequence. Typically, the sum-squared-difference between the predictions and the target values specified in the training sequence is used to represent the error of the current weight vector. Arbitrary global optimization techniques may then be used to minimize this target function.
The most common global optimization method for training RNNs is genetic algorithms, especially in unstructured networks.[72][73][74]
Initially, the genetic algorithm is encoded with the neural network weights in a predefined manner where one gene in the chromosome represents one weight link.The whole network is represented as a single chromosome. The fitness function is evaluated as follows:

Each weight encoded in the chromosome is assigned to the respective weight link of the network.
The training set is presented to the network which propagates the input signals forward.
The mean-squared-error is returned to the fitness function.
This function drives the genetic selection process.
Many chromosomes make up the population; therefore, many different neural networks are evolved until a stopping criterion is satisfied. A common stopping scheme is: 

When the neural network has learnt a certain percentage of the training data or
When the minimum value of the mean-squared-error is satisfied or
When the maximum number of training generations has been reached.
The stopping criterion is evaluated by the fitness function as it gets the reciprocal of the mean-squared-error from each network during training. Therefore, the goal of the genetic algorithm is to maximize the fitness function, reducing the mean-squared-error.
Other global (and/or evolutionary) optimization techniques may be used to seek a good set of weights, such as simulated annealing or particle swarm optimization.

Related fields and models[edit]
RNNs may behave chaotically. In such cases, dynamical systems theory may be used for analysis.
They are in fact recursive neural networks with a particular structure: that of a linear chain. Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step.
In particular, RNNs can appear as nonlinear versions of finite impulse response and infinite impulse response filters and also as a nonlinear autoregressive exogenous model (NARX).[75]

Libraries[edit]
Apache Singa
Caffe: Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in C++, and has Python and MATLAB wrappers.
Chainer: The first stable deep learning library that supports dynamic, define-by-run neural networks. Fully in Python, production support for CPU, GPU, distributed training.
Deeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark. A general-purpose deep learning library for the JVM production stack running on a C++ scientific computing engine. Allows the creation of custom layers. Integrates with Hadoop and Kafka.
Dynet: The Dynamic Neural Networks toolkit.
Keras: High-level, easy to use API, providing a wrapper to many other deep learning libraries.
Microsoft Cognitive Toolkit
MXNet: a modern open-source deep learning framework used to train and deploy deep neural networks.
PyTorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration.
TensorFlow: Apache 2.0-licensed Theano-like library with support for CPU, GPU and Google's proprietary TPU,[76] mobile
Theano: The reference deep-learning library for Python with an API largely compatible with the popular NumPy library. Allows user to write symbolic mathematical expressions, then automatically generates their derivatives, saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast, on-the-GPU implementation.
Torch (www.torch.ch): A scientific computing framework with wide support for machine learning algorithms, written in C and lua. The main author is Ronan Collobert, and it is now used at Facebook AI Research and Twitter.
Applications[edit]
Applications of Recurrent Neural Networks include:

Machine Translation
Robot control[77]
Time series prediction[78][79]
Speech recognition[80][81][82]
Speech synthesis[83]
Time series anomaly detection[84]
Rhythm learning[85]
Music composition[86]
Grammar learning[87][88][89]
Handwriting recognition[90][91]
Human action recognition[92]
Protein Homology Detection[93]
Predicting subcellular localization of proteins[94]
Several prediction tasks in the area of business process management[95]
Prediction in medical care pathways[96]
References[edit]


^ Graves, A.; Liwicki, M.; Fernandez, S.; Bertolami, R.; Bunke, H.; Schmidhuber, J. (2009). "A Novel Connectionist System for Improved Unconstrained Handwriting Recognition" (PDF). IEEE Transactions on Pattern Analysis and Machine Intelligence. 31 (5): 855–868. CiteSeerX 10.1.1.139.4502. doi:10.1109/tpami.2008.137. PMID 19299860..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ a b Sak, Hasim; Senior, Andrew; Beaufays, Francoise (2014). "Long Short-Term Memory recurrent neural network architectures for large scale acoustic modeling" (PDF).

^ a b Li, Xiangang; Wu, Xihong (2014-10-15). "Constructing Long Short-Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition". arXiv:1410.4281 [cs.CL].

^ Miljanovic, Milos (Feb–Mar 2012). "Comparative analysis of Recurrent and Finite Impulse Response Neural Networks in Time Series Prediction" (PDF). Indian Journal of Computer and Engineering. 3 (1).CS1 maint: Date format (link)

^ Williams, Ronald J.; Hinton, Geoffrey E.; Rumelhart, David E. (October 1986). "Learning representations by back-propagating errors". Nature. 323 (6088): 533–536. doi:10.1038/323533a0. ISSN 1476-4687.

^ a b Schmidhuber, Jürgen (1993). Habilitation thesis: System modeling and optimization (PDF). Page 150 ff demonstrates credit assignment across the equivalent of 1,200 layers in an unfolded RNN.

^ a b Hochreiter, Sepp; Schmidhuber, Jürgen (1997-11-01). "Long Short-Term Memory". Neural Computation. 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735.

^ Fernández, Santiago; Graves, Alex; Schmidhuber, Jürgen (2007). An Application of Recurrent Neural Networks to Discriminative Keyword Spotting. Proceedings of the 17th International Conference on Artificial Neural Networks. ICANN'07. Berlin, Heidelberg: Springer-Verlag. pp. 220–229. ISBN 978-3-540-74693-5.

^ a b c 
Schmidhuber, Jürgen (January 2015). "Deep Learning in Neural Networks: An Overview". Neural Networks. 61: 85–117. arXiv:1404.7828. doi:10.1016/j.neunet.2014.09.003. PMID 25462637.

^ Graves, Alex; Schmidhuber, Jürgen (2009).  Bengio, Yoshua; Schuurmans, Dale; Lafferty, John; Williams, Chris editor-K. I.; Culotta, Aron (eds.). "Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks". Neural Information Processing Systems (NIPS) Foundation: 545–552.

^ Hannun, Awni; Case, Carl; Casper, Jared; Catanzaro, Bryan; Diamos, Greg; Elsen, Erich; Prenger, Ryan; Satheesh, Sanjeev; Sengupta, Shubho (2014-12-17). "Deep Speech: Scaling up end-to-end speech recognition". arXiv:1412.5567 [cs.CL].

^ Bo Fan, Lijuan Wang, Frank K. Soong, and Lei Xie (2015). Photo-Real Talking Head with Deep Bidirectional LSTM. In Proceedings of ICASSP 2015.

^ Zen, Heiga; Sak, Hasim (2015). "Unidirectional Long Short-Term Memory Recurrent Neural Network with Recurrent Output Layer for Low-Latency Speech Synthesis" (PDF). Google.com. ICASSP. pp. 4470–4474.

^ Sak, Haşim; Senior, Andrew; Rao, Kanishka; Beaufays, Françoise; Schalkwyk, Johan (September 2015). "Google voice search: faster and more accurate".

^ Sutskever, L.; Vinyals, O.; Le, Q. (2014). "Sequence to Sequence Learning with Neural Networks" (PDF). Electronic Proceedings of the Neural Information Processing Systems Conference. 27: 5346. arXiv:1409.3215. Bibcode:2014arXiv1409.3215S.

^ Jozefowicz, Rafal; Vinyals, Oriol; Schuster, Mike; Shazeer, Noam; Wu, Yonghui (2016-02-07). "Exploring the Limits of Language Modeling". arXiv:1602.02410 [cs.CL].

^ Gillick, Dan; Brunk, Cliff; Vinyals, Oriol; Subramanya, Amarnag (2015-11-30). "Multilingual Language Processing From Bytes". arXiv:1512.00103 [cs.CL].

^ Vinyals, Oriol; Toshev, Alexander; Bengio, Samy; Erhan, Dumitru (2014-11-17). "Show and Tell: A Neural Image Caption Generator". arXiv:1411.4555 [cs.CV].

^ a b Cruse, Holk; Neural Networks as Cybernetic Systems, 2nd and revised edition

^ Elman, Jeffrey L. (1990). "Finding Structure in Time". Cognitive Science. 14 (2): 179–211. doi:10.1016/0364-0213(90)90002-E.

^ Jordan, Michael I. (1997-01-01). Serial Order: A Parallel Distributed Processing Approach. Advances in Psychology. Neural-Network Models of Cognition. 121. pp. 471–495. doi:10.1016/s0166-4115(97)80111-2. ISBN 9780444819314.

^ Kosko, B. (1988). "Bidirectional associative memories". IEEE Transactions on Systems, Man, and Cybernetics. 18 (1): 49–60. doi:10.1109/21.87054.

^ Rakkiyappan, R.; Chandrasekar, A.; Lakshmanan, S.; Park, Ju H. (2 January 2015). "Exponential stability for markovian jumping stochastic BAM neural networks with mode-dependent probabilistic time-varying delays and impulse control". Complexity. 20 (3): 39–65. Bibcode:2015Cmplx..20c..39R. doi:10.1002/cplx.21503.

^ Rául Rojas (1996). Neural networks: a systematic introduction. Springer. p. 336. ISBN 978-3-540-60505-8.

^ Jaeger, Herbert; Haas, Harald (2004-04-02). "Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication". Science. 304 (5667): 78–80. Bibcode:2004Sci...304...78J. CiteSeerX 10.1.1.719.2301. doi:10.1126/science.1091277. PMID 15064413.

^ W. Maass, T. Natschläger, and H. Markram. A fresh look at real-time computation in generic recurrent neural circuits. Technical report, Institute for Theoretical Computer Science, TU Graz, 2002.

^ a b Li, Shuai; Li, Wanqing; Cook, Chris; Zhu, Ce; Yanbo, Gao (2018). "Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN". IEEE Conference on Computer Vision and Pattern Recognition. arXiv:1803.04831.

^ Goller, C.; Küchler, A. (1996). Learning task-dependent distributed representations by backpropagation through structure (PDF). IEEE International Conference on Neural Networks, 1996. 1. p. 347. CiteSeerX 10.1.1.52.4759. doi:10.1109/ICNN.1996.548916. ISBN 978-0-7803-3210-2.

^ Seppo Linnainmaa (1970). The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors. Master's Thesis (in Finnish), Univ. Helsinki, 6-7.

^ Griewank, Andreas; Walther, Andrea (2008). Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation (Second ed.). SIAM. ISBN 978-0-89871-776-1.

^ Socher, Richard; Lin, Cliff; Ng, Andrew Y.; Manning, Christopher D., "Parsing Natural Scenes and Natural Language with Recursive Neural Networks" (PDF), 28th International Conference on Machine Learning (ICML 2011)

^ Socher, Richard; Perelygin, Alex; Y. Wu, Jean; Chuang, Jason; D. Manning, Christopher; Y. Ng, Andrew; Potts, Christopher. "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank" (PDF). Emnlp 2013.

^ a b c d Schmidhuber, Jürgen (1992). "Learning complex, extended sequences using the principle of history compression" (PDF). Neural Computation. 4 (2): 234–242. doi:10.1162/neco.1992.4.2.234.

^ Schmidhuber, Jürgen (2015). "Deep Learning". Scholarpedia. 10 (11): 32832. Bibcode:2015SchpJ..1032832S. doi:10.4249/scholarpedia.32832.

^ a b c Sepp Hochreiter (1991), Untersuchungen zu dynamischen neuronalen Netzen, Diploma thesis. Institut f. Informatik, Technische Univ. Munich. Advisor: J. Schmidhuber.

^ C.L. Giles, C.B. Miller, D. Chen, H.H. Chen, G.Z. Sun, Y.C. Lee, "Learning and Extracting Finite State Automata with Second-Order Recurrent Neural Networks", Neural Computation, 4(3), p. 393, 1992.

^ C.W. Omlin, C.L. Giles, "Constructing Deterministic Finite-State Automata in Recurrent Neural Networks" Journal of the ACM, 45(6), 937-972, 1996.

^ Gers, Felix; Schraudolph, Nicol N.; Schmidhuber, Jürgen (2000). "Learning Precise Timing with LSTM Recurrent Networks (PDF Download Available)". Crossref Listing of Deleted Dois. 1: 115–143. doi:10.1162/153244303768966139. Retrieved 2017-06-13.

^ Bayer, Justin; Wierstra, Daan; Togelius, Julian; Schmidhuber, Jürgen (2009-09-14). Evolving Memory Cell Structures for Sequence Learning (PDF). Artificial Neural Networks – ICANN 2009. Lecture Notes in Computer Science. 5769. Springer, Berlin, Heidelberg. pp. 755–764. doi:10.1007/978-3-642-04277-5_76. ISBN 978-3-642-04276-8.

^ Fernández, Santiago; Graves, Alex; Schmidhuber, Jürgen (2007). "Sequence labelling in structured domains with hierarchical recurrent neural networks". Proc. 20th Int. Joint Conf. On Artificial In℡ligence, Ijcai 2007: 774–779. CiteSeerX 10.1.1.79.1887.

^ Graves, Alex; Fernández, Santiago; Gomez, Faustino (2006). "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks". In Proceedings of the International Conference on Machine Learning, ICML 2006: 369–376. CiteSeerX 10.1.1.75.6306.

^ Gers, F. A.; Schmidhuber, E. (November 2001). "LSTM recurrent networks learn simple context-free and context-sensitive languages". IEEE Transactions on Neural Networks. 12 (6): 1333–1340. doi:10.1109/72.963769. ISSN 1045-9227. PMID 18249962.

^ Heck, Joel; Salem, Fathi M. (2017-01-12). "Simplified Minimal Gated Unit Variations for Recurrent Neural Networks". arXiv:1701.03452 [cs.NE].

^ Dey, Rahul; Salem, Fathi M. (2017-01-20). "Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks". arXiv:1701.05923 [cs.NE].

^ Chung, Junyoung; Gulcehre, Caglar; Cho, KyungHyun; Bengio, Yoshua (2014). "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling". arXiv:1412.3555 [cs.NE].

^ "Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU/LSTM RNN with Python and Theano – WildML". Wildml.com. 2015-10-27. Retrieved May 18, 2016.

^ Graves, Alex; Schmidhuber, Jürgen (2005-07-01). "Framewise phoneme classification with bidirectional LSTM and other neural network architectures". Neural Networks. IJCNN 2005. 18 (5): 602–610. CiteSeerX 10.1.1.331.5800. doi:10.1016/j.neunet.2005.06.042. PMID 16112549.

^ Thireou, T.; Reczko, M. (July 2007). "Bidirectional Long Short-Term Memory Networks for Predicting the Subcellular Localization of Eukaryotic Proteins". IEEE/ACM Transactions on Computational Biology and Bioinformatics. 4 (3): 441–446. doi:10.1109/tcbb.2007.1015. PMID 17666763.

^ Harvey, Inman; Husbands, P.; Cliff, D. (1994), "Seeing the light: Artificial evolution, real vision", 3rd international conference on Simulation of adaptive behavior: from animals to animats 3, pp. 392–401

^ Quinn, Matthew (2001). Evolving communication without dedicated communication channels. Advances in Artificial Life. Lecture Notes in Computer Science. 2159. pp. 357–366. CiteSeerX 10.1.1.28.5890. doi:10.1007/3-540-44811-X_38. ISBN 978-3-540-42567-0.

^ Beer, R.D. (1997). "The dynamics of adaptive behavior: A research program". Robotics and Autonomous Systems. 20 (2–4): 257–289. doi:10.1016/S0921-8890(96)00063-2.

^ Paine, Rainer W.; Tani, Jun (2005-09-01). "How Hierarchical Control Self-organizes in Artificial Adaptive Systems". Adaptive Behavior. 13 (3): 211–225. doi:10.1177/105971230501300303.

^ Recurrent Multilayer Perceptrons for Identification and Control: The Road to Applications. 1995. CiteSeerX 10.1.1.45.3527.

^ Yamashita, Yuichi; Tani, Jun (2008-11-07). "Emergence of Functional Hierarchy in a Multiple Timescale Neural Network Model: A Humanoid Robot Experiment". PLOS Computational Biology. 4 (11): e1000220. Bibcode:2008PLSCB...4E0220Y. doi:10.1371/journal.pcbi.1000220. PMC 2570613. PMID 18989398.

^ Shibata Alnajjar, Fady; Yamashita, Yuichi; Tani, Jun (2013). "The hierarchical and functional connectivity of higher-order cognitive mechanisms: neurorobotic model to investigate the stability and flexibility of working memory". Frontiers in Neurorobotics. 7: 2. doi:10.3389/fnbot.2013.00002. PMC 3575058. PMID 23423881.

^ Graves, Alex; Wayne, Greg; Danihelka, Ivo (2014). "Neural Turing Machines". arXiv:1410.5401 [cs.NE].

^ Sun, Guo-Zheng; Giles, C. Lee; Chen, Hsing-Hen (1998). "The Neural Network Pushdown Automaton: Architecture, Dynamics and Training".  In Giles, C. Lee; Gori, Marco (eds.). Adaptive Processing of Sequences and Data Structures. Lecture Notes in Computer Science. Springer Berlin Heidelberg. pp. 296–345. CiteSeerX 10.1.1.56.8723. doi:10.1007/bfb0054003. ISBN 9783540643418.

^ Werbos, Paul J. (1988). "Generalization of backpropagation with application to a recurrent gas market model". Neural Networks. 1 (4): 339–356. doi:10.1016/0893-6080(88)90007-x.

^ Rumelhart, David E. (1985). Learning Internal Representations by Error Propagation. Institute for Cognitive Science, University of California, San Diego.

^ Robinson, A. J. (1987). The Utility Driven Dynamic Error Propagation Network. Technical Report CUED/F-INFENG/TR.1. University of Cambridge Department of Engineering.

^ Williams, R. J.; Zipser. Gradient-based learning algorithms for recurrent networks and their computational complexity, D. (1 February 2013).  Chauvin, Yves; Rumelhart, David E. (eds.). Backpropagation: Theory, Architectures, and Applications. Psychology Press. ISBN 978-1-134-77581-1.

^ SCHMIDHUBER, JURGEN (1989-01-01). "A Local Learning Algorithm for Dynamic Feedforward and Recurrent Networks". Connection Science. 1 (4): 403–412. doi:10.1080/09540098908915650.

^ Príncipe, José C.; Euliano, Neil R.; Lefebvre, W. Curt (2000). Neural and adaptive systems: fundamentals through simulations. Wiley. ISBN 978-0-471-35167-2.

^ Yann, Ollivier; Corentin, Tallec; Guillaume, Charpiat (2015-07-28). "Training recurrent networks online without backtracking". arXiv:1507.07680 [cs.NE].

^ Schmidhuber, Jürgen (1992-03-01). "A Fixed Size Storage O(n3) Time Complexity Learning Algorithm for Fully Recurrent Continually Running Networks". Neural Computation. 4 (2): 243–248. doi:10.1162/neco.1992.4.2.243.

^ Williams, R. J. (1989). "Complexity of exact gradient computation algorithms for recurrent neural networks. Technical Report Technical Report NU-CCS-89-27". Boston: Northeastern University, College of Computer Science.

^ Pearlmutter, Barak A. (1989-06-01). "Learning State Space Trajectories in Recurrent Neural Networks". Neural Computation. 1 (2): 263–269. doi:10.1162/neco.1989.1.2.263.

^ Hochreiter, S.;  et al. (15 January 2001). "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies".  In Kolen, John F.; Kremer, Stefan C. (eds.). A Field Guide to Dynamical Recurrent Networks. John Wiley & Sons. ISBN 978-0-7803-5369-5.

^ Campolucci; Uncini, A.; Piazza, F.; Rao, B. D. (1999). "On-Line Learning Algorithms for Locally Recurrent Neural Networks". IEEE Transactions on Neural Networks. 10 (2): 253–271. CiteSeerX 10.1.1.33.7550. doi:10.1109/72.750549. PMID 18252525.

^ Wan, E. A.; Beaufays, F. (1996). "Diagrammatic derivation of gradient algorithms for neural networks". Neural Computation. 8: 182–201. doi:10.1162/neco.1996.8.1.182.

^ a b Campolucci, P.; Uncini, A.; Piazza, F. (2000). "A Signal-Flow-Graph Approach to On-line Gradient Calculation". Neural Computation. 12 (8): 1901–1927. CiteSeerX 10.1.1.212.5406. doi:10.1162/089976600300015196.

^ Gomez, F. J.; Miikkulainen, R. (1999), "Solving non-Markovian control tasks with neuroevolution" (PDF), IJCAI 99, Morgan Kaufmann, retrieved 5 August 2017

^ "Applying Genetic Algorithms to Recurrent Neural Networks for Learning Network Parameters and Architecture".

^ Gomez, Faustino; Schmidhuber, Jürgen; Miikkulainen, Risto (June 2008). "Accelerated Neural Evolution Through Cooperatively Coevolved Synapses". J. Mach. Learn. Res. 9: 937–965.

^ Siegelmann, Hava T.; Horne, Bill G.; Giles, C. Lee (1995). Computational Capabilities of Recurrent NARX Neural Networks. University of Maryland. pp. 208–215.

^ Cade Metz (May 18, 2016). "Google Built Its Very Own Chips to Power Its AI Bots". Wired.

^ Mayer, H.; Gomez, F.; Wierstra, D.; Nagy, I.; Knoll, A.; Schmidhuber, J. (October 2006). A System for Robotic Heart Surgery that Learns to Tie Knots Using Recurrent Neural Networks. 2006 IEEE/RSJ International Conference on Intelligent Robots and Systems. pp. 543–548. CiteSeerX 10.1.1.218.3399. doi:10.1109/IROS.2006.282190. ISBN 978-1-4244-0258-8.

^ Wierstra, Daan; Schmidhuber, J.; Gomez, F. J. (2005). "Evolino: Hybrid Neuroevolution/Optimal Linear Search for Sequence Learning". Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI), Edinburgh: 853–858.

^ Petneházi, Gábor (2019-01-01). "Recurrent neural networks for time series forecasting". arXiv:1901.00069 [cs.LG].

^ Graves, A.; Schmidhuber, J. (2005). "Framewise phoneme classification with bidirectional LSTM and other neural network architectures". Neural Networks. 18 (5–6): 602–610. CiteSeerX 10.1.1.331.5800. doi:10.1016/j.neunet.2005.06.042. PMID 16112549.

^ Fernández, Santiago; Graves, Alex; Schmidhuber, Jürgen (2007). An Application of Recurrent Neural Networks to Discriminative Keyword Spotting. Proceedings of the 17th International Conference on Artificial Neural Networks. ICANN'07. Berlin, Heidelberg: Springer-Verlag. pp. 220–229. ISBN 978-3540746935.

^ Graves, Alex; Mohamed, Abdel-rahman; Hinton, Geoffrey (2013). "Speech Recognition with Deep Recurrent Neural Networks". Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on: 6645–6649.

^ Chang, Edward F.; Chartier, Josh; Anumanchipalli, Gopala K. (24 April 2019). "Speech synthesis from neural decoding of spoken sentences". Nature. 568 (7753): 493–498. doi:10.1038/s41586-019-1119-1. ISSN 1476-4687.

^ Malhotra, Pankaj; Vig, Lovekesh; Shroff, Gautam; Agarwal, Puneet (April 2015). "Long Short Term Memory Networks for Anomaly Detection in Time Series" (PDF). European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning — ESANN 2015.

^ Gers, F.; Schraudolph, N.; Schmidhuber, J. (2002). "Learning precise timing with LSTM recurrent networks" (PDF). Journal of Machine Learning Research. 3: 115–143.

^ Eck, Douglas; Schmidhuber, Jürgen (2002-08-28). Learning the Long-Term Structure of the Blues. Artificial Neural Networks — ICANN 2002. Lecture Notes in Computer Science. 2415. Springer, Berlin, Heidelberg. pp. 284–289. CiteSeerX 10.1.1.116.3620. doi:10.1007/3-540-46084-5_47. ISBN 978-3540460848.

^ Schmidhuber, J.; Gers, F.; Eck, D.; Schmidhuber, J.; Gers, F. (2002). "Learning nonregular languages: A comparison of simple recurrent networks and LSTM". Neural Computation. 14 (9): 2039–2041. CiteSeerX 10.1.1.11.7369. doi:10.1162/089976602320263980. PMID 12184841.

^ Gers, F. A.; Schmidhuber, J. (2001). "LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages" (PDF). IEEE Transactions on Neural Networks. 12 (6): 1333–1340. doi:10.1109/72.963769. PMID 18249962.

^ Perez-Ortiz, J. A.; Gers, F. A.; Eck, D.; Schmidhuber, J. (2003). "Kalman filters improve LSTM network performance in problems unsolvable by traditional recurrent nets". Neural Networks. 16 (2): 241–250. CiteSeerX 10.1.1.381.1992. doi:10.1016/s0893-6080(02)00219-8. PMID 12628609.

^ A. Graves, J. Schmidhuber. Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks. Advances in Neural Information Processing Systems 22, NIPS'22, pp 545–552, Vancouver, MIT Press, 2009.

^ Graves, Alex; Fernández, Santiago; Liwicki, Marcus; Bunke, Horst; Schmidhuber, Jürgen (2007). Unconstrained Online Handwriting Recognition with Recurrent Neural Networks. Proceedings of the 20th International Conference on Neural Information Processing Systems. NIPS'07. USA: Curran Associates Inc. pp. 577–584. ISBN 9781605603520.

^ M. Baccouche, F. Mamalet, C Wolf, C. Garcia, A. Baskurt. Sequential Deep Learning for Human Action Recognition. 2nd International Workshop on Human Behavior Understanding (HBU), A.A. Salah, B. Lepri ed. Amsterdam, Netherlands. pp. 29–39. Lecture Notes in Computer Science 7065. Springer. 2011

^ Hochreiter, S.; Heusel, M.; Obermayer, K. (2007). "Fast model-based protein homology detection without alignment". Bioinformatics. 23 (14): 1728–1736. doi:10.1093/bioinformatics/btm247. PMID 17488755.

^ Thireou, T.; Reczko, M. (2007). "Bidirectional Long Short-Term Memory Networks for predicting the subcellular localization of eukaryotic proteins". IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB). 4 (3): 441–446. doi:10.1109/tcbb.2007.1015. PMID 17666763.

^ Tax, N.; Verenich, I.; La Rosa, M.; Dumas, M. (2017). Predictive Business Process Monitoring with LSTM neural networks. Proceedings of the International Conference on Advanced Information Systems Engineering (CAiSE). Lecture Notes in Computer Science. 10253. pp. 477–492. arXiv:1612.02130. doi:10.1007/978-3-319-59536-8_30. ISBN 978-3-319-59535-1.

^ Choi, E.; Bahadori, M.T.; Schuetz, E.; Stewart, W.; Sun, J. (2016). "Doctor AI: Predicting Clinical Events via Recurrent Neural Networks". Proceedings of the 1st Machine Learning for Healthcare Conference: 301–318.


Further reading[edit]
Mandic, D. & Chambers, J. (2001). Recurrent Neural Networks for Prediction: Learning Algorithms, Architectures and Stability. Wiley. ISBN 978-0-471-49517-8.
External links[edit]
RNNSharp CRFs based on recurrent neural networks (C#, .NET)
Recurrent Neural Networks with over 60 RNN papers by Jürgen Schmidhuber's group at Dalle Molle Institute for Artificial Intelligence Research
Elman Neural Network implementation for WEKA
Recurrent Neural Nets & LSTMs in Java
an alternative try for complete RNN / Reward driven



Coordinates: 37°22′14.62″N 121°57′49.46″W﻿ / ﻿37.3707278°N 121.9637389°W﻿ / 37.3707278; -121.9637389

Nvidia CorporationNvidia's current logo, in use since 2006Headquarters at Santa Clara in 2018TypePublicTraded asNASDAQ: NVDANASDAQ-100 componentS&P 100 componentS&P 500 componentIndustrySemiconductorsVideo gamesConsumer electronicsComputer hardwareFoundedApril 1993; 26 years ago (1993-04)FoundersJensen HuangCurtis PriemChris MalachowskyHeadquartersSanta Clara, California, U.S.Area servedWorldwideKey peopleJensen Huang (President & CEO)Colette M. Kress (CFO)ProductsGraphics processing units (GPU)Central processing units (CPU)ChipsetsDriversRevenue US$9.714 billion (2017)[1]Operating income US$3.210 billion (2017)[1]Net income US$3.047 billion (2017)[1]Total assets US$11.241 billion (2017)[1]Total equity US$7.471 billion (2017)[1]Number of employees11,528 [1] (2018)SubsidiariesNVIDIA Advanced Rendering CenterWebsitewww.nvidia.comdeveloper.nvidia.comwww.geforce.com
Nvidia Corporation (/ɛnˈvɪdiə/ en-VID-ee-ə),[2] more commonly referred to as Nvidia (stylized as NVIDIA), is an American technology company incorporated in Delaware and based in Santa Clara, California.[3] It designs graphics processing units (GPUs) for the gaming and professional markets, as well as system on a chip units (SoCs) for the mobile computing and automotive market. Its primary GPU product line, labeled "GeForce", is in direct competition with Advanced Micro Devices' (AMD) "Radeon" products. Nvidia expanded its presence in the gaming industry with its handheld Shield Portable, Shield Tablet and Shield Android TV.
Since 2014,[citation needed] Nvidia has shifted to become a platform company focused on four markets – gaming, professional visualization, data centers and auto. Nvidia is also now focused on artificial intelligence.[4]
In addition to GPU manufacturing, Nvidia provides parallel processing capabilities to researchers and scientists that allow them to efficiently run high-performance applications. They are deployed in supercomputing sites around the world.[5][6] More recently, it has moved into the mobile computing market, where it produces Tegra mobile processors for smartphones and tablets as well as vehicle navigation and entertainment systems.[7][8][9] In addition to AMD, its competitors include Intel, Qualcomm and Arm (e.g., because of Denver, while Nvidia also licenses Arm's designs).

Contents

1 Company history

1.1 Founders and initial investment
1.2 Major releases and acquisitions
1.3 Class action lawsuit
1.4 Apple/Nvidia web driver controversy


2 Finances
3 GPU Technology Conference
4 Product families
5 Open-source software support
6 Deep learning

6.1 DGX


7 Inception Program

7.1 2018 winners[88]
7.2 2017 winners[89]


8 See also
9 References
10 External links


Company history[edit]
 Aerial view of the new Nvidia headquarters building and surrounding campus and area in Santa Clara, California, in 2017.  Apple Park is visible in the distance.
In the early 1990s, the three co-founders hypothesized that the proper direction for the next wave of computing would be accelerated or graphics based. They believed that this model of computing could solve problems that general-purpose computing fundamentally couldn't. They also observed that video games were some of the most computationally challenging problems, but would have incredibly high sales volume. With a capital of $40,000, the company was born. The company initially had no name and the co-founders named all their files NV, as in "next version". The need to incorporate the company prompted the co-founders to review all words with those two letters, leading them to "invidia", the Latin word for "envy".[10]

Founders and initial investment[edit]
Three people co-founded Nvidia in April 1993:[11][12]

Jensen Huang (CEO as of 2019[update]), a Taiwanese American, previously director of CoreWare at LSI Logic and a microprocessor designer at Advanced Micro Devices (AMD)
Chris Malachowsky, an electrical engineer who worked at Sun Microsystems
Curtis Priem, previously a senior staff engineer and graphics chip designer at Sun Microsystems
The company received $20 million of venture capital funding from Sequoia Capital and others.[13]

Major releases and acquisitions[edit]
The release of the RIVA TNT in 1998 solidified Nvidia's reputation for developing capable graphics adapters. In late 1999, Nvidia released the GeForce 256 (NV10), most notably introducing on-board transformation and lighting (T&L) to consumer-level 3D hardware. Running at 120 MHz and featuring four pixel pipelines, it implemented advanced video acceleration, motion compensation and hardware sub-picture alpha blending. The GeForce outperformed existing products by a wide margin.
Due to the success of its products, Nvidia won the contract to develop the graphics hardware for Microsoft's Xbox game console, which earned Nvidia a $200 million advance. However, the project took many of its best engineers away from other projects. In the short term this did not matter, and the GeForce2 GTS shipped in the summer of 2000. In December 2000, Nvidia reached an agreement to acquire the intellectual assets of its one-time rival 3dfx, a pioneer in consumer 3D graphics technology leading the field from mid 1990s until 2000.[14][15] The acquisition process was finalized in April 2002.[16]
In July 2002, Nvidia acquired Exluna for an undisclosed sum. Exluna made software rendering tools and the personnel were merged into the Cg project.[17] In August 2003, Nvidia acquired MediaQ for approximately US$70 million.[18] On April 22, 2004, Nvidia acquired iReady, also a provider of high performance TCP/IP and iSCSI offload solutions.[19] In December 2004, it was announced that Nvidia would assist Sony with the design of the graphics processor (RSX) in the PlayStation 3 game console. 
On December 14, 2005, Nvidia acquired ULI Electronics, which at the time supplied third-party southbridge parts for chipsets to ATI, Nvidia's competitor.[20] In March 2006, Nvidia acquired Hybrid Graphics.[21] In December 2006, Nvidia, along with its main rival in the graphics industry AMD (which had acquired ATI), received subpoenas from the U.S. Department of Justice regarding possible antitrust violations in the graphics card industry.[22]
Forbes named Nvidia its Company of the Year for 2007, citing the accomplishments it made during the said period as well as during the previous five years.[23] On January 5, 2007, Nvidia announced that it had completed the acquisition of PortalPlayer, Inc.[24] In February 2008, Nvidia acquired Ageia, developer of the PhysX physics engine and physics processing unit. Nvidia announced that it planned to integrate the PhysX technology into its future GPU products.[25][26]
In November 2011, after initially unveiling it at Mobile World Congress, Nvidia released its Tegra 3 ARM system-on-chip for mobile devices. Nvidia claimed that the chip featured the first-ever quad-core mobile CPU.[27][28] In May 2011, it was announced that Nvidia had agreed to acquire Icera, a baseband chip making company in the UK, for $367 million.[29] In January 2013, Nvidia unveiled the Tegra 4, as well as the Nvidia Shield, an Android-based handheld game console powered by the new system-on-chip.[30] On July 29, 2013, Nvidia announced that they acquired PGI from STMicroelectronics.[31]
On May 6, 2016, Nvidia unveiled the first GeForce 10 series GPUs, the GTX 1080 and 1070, based on the company's new Pascal microarchitecture. Nvidia claimed that both models outperformed its Maxwell-based Titan X model; the models incorporate GDDR5X and GDDR5 memory respectively, and use a 16 nm manufacturing process. The architecture also supports a new hardware feature known as simultaneous multi-projection (SMP), which is designed to improve the quality of multi-monitor and virtual reality rendering.[32][33][34] Laptops that include these GPUs and are sufficiently thin – as of late 2017, under 0.8 inches (20 mm) – have been designated as meeting Nvidia's "Max-Q" design standard.[35]
In July 2016, Nvidia agreed to a settlement for a false advertising lawsuit regarding its GTX 970 model, as the models were unable to use all of their advertised 4 GB of RAM due to limitations brought by the design of its hardware.[36] In May 2017, Nvidia announced a partnership with Toyota Motor Corp. Toyota will use Nvidia's Drive PX-series artificial intelligence platform for its autonomous vehicles.[37] In July 2017, Nvidia and Chinese search giant Baidu, Inc. announced a far-reaching AI partnership that includes cloud computing, autonomous driving, consumer devices, and Baidu's open-source AI framework PaddlePaddle. Baidu unveiled that Nvidia 's Drive PX 2 AI will be the foundation of its autonomous-vehicle platform.[38]
Nvidia officially released the NVIDIA TITAN V on December 7, 2017.[39][40]
Nvidia officially released the Nvidia Quadro GV100 on March 27, 2018.[41]
Nvidia officially released RTX 2080GPUs September 27, 2018.
In 2018, Google announced that Nvidia's Tesla P4 graphic cards would be integrated into Google Cloud service's artificial intelligence.[42]
On 11 March 2019, Nvidia announced a deal to buy Mellanox Technologies for $6.9 billion[43] to substantially expand its footprint in the high-performance computing market.
In May 2019 Nvidia announced new RTX Studio laptops. The creators say that the new laptop is going to be seven times faster than a top-end MacBook Pro with a Core i9 and AMD’s Radeon Pro Vega 20 graphics in apps like Maya and RedCine-X Pro.[44]

Class action lawsuit[edit]
In July 2008, Nvidia took a write-down of approximately $200 million on its first-quarter revenue, after reporting that certain mobile chipsets and GPUs produced by the company had "abnormal failure rates" due to manufacturing defects. Nvidia, however, did not reveal the affected products. In September 2008, Nvidia became the subject of a class action lawsuit over the defects, claiming that the faulty GPUs had been incorporated into certain laptop models manufactured by Apple Inc., Dell, and HP. In September 2010, Nvidia reached a settlement, in which it would reimburse owners of the affected laptops for repairs or, in some cases, replacement.[45][46]  On January 10, 2011, Nvidia signed a six-year, $1.5 billion cross-licensing agreement with Intel, ending all litigation between the two companies.[47]

Apple/Nvidia web driver controversy[edit]
In May 2018, on the Nvidia user forum, a thread was started[48] asking the company to update users when they would release web drivers for its cards installed on legacy Mac Pro 'cheesegrater' machines up to mid 2012 5,1 running the macOS Mojave operating system 10.14.  Web drivers are required to enable graphics acceleration and multiple display monitor capabilities of the GPU.  On its Mojave update info website, Apple stated that macOS Mojave would run on legacy machines with 'metal compatible' graphics cards[49] and listed metal compatible GPU, including some manufactured by Nvidia.[50] However, this list did not include metal compatible cards that currently work in macOS High Sierra using Nvidia developed web drivers.  In September, Nvidia responded, "Apple fully control drivers for Mac OS. But if Apple allows, our engineers are ready and eager to help Apple deliver great drivers for Mac OS 10.14 (Mojave)."[51]  In October, Nvidia followed this up with another public announcement, "Apple fully controls drivers for Mac OS. Unfortunately, Nvidia currently cannot release a driver unless it is approved by Apple,"[52] suggesting a possible rift between the two companies.[53]  By January 2019, with still no sign of the enabling web drivers, Apple Insider weighed into the controversy with a claim that Apple management "doesn't want Nvidia support in macOS".[54]  The following month, Apple Insider followed this up with another claim that Nvidia support was abandoned because of "relational issues in the past",[55]  and that Apple was developing its own GPU technology.[56]  Without Apple approved Nvidia web drivers, Apple users are faced with replacing their Nvidia cards with a competing supported brand, such as AMD Radeon from the list recommended by Apple.[57]

Finances[edit]
For the fiscal year 2018, Nvidia reported earnings of US$3.047 billion, with an annual revenue of US$9.714 billion, an increase of 40.6% over the previous fiscal cycle. Nvidia's shares traded at over $245 per share, and its market capitalization was valued at over US$120.6 billion in September 2018.[58]



Year[58]

Revenuein mil. USD$

Net incomein mil. USD$

Total assetsin mil. USD$

Price per sharein USD$

Employees


2005

2,010

89

1,629

8.81




2006

2,376

301

1,955

16.76




2007

3,069

449

2,675

25.68




2008

4,098

798

3,748

14.77




2009

3,425

−30

3,351

10.97




2010

3,326

−68

3,586

12.56




2011

3,543

253

4,495

15.63




2012

3,998

581

5,553

12.52




2013

4,280

563

6,412

13.38

5,783


2014

4,130

440

7,251

17.83

6,384


2015

4,682

631

7,201

23.20

6,658


2016

5,010

614

7,370

53.33

6,566


2017

6,910

1,666

9,841

149.38

7,282


2018

9,714

3,047

11,241

245.75

11,528

GPU Technology Conference[edit]
NVIDIA’s GPU Technology Conference (GTC) is a series of technical conferences held around the world.[59] It originated in 2009 in San Jose, California, with an initial focus on the potential for solving computing challenges through GPUs.[60] In recent years, the conference focus has shifted to various applications of artificial intelligence and deep learning, including: self-driving cars, healthcare, high performance computing, and NVIDIA Deep Learning Institute (DLI) training.[61] GTC 2018 attracted over 8400 attendees.[59]

Product families[edit]
 An Nvidia Shield Tablet
Nvidia's family includes primarily graphics, wireless communication, PC processors and automotive hardware/software. Some families are listed below:

GeForce, consumer-oriented graphics processing products
Quadro, computer-aided design and digital content creation workstation graphics processing products
NVS, multi-display business graphics solution
Tegra, a system on a chip series for mobile devices
Tesla, dedicated general purpose GPU for high-end image generation applications in professional and scientific fields
nForce, a motherboard chipset created by Nvidia for Intel (Celeron, Pentium and Core 2) and AMD (Athlon and Duron) microprocessors
Nvidia Grid, a set of hardware and services by Nvidia for graphics virtualization
Nvidia Shield, a range of gaming hardware including the Shield Portable, Shield Tablet and, most recently, the Shield Android TV
Nvidia Drive automotive solutions, a range of hardware and software products for assisting car drivers. The Drive PX-series is a high performance computer platform aimed at autonomous driving through deep learning,[62] while Driveworks is an operating system for driverless cars.[63]
Open-source software support[edit]
See also: Free and open-source graphics device driver, Mesa 3D, and nouveau (software)
Until September 23, 2013, Nvidia had not published any documentation for its hardware,[64] meaning that programmers could not write free and open-source device driver for its products without resorting to (clean room) reverse engineering.
Instead, Nvidia provides its own binary GeForce graphics drivers for X.Org and an open-source library that interfaces with the Linux, FreeBSD or Solaris kernels and the proprietary graphics software. Nvidia also provided but stopped supporting an obfuscated open-source driver that only supports two-dimensional hardware acceleration and ships with the X.Org distribution.[65]
The proprietary nature of Nvidia's drivers has generated dissatisfaction within free-software communities.[66] Some Linux and BSD users insist on using only open-source drivers and regard Nvidia's insistence on providing nothing more than a binary-only driver as inadequate, given that competing manufacturers such as Intel offer support and documentation for open-source developers and that others (like AMD) release partial documentation and provide some active development.[67][68]
Because of the closed nature of the drivers, Nvidia video cards cannot deliver adequate features on some platforms and architectures given that the company only provides x86/x64 and ARMv7-A driver builds.[69] As a result, support for 3D graphics acceleration in Linux on PowerPC does not exist, nor does support for Linux on the hypervisor-restricted PlayStation 3 console.
Some users claim that Nvidia's Linux drivers impose artificial restrictions, like limiting the number of monitors that can be used at the same time, but the company has not commented on these accusations.[70]

Deep learning[edit]
Nvidia GPUs are used in deep learning, artificial intelligence, and accelerated analytics. The company developed GPU-based deep learning in order to use artificial intelligence to approach problems like cancer detection, weather prediction, and self-driving vehicles.[71] They are included in all Tesla vehicles.[72] The purpose is to help networks learn to “think”.[73] According to TechRepublic, Nvidia GPUs "work well for deep learning tasks because they are designed for parallel computing and do well to handle the vector and matrix operations that are prevalent in deep learning".[74] These GPUs are used by researchers, laboratories, tech companies and enterprise companies.[75] In 2009, Nvidia was involved in what was called the "big bang" of deep learning, "as deep-learning neural networks were combined with Nvidia graphics processing units (GPUs)".[76] That year, the Google Brain used Nvidia GPUs to create Deep Neural Networks capable of machine learning, where Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times.[77]

DGX[edit]
In April 2016, Nvidia produced the DGX-1 supercomputer based on an 8 GPU cluster, to improve the ability of users to use deep learning by combining GPUs with integrated deep learning software.[78] It also developed Nvidia Tesla K80 and P100 GPU-based virtual machines, which are available through Google Cloud, which Google installed in November 2016.[79] Microsoft added GPU servers in a preview offering of its N series based on Nvidia's Tesla K80s, each containing 4992 processing cores. Later that year, AWS's P2 instance was produced using up to 16 Nvidia Tesla K80 GPUs. That month Nvidia also partnered with IBM to create a software kit that boosts the AI capabilities of Watson,[80] called IBM PowerAI.[81][82] Nvidia also offers its own NVIDIA Deep Learning software development kit.[83] In 2017, the GPUs were also brought online at the RIKEN Center for Advanced Intelligence Project for Fujitsu.[84] The company's deep learning technology led to a boost in its 2017 earnings.[85]
In May 2018, researchers at the artificial intelligence department of Nvidia realized the possibility that a robot can learn to perform a job simply by observing the person doing the same job. They have created a system that, after a short revision and testing, can already be used to control the universal robots of the next generation. In addition to GPU manufacturing, Nvidia provides parallel processing capabilities to researchers and scientists that allow them to efficiently run high-performance applications.[86]

Inception Program[edit]
Nvidia's Inception Program was created to support startups making exceptional advances in the fields of AI and Data Science. Award winners are announced at Nvidia's GTC Conference. There are currently 2,800 startups in the Inception Program.[87]

2018 winners[88][edit]
Subtle Medical (healthcare)
AiFi (enterprise)
Kinema Systems (autonomous vehicles)
2017 winners[89][edit]
Genetesis  (social innovation)
Athelas (hottest emerging)
Deep Instinct (most disruptive)
See also[edit]


San Francisco Bay Area portal
Companies portal

CUDA
Fast approximate anti-aliasing
GeForce
General-purpose computing on graphics processing units
Graphics processing unit
G-Sync
List of Nvidia 3D Vision Ready games
List of Nvidia graphics processing units
Molecular modeling on GPUs
Nvision
Nvidia demos
Nvidia Ion
Nvidia Shadowplay
OpenCL
OptiX
PhysX
Project Denver
Shield Android TV
Shield Portable
Shield Tablet
Tegra
GeForce 10 series
GeForce 20 series

References[edit]


^ a b c d e f "Annual Report (Form 10-K)" (PDF). Retrieved March 17, 2018..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ "NVIDIA Logo Guidelines at a Glance" (PDF). NVIDIA.com. NVIDIA. Retrieved March 21, 2018.

^ "NVIDIA Corporation - Investor Resources - FAQs". investor.nvidia.com.

^ Freund, Karl (November 17, 2016). "NVIDIA Is Not Just Accelerating AI, It Aims To Reshape Computing". Forbes. Retrieved February 11, 2017.

^ Clark, Don (August 4, 2011). "J.P. Morgan Shows Benefits from Chip Change". WSJ Digits Blog. Retrieved September 14, 2011.

^ "Top500 Supercomputing Sites". Top500. Retrieved September 14, 2011.

^ Burns, Chris. "2011 The Year of Nvidia dominating Android Superphones and tablets". SlashGear. Retrieved September 14, 2011.

^ "Tegra Super Tablets". Nvidia. Retrieved September 14, 2011.

^ "Tegra Super Phones". Nvidia. Retrieved September 14, 2011.

^ Nusca, Andrew (16 November 2017). "This Man Is Leading an AI Revolution in Silicon Valley—And He's Just Getting Started". Fortune. Archived from the original on 16 November 2017. Retrieved 28 November 2017.

^ "Company Info". Nvidia.com. Retrieved November 9, 2010.

^ "Jensen Huang: Executive Profile & Biography". Bloomberg News. Retrieved June 21, 2018.

^ Williams, Elisa (April 15, 2002). "Crying wolf". Forbes. Retrieved February 11, 2017. Huang, a chip designer at AMD and LSI Logic, cofounded the company in 1993 with $20 million from Sequoia Capital and others.

^ Perez, Derek; Hara, Michael (December 15, 2000). "NVIDIA to Acquire 3dfx Core Graphics Assets" (Press release). Santa Clara, CA. Retrieved January 23, 2017.

^ Leupp, Alex; Sellers, Scott (December 15, 2000). "3dfx Announces Three Major Initiatives To Protect Creditors and Maximize Shareholder Value" (Press release). San Jose, CA. Archived from the original on February 5, 2001. Retrieved January 23, 2017. Board of Directors Initiates Cost-Cutting Measures, Recommends to Shareholders Sale of Company Assets to NVIDIA Corporation for $112 million and Dissolution of Company

^ Kanellos, Michael (April 11, 2002). "NNvidia buys out 3dfx graphics chip business". CNET. Retrieved January 23, 2017.

^ Becker, David. "Nvidia buys out Exluna". News.cnet.com. Retrieved November 9, 2010.

^ "NVIDIA Completes Purchase of MediaQ". Press Release. NVIDIA Corporation. August 21, 2003. Retrieved August 21, 2016.

^ "NVIDIA Announces Acquisition of iReady". Press Release. NVIDIA Corporation. April 22, 2004. Retrieved August 21, 2016.

^ "NVIDIA to Acquire ULi Electronics, a Leading Developer of Core Logic Technology". Press Release. NVIDIA Corporation. December 14, 2005. Retrieved August 21, 2016.

^ Smith, Tony (March 22, 2006). "Nvidia acquires Hybrid Graphics - Middleware purchase". Hardware. The Register. Retrieved August 21, 2016.

^ Krazit, Tom; McCarthy, Caroline (December 1, 2006). "Justice Dept. subpoenas AMD, Nvidia". New York Times. Archived from the original on December 8, 2006.

^ Brian Caulfield (January 7, 2008). "Shoot to Kill". Forbes. Retrieved December 26, 2007.

^ "Nvidia acquires PortalPlayer". Press Release. NVIDIA Corporation. January 5, 2007. Retrieved August 21, 2016.

^ "Nvidia to acquire Ageia for the PhysX chip". CNET. Retrieved May 26, 2017.

^ "Did NVIDIA cripple its CPU gaming physics library to spite Intel?". Ars Technica. Retrieved May 26, 2017.

^ "Nvidia Tegra 3: what you need to know". Techradar. Retrieved May 26, 2017.

^ "Nvidia Quad Core Mobile Processors Coming in August". PC World. Retrieved February 15, 2011.

^ "Cambridge coup as Icera goes to Nvidia for £225m". Business Weekly. May 9, 2011. Retrieved May 10, 2011.

^ "Nvidia announces Project Shield handheld gaming system with 5-inch multitouch display, available in Q2 of this year". The Verge. Retrieved May 26, 2017.

^ "NVIDIA Pushes Further into HPC With Portland Group Acquisition - insideHPC". insideHPC. 2013-07-29. Retrieved 2017-08-25.

^ "Nvidia's new graphics cards are a big deal". The Verge. Retrieved May 26, 2017.

^ Mark Walton (May 7, 2016). "Nvidia's GTX 1080 and GTX 1070 revealed: Faster than Titan X at half the price". Ars Technica.

^ 
Joel Hruska (May 10, 2016). "Nvidia's Ansel, VR Funhouse apps will enhance screenshots, showcase company's VR technology". ExtremeTech.

^ Crider, Michael (October 5, 2017). "What Are NVIDIA MAX-Q Laptops?". How-To Geek. Retrieved December 18, 2017.

^ Smith, Ryan. "Update: NVIDIA GeForce GTX 970 Settlement Claims Website Now Open". Anandtech. Purch, Inc. Retrieved November 15, 2016.

^ Alexandria Sage (May 10, 2017). "Nvidia says Toyota will use its AI technology for self-driving cars". Reuters.

^ "Nvidia and Baidu join forces in far reaching AI partnership".

^ Newsroom, NVIDIA. "NVIDIA TITAN V Transforms the PC into AI Supercomputer". NVIDIA Newsroom Newsroom.

^ "Introducing NVIDIA TITAN V: The World's Most Powerful PC Graphics Card". NVIDIA.

^ https://nvidianews.nvidia.com/news/nvidia-reinvents-the-workstation-with-real-time-ray-tracing-6683520

^ "Google Cloud gets support for Nvidia's Tesla P4 inferencing accelerators". Tech Crunch. Retrieved 30 August 2018.

^ "Nvidia to acquire Mellanox Technologies for about $7 billion in cash". www.cnbc.com. 2019-03-11. Retrieved 2019-03-11.

^ Byford, Sam (2019-05-27). "Nvidia announces RTX Studio laptops aimed at creators". The Verge. Retrieved 2019-05-27.

^ "Nvidia GPU Class-Action Settlement Offers Repairs, New Laptops". PC Magazine. Retrieved May 26, 2017.

^ "Update: Nvidia Says Older Mobile GPUs, Chipsets Failing". ExtremeTech. Retrieved May 26, 2017.

^ "Intel agrees to pay NVIDIA $1.5b in patent license fees, signs cross-license". Engadget. Retrieved May 26, 2017.

^ 10 May 2018.  'When will the Nvidia Web Drivers be released for macOS Mojave 10.14'.  Nvidia

^ Upgrade to macOS Mojave. Apple Computer

^ Install macOS 10.14 Mojave on Mac Pro (Mid 2010) and Mac Pro (Mid 2012).  Apple Computer

^ 28 September 2018.  CUDA 10 and macOS 10.14.  Nvidia

^ 18 October 2018.  FAQ about MacOS 10.14 (Mojave) NVIDIA drivers

^ Florian Maislinger.  22 January 2019. 'Apple and Nvidia are said to have a silent hostility'.  PC Builders Club.

^ William Gallagher and Mike Wuerthele.  18 January 18 2019. 'Apple's management doesn't want Nvidia support in macOS, and that's a bad sign for the Mac Pro'

^ Vadim Yuryev.  14 February 2019.  Video: Nvidia support was abandoned in macOS Mojave, and here's why.  Apple Insider

^ Daniel Eran Dilger.  4 April 2017.  'Why Apple's new GPU efforts are a major disruptive threat to Nvidia'.  Apple Insider

^ 'Install macOS 10.14 Mojave on Mac Pro (Mid 2010) and Mac Pro (Mid 2012'.  Apple Inc.

^ a b "NVIDIA Corporation – Financial Info – Annual Reports and Proxies". investor.nvidia.com. Retrieved 2018-11-18.

^ a b "GPU Technology Conference". GPU Technology Conference. Retrieved 2018-06-13.

^ "Company History | NVIDIA". www.nvidia.com. Retrieved 2018-06-13.

^ "Deep Learning and GPU-Programming Workshops at GTC 2018". NVIDIA. Retrieved 2018-06-13.

^ "Nvidia automotive solutions". Nvidia. Retrieved March 29, 2016.

^ "Nvidia unveils driverless car OS and partnership with TomTom". September 29, 2016. Retrieved October 20, 2016.

^ "Nvidia Offers to Release Public Documentation on Certain Aspects of Their GPUs". September 23, 2013. Retrieved September 24, 2013.

^ "nv". Retrieved August 6, 2015.

^ "Linus Torvalds: 'Damn You, Nvidia' for not Supporting Linux". The Verge. June 17, 2012. Retrieved July 9, 2013.

^ 
"X.org, distributors, and proprietary modules". Linux Weekly News. Eklektix. August 14, 2006. Retrieved November 3, 2008.

^ An overview of graphic card manufacturers and how well they work with Ubuntu Ubuntu Gamer, January 10, 2011 (Article by Luke Benstead)

^ "Unix Drivers". Retrieved August 6, 2015.

^ Kevin Parrish. "Nvidia Removed Linux Driver Feature Due to Windows". Tom's Hardware. Retrieved August 6, 2015.

^ Jr, Berkeley Lovelace (February 10, 2017). "Cramer on AI: 'This is the replacement of us. We don't need us with Nvidia'".

^ Markman, Jon. "Deep Learning, Cloud Power Nvidia".

^ Strategy, Moor Insights and. "A Machine Learning Landscape: Where AMD, Intel, NVIDIA, Qualcomm And Xilinx AI Engines Live".

^ "Google Cloud adds NVIDIA Tesla K80 GPU support to boost deep learning performance - TechRepublic".

^ "Intel, Nvidia Trade Shots Over AI, Deep Learning".

^ "Nvidia CEO bets big on deep learning and VR". April 5, 2016.

^ "From not working to neural networking".

^ Coldewey, Devin. "NVIDIA announces a supercomputer aimed at deep learning and AI".

^ 21:07, 21 February 2017 at; tweet_btn(), Shaun Nichols. "Google rents out Nvidia Tesla GPUs in its cloud. If you ask nicely, that'll be 70 cents an hour, bud".

^ "IBM, NVIDIA partner for 'fastest deep learning enterprise solution' in the world - TechRepublic".

^ "IBM and Nvidia team up to create deep learning hardware". November 14, 2016.

^ "IBM and Nvidia make deep learning easy for AI service creators with a new bundle". November 15, 2016.

^ "Facebook 'Big Basin' AI Compute Platform Adopts NVIDIA Tesla P100 For Next Gen Data Centers".

^ "Nvidia to Power Fujitsu's New Deep Learning System at RIKEN - insideHPC". March 5, 2017.

^ Tilley, Aaron. "Nvidia Beats Earnings Estimates As Its Artificial Intelligence Business Keeps On Booming".

^ "Robot see, robot do: Nvidia system lets robots learn by watching humans" New Atlas, May 23, 2018

^ Dean Takahashi. "Nvidia's Inception AI contest awards $1 million to 3 top startups". Venture Beat. Retrieved 6 September 2018.

^  https://blogs.nvidia.com/blog/2017/05/10/these-six-ai-startups-just-snagged-a-share-of-1-5-million-in-cash-prizes/. Missing or empty |title= (help)

^ "Six Startups Split $1.5 Million in Cash in AI startup competition". The Official NVIDIA Blog. 2017-05-10. Retrieved 2018-03-28.


External links[edit]



Wikimedia Commons has media related to Nvidia.

Official website 
Nvidia.com Driver Downloads
GeForce.com, official gaming community site
Business data for Nvidia: Google FinanceYahoo! FinanceReutersSEC filings

vteNvidiaGeForce (List of GPUs)Fixed pixel pipeline
NV1
NV2
RIVA 128
RIVA TNT
RIVA TNT2
GeForce 256
2
4 MX
Vertex and pixel shaders
GeForce 3
4 Ti
FX
6
7
Unified shaders
GeForce 8
9
100
200
300
GeForce 400
500
Unified shaders & NUMA
GeForce 600
700
GeForce 800M
GeForce 900
GeForce 10
GeForce 16
Ray tracing
GeForce 20
Software and technologiesMultimedia  acceleration
NVENC (video encoding)
PureVideo (video decoding)
Software
Cg (shading language)
CUDA
Gelato (offline renderer)
Nvidia GameWorks
OptiX (ray tracing API)
PhysX (physics SDK)
Nvidia RTX (ray tracing platform)
Nvidia System Tools
VDPAU (video decode API)
Technologies
Nvidia 3D Vision (stereo 3D)
Nvidia G-Sync (variable refresh rate)
Nvidia Optimus (GPU switching)
Nvidia Surround (multi-monitor)
NVLink (protocol)
Scalable Link Interface (multi-GPU)
TurboCache (framebuffer in system memory)
GPU microarchitectures
Tesla
Fermi
Kepler
Maxwell
Pascal
Turing
Other productsGraphics processing
Nvidia Quadro
Volta
Quadro Plex
Nvidia Tesla
Console components
NV2A (Xbox)
RSX 'Reality Synthesizer' (PlayStation 3)
Tegra NX-SoC (Nintendo Switch)
Nvidia Shield
Shield Portable
Shield Tablet
Shield Android TV
GeForce Now
SoCs and embedded
GoForce
DGX
Drive
Jetson
Tegra
CPUs
Project Denver
Computer chipsets
nForce
CompanyKey people
Jen-Hsun Huang
Chris Malachowsky
Curtis Priem
David Kirk
Bill Dally
Debora Shoquist
Ranga Jayaraman
Jonah M. Alben
Acquisitions
3dfx Interactive
Ageia
ULi
Icera
Mental Images
PortalPlayer
Exluna
MediaQ
Stexar

Links to related articles
vteOpen Handset AllianceMobile operators
Bouygues Telecom
China Mobile
China Telecommunications Corporation
China Unicom
KDDI
Nepal Telecom
NTT DoCoMo
SoftBank Group
Sprint Corporation
T-Mobile
Telecom Italia
Telefónica
Telus
Vodafone
Software companies
Access
Ascender Corporation
eBay
Google
Myriad Group
Nuance Communications
NXP Software
Omron
PacketVideo
SVOX
VisualOn
Semiconductor companies
AKM Semiconductor, Inc.
Arm Holdings
Audience
Broadcom
CSR plc (joined as SiRF)
Cypress Semiconductor
Freescale Semiconductor
Gemalto
Intel
Marvell Technology Group
MediaTek
MIPS Technologies
Nvidia
Qualcomm
Qualcomm Atheros
Renesas Electronics
ST-Ericsson (joined as Ericsson Mobile Platforms)
Synaptics
Texas Instruments
Handset makers
Acer Inc.
Alcatel Mobile Phones
Asus
Chaudhary Group (with association of LG)
CCI
Dell
Foxconn
Garmin
HTC
Huawei
Kyocera
Lenovo Mobile
LG Electronics
Motorola Mobility
NEC Corporation
Samsung Electronics
Sharp Corporation
Sony Mobile
Toshiba
ZTE
Commercialization companies
Accenture
Borqs
Sasken Communication Technologies
Teleca
The Astonishing Tribe
Wind River Systems
Wipro Technologies
See also
Android
Dalvik virtual machine
Google Nexus
T-Mobile G1

vteCompanies of the NASDAQ-100 index
Activision Blizzard
Adobe Inc.
Advanced Micro Devices
Alexion Pharmaceuticals
Align Technology
Alphabet
Amazon.com
American Airlines Group
Amgen
Analog Devices
Apple
Applied Materials
ASML Holding
Autodesk
Automatic Data Processing
Baidu
Biogen
BioMarin Pharmaceutical
Booking Holdings
Broadcom Inc.
Cadence Design Systems
Celgene
Cerner
Charter Communications
Check Point
Cintas
Cisco Systems
Citrix Systems
Cognizant
Comcast
Costco
CSX
Ctrip.com International
Dollar Tree
eBay
Electronic Arts
Expedia
Facebook
Fastenal
Fiserv
Fox Corporation
Gilead Sciences
Hasbro
Henry Schein
Idexx Laboratories
Illumina
Incyte
Intel
Intuit
Intuitive Surgical
J. B. Hunt Transport Services
JD.com
KLA-Tencor
Kraft Heinz
Lam Research
Liberty Global
Lululemon Athletica
Marriott International
Maxim Integrated Products
MercadoLibre
Microchip Technology
Micron Technology
Microsoft
Mondelez International
Monster Beverage
Mylan
NetApp
NetEase
Netflix
Nvidia
NXP Semiconductors
O'Reilly Auto Parts
Paccar
Paychex
PayPal
PepsiCo
Qualcomm
Regeneron
Ross Stores
Sirius XM Holdings
Skyworks Solutions
Starbucks
Symantec
Synopsys
T-Mobile US
Take-Two Interactive
Tesla, Inc.
Texas Instruments
Ulta Beauty
United Continental
Verisign
Verisk Analytics
Vertex Pharmaceuticals
Walgreens Boots Alliance
Western Digital
Willis Towers Watson
Workday
Wynn Resorts
Xcel Energy
Xilinx

vteHome theater PC software, devices, and related articlesWindows
Beyond TV
DVBViewer
DVB Dream
NextPVR (formerly GB-PVR)
JRiver Media Center
MediaPortal
ShowShifter
Windows Media Center
macOS
Front Row
Linux
GeeXboX
LibreELEC
LinuxMCE
MythTV
Mythbuntu
OpenELEC
Tvheadend
Video Disk Recorder
Cross-platform
Emby
Kodi (formerly XBMC)
Plex
SageTV
Serviio
Set-top boxes,digital media receivers
Amazon Fire TV
Android TV
Apple TV
Boxee Box
Chromecast
Dreambox
Ericsson Mediaroom
Google TV
Hauppauge MediaMVP
HP MediaSmart Connect
Netgear Digital Entertainer
ReplayTV
Roku
TiVo
Unibox
WD TV
Windows Media Center Extender
Related hardware
ATI Theater Cards
DBox2
Dreambox
Elgato EyeTV devices
Hauppauge Computer Works WinTV PVR Cards
HDHomeRun
Mac Mini
Monsoon HAVA
Quiet PC
Slingbox
Touchscreen remote control
VBox Home TV Gateway
Related articles
10-foot user interface
Comparison of audio player software
Comparison of video player software
Comparison of streaming media systems
Digital Living Network Alliance
Digital media receiver
Home cinema
Home theater PC
Hybrid IPTV
Internet television
Media server
Set-top box
Smart TV
Video player

vteMajor semiconductor companiesCompanies with an annual revenue of over US$3 billion
ASE Group
Fujitsu
Infineon Technologies
Integrated Micro-Electronics, Inc.
Intel
NXP Semiconductors (Freescale)
ON Semiconductor
Panasonic
Renesas Electronics
Samsung Electronics
Sony
STMicroelectronics
Texas Instruments
Fabless
Advanced Micro Devices
Apple Inc.
Broadcom Inc.
Marvell Technology Group
MediaTek
Nvidia
Qualcomm
VIA Technologies
Memory
Micron Technology
Samsung Electronics
SanDisk
SK Hynix
Toshiba
Foundries
GlobalFoundries
TSMC
United Microelectronics Corporation
Samsung Foundry
SMIC
Equipment
ASML
Applied Materials
KLA-Tencor
Lam Research
Tokyo Electron


See also
Largest IT companies
Semiconductor industry
Category:Semiconductor companies

Authority control 
GND: 6067931-1
ISNI: 0000 0004 0458 4453
LCCN: no2004070743
VIAF: 135222459
 WorldCat Identities (via VIAF): 135222459




For reinforcement learning in psychology, see Reinforcement and Operant conditioning.
Machine learning anddata mining
Problems
Classification
Clustering
Regression
Anomaly detection
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction


Supervised learning.mw-parser-output .nobold{font-weight:normal}(classification • regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)


Clustering
BIRCH
CURE
Hierarchical
k-means
Expectation–maximization (EM)
DBSCAN
OPTICS
Mean-shift


Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
t-SNE


Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov


Anomaly detection
k-NN
Local outlier factor


Artificial neural networks
Autoencoder
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net


Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)


Theory
Bias–variance dilemma
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory


Machine-learning venues
NIPS
ICML
ML
JMLR
ArXiv:cs.LG


Glossary of artificial intelligence
Glossary of artificial intelligence


Related articles
List of datasets for machine-learning research
Outline of machine learning


 Machine learning portalvte
Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.
It differs from supervised learning in that labelled input/output pairs need not be presented, and sub-optimal actions need not be explicitly corrected. Instead the focus is finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).[1]

The environment is typically formulated as a Markov decision process (MDP), as many reinforcement learning algorithms for this context utilize dynamic programming techniques.[2][3][4] The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.[2][3].mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}Contents

1 Introduction
2 Exploration
3 Algorithms for control learning

3.1 Criterion of optimality

3.1.1 Policy
3.1.2 State-value function


3.2 Brute force
3.3 Value function

3.3.1 Monte Carlo methods
3.3.2 Temporal difference methods


3.4 Direct policy search


4 Theory
5 Research
6 Comparison of reinforcement learning algorithms

6.1 Deep reinforcement learning
6.2 Inverse reinforcement learning
6.3 Apprenticeship learning


7 See also
8 Footnotes
9 References
10 Literature

10.1 Conferences, journals


11 External links



Introduction[edit]
 The typical framing of a Reinforcement Learning (RL) scenario: an agent takes actions in an environment, which is interpreted into a reward and a representation of the state, which are fed back into the agent.
Reinforcement learning, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In the operations research and control literature, reinforcement learning is called approximate dynamic programming, or neuro-dynamic programming.[3][2] The problems of interest in reinforcement learning have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.
Basic reinforcement is modeled as a Markov decision process:

a set of environment and agent states, S;
a set of actions, A, of the agent;





P

a


(
s
,

s
′

)
=
Pr
(

s

t
+
1


=

s
′

∣

s

t


=
s
,

a

t


=
a
)


{\displaystyle P_{a}(s,s')=\Pr(s_{t+1}=s'\mid s_{t}=s,a_{t}=a)}

 is the probability of transition from state 



s


{\displaystyle s}

 to state 




s
′



{\displaystyle s'}

 under action 



a


{\displaystyle a}

.





R

a


(
s
,

s
′

)


{\displaystyle R_{a}(s,s')}

 is the immediate reward after transition from 



s


{\displaystyle s}

 to 




s
′



{\displaystyle s'}

 with action 



a


{\displaystyle a}

.
rules that describe what the agent observes
Rules are often stochastic. The observation typically involves the scalar, immediate reward associated with the last transition. In many works, the agent is assumed to observe the current environmental state (full observability). If not, the agent has partial observability. Sometimes the set of actions available to the agent is restricted (a zero balance cannot be reduced. For example, if the current value of the agent is 3 and the state transition reduces the value by 4, the transition will not be allowed).
A reinforcement learning agent interacts with its environment in discrete time steps. At each time t, the agent receives an observation 




o

t




{\displaystyle o_{t}}

, which typically includes the reward 




r

t




{\displaystyle r_{t}}

. It then chooses an action 




a

t




{\displaystyle a_{t}}

 from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state 




s

t
+
1




{\displaystyle s_{t+1}}

 and the reward 




r

t
+
1




{\displaystyle r_{t+1}}

 associated with the transition 



(

s

t


,

a

t


,

s

t
+
1


)


{\displaystyle (s_{t},a_{t},s_{t+1})}

 is determined. The goal of a reinforcement learning agent is to collect as much reward as possible. The agent can (possibly randomly) choose any action as a function of the history.
When the agent's performance is compared to that of an agent that acts optimally, the difference in performance gives rise to the notion of regret. In order to act near optimally, the agent must reason about the long term consequences of its actions (i.e., maximize future income), although the immediate reward associated with this might be negative.
Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including robot control, elevator scheduling, telecommunications, backgammon, checkers[5] and go (AlphaGo).
Two elements make reinforcement learning powerful: the use of samples to optimize performance and the use of function approximation to deal with large environments. Thanks to these two key components, reinforcement learning can be used in large environments in the following situations:

A model of the environment is known, but an analytic solution is not available;
Only a simulation model of the environment is given (the subject of simulation-based optimization);[6]
The only way to collect information about the environment is to interact with it.
The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.

Exploration[edit]
The exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and for finite state space MDPs in Burnetas and Katehakis (1997)[7]. 
Reinforcement learning requires clever exploration mechanisms. Randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.
One such method is 



ϵ


{\displaystyle \epsilon }

-greedy, when the agent chooses the action that it believes has the best long-term effect with probability 



1
−
ϵ


{\displaystyle 1-\epsilon }

. If no action which satisfies this condition is found, the agent chooses an action uniformly at random. Here, 



0
<
ϵ
<
1


{\displaystyle 0<\epsilon <1}

 is a tuning parameter, which is sometimes changed, either according to a fixed schedule (making the agent explore progressively less), or adaptively based on heuristics.[8]

Algorithms for control learning[edit]
Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions are good.

Criterion of optimality[edit]
Policy[edit]
The agent's action selection is modeled as a map called policy:





π
:
A
×
S
→
[
0
,
1
]


{\displaystyle \pi :A\times S\rightarrow [0,1]}






π
(
a
,
s
)
=
Pr
(

a

t


=
a
∣

s

t


=
s
)


{\displaystyle \pi (a,s)=\Pr(a_{t}=a\mid s_{t}=s)}


The policy map gives the probability of taking action 



a


{\displaystyle a}

 when in state 



s


{\displaystyle s}

.[9]:61 There are also non-probabilistic policies.

State-value function[edit]
Value function 




V

π


(
s
)


{\displaystyle V_{\pi }(s)}

 is defined as the expected return starting with state 



s


{\displaystyle s}

, i.e. 




s

0


=
s


{\displaystyle s_{0}=s}

, and successively following policy 



π


{\displaystyle \pi }

. Hence, roughly speaking, the value function estimates "how good" it is to be in a given state.[9]:60






V

π


(
s
)
=
E
⁡
[
R
]
=
E
⁡

[


∑

t
=
0


∞



γ

t



r

t


∣

s

0


=
s

]

,


{\displaystyle V_{\pi }(s)=\operatorname {E} [R]=\operatorname {E} \left[\sum _{t=0}^{\infty }\gamma ^{t}r_{t}\mid s_{0}=s\right],}


where the random variable 



R


{\displaystyle R}

 denotes the return, and is defined as the sum of future discounted rewards[clarification needed]





R
=

∑

t
=
0


∞



γ

t



r

t


,


{\displaystyle R=\sum _{t=0}^{\infty }\gamma ^{t}r_{t},}


where 




r

t




{\displaystyle r_{t}}

 is the reward at step 



t


{\displaystyle t}

, 



γ
∈
[
0
,
1
]


{\displaystyle \gamma \in [0,1]}

 is the discount-rate[clarification needed].
The algorithm must find a policy with maximum expected return. From the theory of MDPs it is known that, without loss of generality, the search can be restricted to the set of so-called stationary policies. A policy is stationary if the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted to deterministic stationary policies. A deterministic stationary policy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.

Brute force[edit]
The brute force approach entails two steps:

For each possible policy, sample returns while following it
Choose the policy with the largest expected return
One problem with this is that the number of policies can be large, or even infinite. Another is that variance of the returns may be large, which requires many samples to accurately estimate the return of each policy.
These problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this are value function estimation and direct policy search.

Value function[edit]
Value function approaches attempt to find a policy that maximizes the return by maintaining a set of estimates of expected returns for some policy (usually either the "current" [on-policy] or the optimal [off-policy] one).
These methods rely on the theory of MDPs, where optimality is defined in a sense that is stronger than the above one: A policy is called optimal if it achieves the best expected return from any initial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found amongst stationary policies.
To define optimality in a formal manner, define the value of a policy 



π


{\displaystyle \pi }

 by






V

π


(
s
)
=
E
[
R
∣
s
,
π
]
,


{\displaystyle V^{\pi }(s)=E[R\mid s,\pi ],}


where 



R


{\displaystyle R}

 stands for the return associated with following 



π


{\displaystyle \pi }

 from the initial state 



s


{\displaystyle s}

. Defining 




V

∗


(
s
)


{\displaystyle V^{*}(s)}

 as the maximum possible value of 




V

π


(
s
)


{\displaystyle V^{\pi }(s)}

, where 



π


{\displaystyle \pi }

 is allowed to change,






V

∗


(
s
)
=

max

π



V

π


(
s
)
.


{\displaystyle V^{*}(s)=\max _{\pi }V^{\pi }(s).}


A policy that achieves these optimal values in each state is called optimal. Clearly, a policy that is optimal in this strong sense is also optimal in the sense that it maximizes the expected return 




ρ

π




{\displaystyle \rho ^{\pi }}

, since 




ρ

π


=
E
[

V

π


(
S
)
]


{\displaystyle \rho ^{\pi }=E[V^{\pi }(S)]}

, where 



S


{\displaystyle S}

 is a state randomly sampled from the distribution 



μ


{\displaystyle \mu }

[clarification needed].
Although state-values suffice to define optimality, it is useful to define action-values. Given a state 



s


{\displaystyle s}

, an action 



a


{\displaystyle a}

 and a policy 



π


{\displaystyle \pi }

, the action-value of the pair 



(
s
,
a
)


{\displaystyle (s,a)}

 under 



π


{\displaystyle \pi }

 is defined by






Q

π


(
s
,
a
)
=
E
⁡
[
R
∣
s
,
a
,
π
]
,



{\displaystyle Q^{\pi }(s,a)=\operatorname {E} [R\mid s,a,\pi ],\,}


where 



R


{\displaystyle R}

 now stands for the random return associated with first taking action 



a


{\displaystyle a}

 in state 



s


{\displaystyle s}

 and following 



π


{\displaystyle \pi }

, thereafter.
The theory of MDPs states that if 




π

∗




{\displaystyle \pi ^{*}}

 is an optimal policy, we act optimally (take the optimal action) by choosing the action from 




Q


π

∗




(
s
,
⋅
)


{\displaystyle Q^{\pi ^{*}}(s,\cdot )}

 with the highest value at each state, 



s


{\displaystyle s}

. The action-value function of such an optimal policy (




Q


π

∗






{\displaystyle Q^{\pi ^{*}}}

) is called the optimal action-value function and is commonly denoted by 




Q

∗




{\displaystyle Q^{*}}

. In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.
Assuming full knowledge of the MDP, the two basic approaches to compute the optimal action-value function are value iteration and policy iteration. Both algorithms compute a sequence of functions 




Q

k




{\displaystyle Q_{k}}

 (



k
=
0
,
1
,
2
,
…


{\displaystyle k=0,1,2,\ldots }

) that converge to 




Q

∗




{\displaystyle Q^{*}}

. Computing these functions involves computing expectations over the whole state-space, which is impractical for all but the smallest (finite) MDPs. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state-action spaces.

Monte Carlo methods[edit]
Monte Carlo methods can be used in an algorithm that mimics policy iteration. Policy iteration consists of two steps: policy evaluation and policy improvement.
Monte Carlo is used in the policy evaluation step. In this step, given a stationary, deterministic policy 



π


{\displaystyle \pi }

, the goal is to compute the function values 




Q

π


(
s
,
a
)


{\displaystyle Q^{\pi }(s,a)}

 (or a good approximation to them) for all state-action pairs 



(
s
,
a
)


{\displaystyle (s,a)}

. Assuming (for simplicity) that the MDP is finite, that sufficient memory is available to accommodate the action-values and that the problem is episodic and after each episode a new one starts from some random initial state. Then, the estimate of the value of a given state-action pair 



(
s
,
a
)


{\displaystyle (s,a)}

 can be computed by averaging the sampled returns that originated from 



(
s
,
a
)


{\displaystyle (s,a)}

 over time.  Given sufficient time, this procedure can thus construct a precise estimate 



Q


{\displaystyle Q}

 of the action-value function 




Q

π




{\displaystyle Q^{\pi }}

. This finishes the description of the policy evaluation step.
In the policy improvement step, the next policy is obtained by computing a greedy policy with respect to 



Q


{\displaystyle Q}

: Given a state 



s


{\displaystyle s}

, this new policy returns an action that maximizes 



Q
(
s
,
⋅
)


{\displaystyle Q(s,\cdot )}

. In practice lazy evaluation can defer the computation of the maximizing actions to when they are needed.
Problems with this procedure include:

The procedure may spend too much time evaluating a suboptimal policy.
It uses samples inefficiently in that a long trajectory improves the estimate only of the single state-action pair that started the trajectory.
When the returns along the trajectories have high variance, convergence is slow.
It works in episodic problems only;
It works in small, finite MDPs only.
Temporal difference methods[edit]
Main article: Temporal difference learning
The first problem is corrected by allowing the procedure to change the policy (at some or all states) before the values settle. This too may be problematic as it might prevent convergence. Most current algorithms do this, giving rise to the class of generalized policy iteration algorithms. Many actor critic methods belong to this category.
The second issue can be corrected by allowing trajectories to contribute to any state-action pair in them. This may also help to some extent with the third problem, although a better solution when returns have high variance is Sutton's[10][11] temporal difference (TD) methods that are based on the recursive Bellman equation. Note that the computation in TD methods can be incremental (when after each transition the memory is changed and the transition is thrown away), or batch (when the transitions are batched and the estimates are computed once based on the batch). Batch methods, such as the least-squares temporal difference method,[12] may use the information in the samples better, while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity. Some methods try to combine the two approaches. Methods based on temporal differences also overcome the fourth issue.
In order to address the fifth issue, function approximation methods are used. Linear function approximation starts with a mapping 



ϕ


{\displaystyle \phi }

 that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair 



(
s
,
a
)


{\displaystyle (s,a)}

 are obtained by linearly combining the components of 



ϕ
(
s
,
a
)


{\displaystyle \phi (s,a)}

 with some weights 



θ


{\displaystyle \theta }

:





Q
(
s
,
a
)
=

∑

i
=
1


d



θ

i



ϕ

i


(
s
,
a
)
.


{\displaystyle Q(s,a)=\sum _{i=1}^{d}\theta _{i}\phi _{i}(s,a).}


The algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs. Methods based on ideas from nonparametric statistics (which can be seen to construct their own features) have been explored.
Value iteration can also be used as a starting point, giving rise to the Q-learning algorithm and its many variants. [13]
The problem with using action-values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy. Though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency. Another problem specific to TD comes from their reliance on the recursive Bellman equation. Most TD methods have a so-called 



λ


{\displaystyle \lambda }

 parameter 



(
0
≤
λ
≤
1
)


{\displaystyle (0\leq \lambda \leq 1)}

 that can continuously interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations. This can be effective in palliating this issue.

Direct policy search[edit]
An alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case of stochastic optimization. The two approaches available are gradient-based and gradient-free methods.
Gradient-based methods (policy gradient methods) start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector 



θ


{\displaystyle \theta }

, let 




π

θ




{\displaystyle \pi _{\theta }}

 denote the policy associated to 



θ


{\displaystyle \theta }

. Defining the performance function by





ρ
(
θ
)
=

ρ


π

θ




,


{\displaystyle \rho (\theta )=\rho ^{\pi _{\theta }},}


under mild conditions this function will be differentiable as a function of the parameter vector 



θ


{\displaystyle \theta }

. If the gradient of 



ρ


{\displaystyle \rho }

 was known, one could use gradient ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams' REINFORCE[14] method (which is known as the likelihood ratio method in the simulation-based optimization literature).[15] Policy search methods have been used in the robotics context.[16] Many policy search methods may get stuck in local optima (as they are based on local search).
A large class of methods avoids relying on gradient information.These include simulated annealing, cross-entropy search or methods of evolutionary computation. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.
Policy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years, actor–critic methods have been proposed and performed well on various problems.[17]

Theory[edit]
Both the asymptotic and finite-sample behavior of most algorithms is well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.
Efficient exploration of large MDPs is largely unexplored (except for the case of bandit problems).[clarification needed] Although finite-time performance bounds appeared for many algorithms, these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.
For incremental algorithms, asymptotic convergence issues have been settled. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).

Research[edit]
Research topics include 

adaptive methods that work with fewer (or no) parameters under a large number of condition
addressing the exploration problem in large MDPs
large-scale empirical evaluations
learning and acting under partial information (e.g., using predictive state representation)
modular and hierarchical reinforcement learning
improving existing value-function and policy search methods
algorithms that work well with large (or continuous) action spaces
transfer learning
lifelong learning
efficient sample-based planning (e.g., based on Monte Carlo tree search).
bug detection in software projects[18]
Multiagent or distributed reinforcement learning is a topic of interest. Applications are expanding.[19]

Actor-critic reinforcement learning
Reinforcement learning algorithms such as TD learning are under investigation as a model for dopamine-based learning in the brain. In this model, the dopaminergic projections from the substantia nigra to the basal ganglia function as the prediction error. Reinforcement learning has been used as a part of the model for human skill learning, especially in relation to the interaction between implicit and explicit learning in skill acquisition (the first publication on this application was in 1995–1996).[20]

Comparison of reinforcement learning algorithms[edit]


Algorithm
Description
Model
Policy
Action Space
State Space
Operator


Monte Carlo
Every visit to Monte Carlo
Model-Free
Off-policy
Discrete
Discrete
Sample-means


Q-learning
State–action–reward–state
Model-Free
Off-policy
Discrete
Discrete
Q-value


SARSA
State–action–reward–state–action
Model-Free
On-policy
Discrete
Discrete
Q-value


Q-learning - Lambda
State–action–reward–state with eligibility traces
Model-Free
Off-policy
Discrete
Discrete
Q-value


SARSA - Lambda
State–action–reward–state–action with eligibility traces
Model-Free
On-policy
Discrete
Discrete
Q-value


DQN
Deep Q Network
Model-Free
Off-policy
Discrete
Continuous
Q-value


DDPG
Deep Deterministic Policy Gradient
Model-Free
Off-policy
Continuous
Continuous
Q-value


A3C
Asynchronous Advantage Actor-Critic Algorithm
Model-Free
On-policy
Continuous
Continuous
Advantage


NAF
Q-Learning with Normalized Advantage Functions
Model-Free
Off-policy
Continuous
Continuous
Advantage


TRPO
Trust Region Policy Optimization
Model-Free
On-policy
Continuous
Continuous
Advantage


PPO
Proximal Policy Optimization
Model-Free
On-policy
Continuous
Continuous
Advantage

Deep reinforcement learning[edit]
This approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space.[21] The work on learning ATARI games by Google DeepMind[22] increased attention to deep reinforcement learning or end-to-end reinforcement learning.

Inverse reinforcement learning[edit]
In inverse reinforcement learning (IRL), no reward function is given. Instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic observed behavior, which is often optimal or close to optimal.[23]

Apprenticeship learning[edit]
In apprenticeship learning, an expert demonstrates the target behavior. The system tries to recover the policy via observation.

See also[edit]
Temporal difference learning
Q-learning
State–action–reward–state–action (SARSA)
Fictitious play
Learning classifier system
Optimal control
Dynamic treatment regimes
Error-driven learning
Multi-agent system
Distributed artificial intelligence
Footnotes[edit]


^ Kaelbling, Leslie P.; Littman, Michael L.; Moore, Andrew W. (1996). "Reinforcement Learning: A Survey". Journal of Artificial Intelligence Research. 4: 237–285. arXiv:cs/9605103. doi:10.1613/jair.301. Archived from the original on 2001-11-20..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ a b c Dimitri P. Bertsekas and John N. Tsitsiklis. "Neuro-Dynamic Programming", Athena Scientific, 1996,[1]

^ a b c Dimitri P. Bertsekas. "Dynamic Programming and Optimal Control: Approximate Dynamic Programming, Vol.II", Athena Scientific, 2012,[2]

^ van Otterlo, M.; Wiering, M. (2012). Reinforcement learning and markov decision processes. Reinforcement Learning. Adaptation, Learning, and Optimization. 12. pp. 3–42. doi:10.1007/978-3-642-27645-3_1. ISBN 978-3-642-27644-6.

^ Sutton & Barto, Chapter 11.

^ Gosavi 2003.

^ Burnetas, Apostolos N.; Katehakis, Michael N. (1997), "Optimal adaptive policies for Markov Decision Processes", Mathematics of Operations Research, 22: 222--255

^ Tokic, Michel; Palm, Günther (2011), "Value-Difference Based Exploration: Adaptive Control Between Epsilon-Greedy and Softmax" (PDF), KI 2011: Advances in Artificial Intelligence, Lecture Notes in Computer Science, 7006, Springer, pp. 335–346, ISBN 978-3-642-24455-1

^ a b Reinforcement learning: An introduction (PDF).

^ Sutton 1984.

^ Sutton & Barto 1998, §6. Temporal-Difference Learning.

^ Bradke & Barto 1996.

^ Watkins 1989.

^ Williams 1987.

^ Peters, Vijayakumar & Schall 2003.

^ Deisenroth, Neumann & Peters 2013.

^ Juliani, Arthur (2016-12-17). "Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)". Medium. Retrieved 2018-02-22.

^ "On the Use of Reinforcement Learning for Testing Game Mechanics : ACM - Computers in Entertainment". cie.acm.org. Retrieved 2018-11-27.

^ "Reinforcement Learning / Successes of Reinforcement Learning". umichrl.pbworks.com. Retrieved 2017-08-06.

^ [3] Archived 2017-04-26 at the Wayback Machine

^ Francois-Lavet, Vincent;  et al. (2018). "An Introduction to Deep Reinforcement Learning". Foundations and Trends in Machine Learning. 11 (3–4): 219–354. arXiv:1811.12560. doi:10.1561/2200000071.

^ Mnih, Volodymyr;  et al. (2015). "Human-level control through deep reinforcement learning". Nature. 518 (7540): 529–533. Bibcode:2015Natur.518..529M. doi:10.1038/nature14236. PMID 25719670.

^ Ng, A. Y., & Russell, S. J. (2000, June). Algorithms for inverse reinforcement learning. In Icml (pp. 663-670).


References[edit]
Auer, Peter; Jaksch, Thomas; Ortner, Ronald (2010). "Near-optimal regret bounds for reinforcement learning". Journal of Machine Learning Research. 11: 1563–1600.
Bertsekas, Dimitri P.; Tsitsiklis, John (1996). Neuro-Dynamic Programming. Nashua, NH: Athena Scientific. ISBN 978-1-886529-10-6.
Bertsekas, Dimitri P. (2012). Dynamic Programming and Optimal Control: Approximate Dynamic Programming, Vol.II. Nashua, NH: Athena Scientific. ISBN 978-1-886529-44-1.
Busoniu, Lucian; Babuska, Robert; De Schutter, Bart; Ernst, Damien (2010). Reinforcement Learning and Dynamic Programming using Function Approximators. Taylor & Francis CRC Press. ISBN 978-1-4398-2108-4.
Deisenroth, Marc Peter; Neumann, Gerhard; Peters, Jan (2013). A Survey on Policy Search for Robotics. Foundations and Trends in Robotics. 2. NOW Publishers. pp. 1–142. hdl:10044/1/12051.Bradtke, Steven J.; Barto, Andrew G. (1996). "Learning to predict by the method of temporal differences". Machine Learning. 22: 33–57. CiteSeerX 10.1.1.143.857. doi:10.1023/A:1018056104778.
Gosavi, Abhijit (2003). Simulation-based Optimization: Parametric Optimization Techniques and Reinforcement. Operations Research/Computer Science Interfaces Series. Springer. ISBN 978-1-4020-7454-7.
Peters, Jan; Vijayakumar, Sethu; Schaal, Stefan (2003). "Reinforcement Learning for Humanoid Robotics" (PDF). IEEE-RAS International Conference on Humanoid Robots.
Powell, Warren (2007). Approximate dynamic programming: solving the curses of dimensionality. Wiley-Interscience. ISBN 978-0-470-17155-4.
Sutton, Richard S.; Barto, Andrew G. (1998). Reinforcement Learning: An Introduction. MIT Press. ISBN 978-0-262-19398-6.
Sutton, Richard S. (1988). "Learning to predict by the method of temporal differences". Machine Learning. 3: 9–44. doi:10.1007/BF00115009.
Sutton, Richard S. (1984). Temporal Credit Assignment in Reinforcement Learning (PhD thesis). University of Massachusetts, Amherst, MA.
Szita, Istvan; Szepesvari, Csaba (2010). "Model-based Reinforcement Learning with Nearly Tight Exploration Complexity Bounds" (PDF). ICML 2010. Omnipress. pp. 1031–1038. Archived from the original (PDF) on 2010-07-14.
Williams, Ronald J. (1987). "A class of gradient-estimating algorithms for reinforcement learning in neural networks". Proceedings of the IEEE First International Conference on Neural Networks. CiteSeerX 10.1.1.129.8871.
Watkins, Christopher J.C.H. (1989). Learning from Delayed Rewards (PDF) (PhD thesis). King’s College, Cambridge, UK.
François-Lavet, Vincent; Henderson, Peter; Islam, Riashat; Bellemare, Marc G.; Pineau, Joelle (2018). "An Introduction to Deep Reinforcement Learning". Foundations and Trends in Machine Learning. 11 (3–4): 219–354. arXiv:1811.12560. doi:10.1561/2200000071.
Literature[edit]
Conferences, journals[edit]
Most reinforcement learning papers are published at the major machine learning and AI conferences (ICML, NIPS, AAAI, IJCAI, UAI, AI and Statistics) and journals (JAIR, JMLR, Machine learning journal, IEEE T-CIAIG). Some theory papers are published at COLT and ALT. However, many papers appear in robotics conferences (IROS, ICRA) and the "agent" conference AAMAS. Operations researchers publish their papers at the INFORMS conference and, for example, in the Operation Research, and the Mathematics of Operations Research journals. Control researchers publish their papers at the CDC and ACC conferences, or, e.g., in the journals IEEE Transactions on Automatic Control, or Automatica, although applied works tend to be published in more specialized journals. The Winter Simulation Conference also publishes many relevant papers. Other than this, papers also published in the major conferences of the neural networks, fuzzy, and evolutionary computation communities. The annual IEEE symposium titled Approximate Dynamic Programming and Reinforcement Learning (ADPRL) and the biannual European Workshop on Reinforcement Learning (EWRL) are two regularly held meetings where RL researchers meet.

External links[edit]
Website for Reinforcement Learning: An Introduction (1998), by Rich Sutton and Andrew Barto, MIT Press, including a link to an html version of the book.
Reinforcement Learning Repository
Reinforcement Learning and Artificial Intelligence (RLAI, Rich Sutton's lab at the University of Alberta)
A Beginner's Guide to Deep Reinforcement Learning
Autonomous Learning Laboratory (ALL, Andrew Barto's lab at the University of Massachusetts Amherst)
Hybrid reinforcement learning
Real-world reinforcement learning experiments at Delft University of Technology
Stanford University Andrew Ng Lecture on Reinforcement Learning
Dissecting Reinforcement Learning Series of blog post on RL with Python code
An Introduction to Deep Reinforcement Learning
vteComputer scienceNote: This template roughly follows the 2012 ACM Computing Classification System.Hardware
Printed circuit board
Peripheral
Integrated circuit
Very Large Scale Integration
Systems on Chip (SoCs)
Energy consumption (Green computing)
Electronic design automation
Hardware acceleration
Computer systemsorganization
Computer architecture
Embedded system
Real-time computing
Dependability
Networks
Network architecture
Network protocol
Network components
Network scheduler
Network performance evaluation
Network service
Software organization
Interpreter
Middleware
Virtual machine
Operating system
Software quality
Software notationsand tools
Programming paradigm
Programming language
Compiler
Domain-specific language
Modeling language
Software framework
Integrated development environment
Software configuration management
Software library
Software repository
Software development
Software development process
Requirements analysis
Software design
Software construction
Software deployment
Software maintenance
Programming team
Open-source model
Theory of computation
Model of computation
Formal language
Automata theory
Computational complexity theory
Logic
Semantics
Algorithms
Algorithm design
Analysis of algorithms
Algorithmic efficiency
Randomized algorithm
Computational geometry
Mathematicsof computing
Discrete mathematics
Probability
Statistics
Mathematical software
Information theory
Mathematical analysis
Numerical analysis
Informationsystems
Database management system
Information storage systems
Enterprise information system
Social information systems
Geographic information system
Decision support system
Process control system
Multimedia information system
Data mining
Digital library
Computing platform
Digital marketing
World Wide Web
Information retrieval
Security
Cryptography
Formal methods
Security services
Intrusion detection system
Hardware security
Network security
Information security
Application security
Human–computerinteraction
Interaction design
Social computing
Ubiquitous computing
Visualization
Accessibility
Concurrency
Concurrent computing
Parallel computing
Distributed computing
Multithreading
Multiprocessing
Artificialintelligence
Natural language processing
Knowledge representation and reasoning
Computer vision
Automated planning and scheduling
Search methodology
Control method
Philosophy of artificial intelligence
Distributed artificial intelligence
Machine learning
Supervised learning
Unsupervised learning
Reinforcement learning
Multi-task learning
Cross-validation
Graphics
Animation
Rendering
Image manipulation
Graphics processing unit
Mixed reality
Virtual reality
Image compression
Solid modeling
Appliedcomputing
E-commerce
Enterprise software
Computational mathematics
Computational physics
Computational chemistry
Computational biology
Computational social science
Computational engineering
Computational healthcare
Digital art
Electronic publishing
Cyberwarfare
Electronic voting
Video games
Word processing
Operations research
Educational technology
Document management

 Book
 Category
 Portal
WikiProject
 Commons




For the journal, see Machine Learning (journal).
"Statistical learning" redirects here. For statistical learning in linguistics, see statistical learning in language acquisition.
Machine learning anddata mining
Problems
Classification
Clustering
Regression
Anomaly detection
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction


Supervised learning.mw-parser-output .nobold{font-weight:normal}(classification • regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)


Clustering
BIRCH
CURE
Hierarchical
k-means
Expectation–maximization (EM)
DBSCAN
OPTICS
Mean-shift


Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
t-SNE


Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov


Anomaly detection
k-NN
Local outlier factor


Artificial neural networks
Autoencoder
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net


Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)


Theory
Bias–variance dilemma
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory


Machine-learning venues
NIPS
ICML
ML
JMLR
ArXiv:cs.LG


Glossary of artificial intelligence
Glossary of artificial intelligence


Related articles
List of datasets for machine-learning research
Outline of machine learning


 Machine learning portalvte
Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use in order to perform a specific task effectively without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as "training data", in order to make predictions or decisions without being explicitly programmed to perform the task.[1][2]:2 Machine learning algorithms are used in a wide variety of applications, such as email filtering, and computer vision, where it is infeasible to develop an algorithm of specific instructions for performing the task. Machine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a field of study within machine learning, and focuses on exploratory data analysis through unsupervised learning.[3][4] In its application across business problems, machine learning is also referred to as predictive analytics.

Contents

1 Overview

1.1 Machine learning tasks


2 History and relationships to other fields

2.1 Relation to data mining
2.2 Relation to optimization
2.3 Relation to statistics


3 Theory
4 Approaches

4.1 Types of learning algorithms

4.1.1 Supervised learning
4.1.2 Unsupervised learning
4.1.3 Reinforcement learning
4.1.4 Feature learning
4.1.5 Sparse dictionary learning
4.1.6 Anomaly detection
4.1.7 Association rules


4.2 Models

4.2.1 Artificial neural networks
4.2.2 Decision trees
4.2.3 Support vector machines
4.2.4 Bayesian networks
4.2.5 Genetic algorithms


4.3 Training models

4.3.1 Federated learning




5 Applications
6 Limitations

6.1 Bias


7 Model assessments
8 Ethics
9 Software

9.1 Free and open-source software
9.2 Proprietary software with free and open-source editions
9.3 Proprietary software


10 Journals
11 Conferences
12 See also
13 References
14 Further reading
15 External links


Overview[edit]
The name machine learning was coined in 1959 by Arthur Samuel.[5] Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E."[6] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper "Computing Machinery and Intelligence", in which the question "Can machines think?" is replaced with the question "Can machines do what we (as thinking entities) can do?".[7] In Turing's proposal the various characteristics that could be possessed by a thinking machine and the various implications in constructing one are exposed.

Machine learning tasks[edit]


 A support vector machine is a supervised learning model that divides the data into regions separated by a linear boundary. Here, the linear boundary divides the black circles from the white.
Machine learning tasks are classified into several broad categories. In supervised learning, the algorithm builds a mathematical model from a set of data that contains both the inputs and the desired outputs. For example, if the task were determining whether an image contained a certain object, the training data for a supervised learning algorithm would include images with and without that object (the input), and each image would have a label (the output) designating whether it contained the object. In special cases, the input may be only partially available, or restricted to special feedback.[clarification needed] Semi-supervised learning algorithms develop mathematical models from incomplete training data, where a portion of the sample input doesn't have labels.
Classification algorithms and regression algorithms are types of supervised learning. Classification algorithms are used when the outputs are restricted to a limited set of values. For a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email. For an algorithm that identifies spam emails, the output would be the prediction of either "spam" or "not spam", represented by the Boolean values true and false. Regression algorithms are named for their continuous outputs, meaning they may have any value within a range. Examples of a continuous value are the temperature, length, or price of an object.
In unsupervised learning, the algorithm builds a mathematical model from a set of data which contains only inputs and no desired output labels. Unsupervised learning algorithms are used to find structure in the data, like grouping or clustering of data points. Unsupervised learning can discover patterns in the data, and can group the inputs into categories, as in feature learning. Dimensionality reduction is the process of reducing the number of "features", or inputs, in a set of data. 
Active learning algorithms access the desired outputs (training labels) for a limited set of inputs based on a budget, and optimize the choice of inputs for which it will acquire training labels. When used interactively, these can be presented to a human user for labeling. Reinforcement learning algorithms are given feedback in the form of positive or negative reinforcement in a dynamic environment, and are used in autonomous vehicles or in learning to play a game against a human opponent.[2]:3 Other specialized algorithms in machine learning include topic modeling, where the computer program is given a set of natural language documents and finds other documents that cover similar topics. Machine learning algorithms can be used to find the unobservable probability density function in density estimation problems. Meta learning algorithms learn their own inductive bias based on previous experience. In developmental robotics, robot learning algorithms generate their own sequences of learning experiences, also known as a curriculum, to cumulatively acquire new skills through self-guided exploration and social interaction with humans. These robots use guidance mechanisms such as active learning, maturation, motor synergies, and imitation.[clarification needed]

History and relationships to other fields[edit]
See also: Timeline of machine learning
Arthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term "Machine Learning" in 1959 while at IBM[8]. 
As a scientific endeavour, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed "neural networks"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[9] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[10]:488
However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[10]:488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[11] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[10]:708–710; 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as "connectionism", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[10]:25
Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.[11] It also benefited from the increasing availability of digitized information, and the ability to distribute it via the Internet.

Relation to data mining[edit]
Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.

Relation to optimization[edit]
Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.[12]

Relation to statistics[edit]
Machine learning and statistics are closely related fields. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[13] He also suggested the term data science as a placeholder to call the overall field.[13]
Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[14] wherein "algorithmic model" means more or less the machine learning algorithms like Random forest.
Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[15]

 Theory[edit]
Main articles: Computational learning theory and Statistical learning theory
A core objective of a learner is to generalize from its experience.[2][16] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.
The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.
For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfit the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[17]
In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.

Approaches[edit]
Types of learning algorithms[edit]
The types of machine learning algorithms differ in their approach, the type of data they input and output, and the type of task or problem that they are intended to solve.

Supervised learning[edit]
Main article: Supervised learning
Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[18] The data is known as training data, and consists of a set of training examples. Each training example has one or more inputs and a desired output, also known as a supervisory signal.  In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[19] An optimal function will allow the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.[6]
Supervised learning algorithms include classification and regression.[20] Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.
In the case of semi-supervised learning algorithms, some of the training examples are missing training labels, but they can nevertheless be used to improve the quality of a model. In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.[21]

Unsupervised learning[edit]
Main article: Unsupervised learningSee also: Cluster analysis
Unsupervised learning algorithms take a set of data that contains only inputs, and find structure in the data, like grouping or clustering of data points. The algorithms therefore learn from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. A central application of unsupervised learning is in the field of density estimation in statistics,[22] though unsupervised learning encompasses other domains involving summarizing and explaining data features.
Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.

Reinforcement learning[edit]
Main article: Reinforcement learning
Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms.[23][24] In machine learning, the environment is typically represented as a Markov Decision Process (MDP). Many reinforcement learning algorithms use dynamic programming techniques.[23][24][25] Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP, and are used when exact models are infeasible.[23][24] Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.

Feature learning[edit]
Main article: Feature learning
Several learning algorithms aim at discovering better representations of the inputs provided during training.[26] Classic examples include principal components analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.
Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization[27] and various forms of clustering.[28][29][30]
Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors.[31] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[32]
Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.

Sparse dictionary learning[edit]
Main article: Sparse dictionary learning
Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions, and is assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately.[33] A popular heuristic method for sparse dictionary learning is the K-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine to which classes a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[34]

Anomaly detection[edit]
Main article: Anomaly detection
In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.[35] Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.[36]
In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular, unsupervised algorithms) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.[37]
Three broad categories of anomaly detection techniques exist.[38] Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as "normal" and "abnormal" and involves training a classifier (the key difference to many other statistical classification problems is the inherent unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the model.

Association rules[edit]
Main article: Association rule learningSee also: Inductive logic programming
Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of "interestingness".[39]
Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves "rules" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[40] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.
Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets.[41] For example, the rule 



{

o
n
i
o
n
s
,
p
o
t
a
t
o
e
s

}
⇒
{

b
u
r
g
e
r

}


{\displaystyle \{\mathrm {onions,potatoes} \}\Rightarrow \{\mathrm {burger} \}}

 found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.
Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[42]
Inductive logic programming (ILP) is an approach to rule-learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.
Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[43][44][45] Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.[46] The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.

Models[edit]
Performing machine learning involves creating a model, which is trained on some training data and then can process additional data to make predictions. Various types of models have been used and researched for machine learning systems.

Artificial neural networks[edit]
Main article: Artificial neural networkSee also: Deep learning
 An artificial neural network is an interconnected group of nodes, akin to the vast network of neurons in a brain. Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another.
Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems "learn" to perform tasks by considering examples, generally without being programmed with any task-specific rules.
An ANN is a model based on a collection of connected units or nodes called "artificial neurons", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a "signal", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called "edges". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.
The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[47]

Decision trees[edit]
Main article: Decision tree learning
Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision making.

Support vector machines[edit]
Main article: Support vector machines
Support vector machines (SVMs), also known as support vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.[48]  An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.

Bayesian networks[edit]
Main article: Bayesian network
 A simple Bayesian network. Rain influences whether the sprinkler is activated, and both rain and the sprinkler influence whether the grass is wet.
A Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.

Genetic algorithms[edit]
Main article: Genetic algorithm
A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s.[49][50] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[51]

Training models[edit]
Usually, machine learning models require a lot of data in order for them to perform well. Usually, when training a machine learning model, one needs to collect a large, representative sample of data from a training set. Data from the training set can be as varied as a corpus of text, a collection of images, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model.

Federated learning[edit]
Main article: Federated learning
Federated learning is a new approach to training machine learning models that decentralizes the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.[52]

Applications[edit]
There are many applications for machine learning, including:


Agriculture
Anatomy
Adaptive websites
Affective computing
Banking
Bioinformatics
Brain–machine interfaces
Cheminformatics
Computer Networks
Computer vision
Credit-card fraud detection
Data quality
DNA sequence classification
Economics
Financial market analysis
General game playing
Handwriting recognition
Information retrieval
Insurance
Internet fraud detection
Linguistics
Machine learning control
Machine perception
Machine translation
Marketing
Medical diagnosis
Natural language processing
Natural language understanding
Online advertising
Optimization
Recommender systems
Robot locomotion
Search engines
Sentiment analysis
Sequence mining
Software engineering
Speech recognition
Structural health monitoring
Syntactic pattern recognition
Telecommunication
Theorem proving
Time series forecasting
User behavior analytics

In 2006, the online movie company Netflix held the first "Netflix Prize" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%.  A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[53] Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns ("everything is a recommendation") and they changed their recommendation engine accordingly.[54] In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis.[55] In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors' jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[56] In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings, and that it may have revealed previously unrecognized influences among artists.[57]In 2019 Springer Nature published the first research book created using machine learning.[58]

Limitations[edit]
Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.[59][60][61] Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.[62]
In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision.[63] Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of investment.[64][65]

Bias[edit]
Main article: Algorithmic bias
Machine learning approaches in particular can suffer from different data biases. A machine learning system trained on current customers only may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on man-made data, machine learning is likely to pick up the same constitutional and unconscious biases already present in society.[66] Language models learned from data have been shown to contain human-like biases.[67][68] Machine learning systems used for criminal risk assessment have been found to be biased against black people.[69][70] In 2015, Google photos would often tag black people as gorillas,[71] and in 2018 this still was not well resolved, but Google reportedly was still using the workaround to remove all gorilla from the training data, and thus was not able to recognize real gorillas at all.[72] Similar issues with recognizing non-white people have been found in many other systems.[73] In 2016, Microsoft tested a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.[74] Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.[75] Concern for reducing bias in machine learning and propelling its use for human good is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who reminds engineers that "There’s nothing artificial about AI...It’s inspired by people, it’s created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.”[76]

Model assessments[edit]
Classification machine learning models can be validated by accuracy estimation techniques like the Holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[77]
In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the False Positive Rate (FPR) as well as the False Negative Rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The Total Operating Characteristic (TOC) is an effective method to express a model's diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used Receiver Operating Characteristic (ROC) and ROC's associated Area Under the Curve (AUC).[78]

Ethics[edit]
Machine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[79] For example, using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants.[80][81] Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.
Because language contains biases, machines trained on language corpora will necessarily also learn bias.[82]
Other forms of ethical challenges, not related to personal biases, are more seen in health care. There are concerns among health care professionals that these systems might not be designed in the public's interest, but as income generating machines. This is especially true in the United States where there is a perpetual ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes in. There is huge potential for machine learning in health care to provide professionals a great tool to diagnose, medicate, and even plan recovery paths for patients, but this will not happen until the personal biases mentioned previously, and these "greed" biases are addressed.[83]

Software[edit]
Software suites containing a variety of machine learning algorithms include the following:

Free and open-source software[edit]

CNTK
Deeplearning4j
ELKI
H2O
Keras
Mahout
Mallet
mlpack
MXNet
GNU Octave
OpenNN
Orange
scikit-learn
Shogun
Spark MLlib
Apache SystemML
TensorFlow
ROOT (TMVA with ROOT)
Torch / PyTorch
Weka / MOA
Yooreeka

Proprietary software with free and open-source editions[edit]

KNIME
RapidMiner

Proprietary software[edit]

Amazon Machine Learning
Angoss KnowledgeSTUDIO
Azure Machine Learning
Ayasdi
IBM Data Science Experience
Google Prediction API
IBM SPSS Modeler
KXEN Modeler
LIONsolver
Mathematica
MATLAB
Microsoft Azure Machine Learning
Neural Designer
NeuroSolutions
Oracle Data Mining
Oracle AI Platform Cloud Service
RCASE
SAS Enterprise Miner
SequenceL
Splunk
STATISTICA Data Miner

Journals[edit]
Journal of Machine Learning Research
Machine Learning
Nature Machine Intelligence
Neural Computation
Conferences[edit]
Conference on Neural Information Processing Systems
International Conference on Machine Learning
See also[edit]


Artificial intelligence portal
Machine learning portal

Automated machine learning
Big data
Explanation-based learning
Important publications in machine learning
List of datasets for machine learning research
Predictive analytics
Quantum machine learning
Machine-learning applications in bioinformatics

References[edit]


^ The definition "without being explicitly programmed" is often attributed to Arthur Samuel, who coined the term "machine learning" in 1959, but the phrase is not found verbatim in this publication, and may be a paraphrase that appeared later. Confer "Paraphrasing Arthur Samuel (1959), the question is: How can computers learn to solve problems without being explicitly programmed?" in Koza, John R.; Bennett, Forrest H.; Andre, David; Keane, Martin A. (1996). Automated Design of Both the Topology and Sizing of Analog Electrical Circuits Using Genetic Programming. Artificial Intelligence in Design '96. Springer, Dordrecht. pp. 151–170. doi:10.1007/978-94-009-0279-4_9..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ a b c d Bishop, C. M. (2006), Pattern Recognition and Machine Learning, Springer, ISBN 978-0-387-31073-2

^ Machine learning and pattern recognition "can be viewed as two facets of the same field."[2]:vii

^ Friedman, Jerome H. (1998). "Data Mining and Statistics: What's the connection?". Computing Science and Statistics. 29 (1): 3–9.

^ Samuel, Arthur (1959). "Some Studies in Machine Learning Using the Game of Checkers". IBM Journal of Research and Development. 3 (3): 210–229. CiteSeerX 10.1.1.368.2254. doi:10.1147/rd.33.0210.

^ a b Mitchell, T. (1997). Machine Learning. McGraw Hill. p. 2. ISBN 978-0-07-042807-2.

^ Harnad, Stevan (2008), "The Annotation Game: On Turing (1950) on Computing, Machinery, and Intelligence",  in Epstein, Robert; Peters, Grace (eds.), The Turing Test Sourcebook: Philosophical and Methodological Issues in the Quest for the Thinking Computer, Kluwer

^ R. Kohavi and F. Provost, "Glossary of terms," Machine Learning, vol. 30, no. 2–3, pp. 271–274, 1998.

^ Sarle, Warren (1994). "Neural Networks and statistical models". CiteSeerX 10.1.1.27.699.

^ a b c d Russell, Stuart; Norvig, Peter (2003) [1995]. Artificial Intelligence: A Modern Approach (2nd ed.). Prentice Hall. ISBN 978-0137903955.

^ a b Langley, Pat (2011). "The changing science of machine learning". Machine Learning. 82 (3): 275–279. doi:10.1007/s10994-011-5242-y.

^ Le Roux, Nicolas; Bengio, Yoshua; Fitzgibbon, Andrew (2012). "Improving First and Second-Order Methods by Modeling Uncertainty".  In Sra, Suvrit; Nowozin, Sebastian; Wright, Stephen J. (eds.). Optimization for Machine Learning. MIT Press. p. 404.

^ a b Michael I. Jordan (2014-09-10). "statistics and machine learning". reddit. Retrieved 2014-10-01.

^ Cornell University Library. "Breiman: Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)". Retrieved 8 August 2015.

^ Gareth James; Daniela Witten; Trevor Hastie; Robert Tibshirani (2013). An Introduction to Statistical Learning. Springer. p. vii.

^ Mohri, Mehryar; Rostamizadeh, Afshin; Talwalkar, Ameet (2012). Foundations of Machine Learning. USA, Massachusetts: MIT Press. ISBN 9780262018258.

^ Alpaydin, Ethem (2010). Introduction to Machine Learning. London: The MIT Press. ISBN 978-0-262-01243-0. Retrieved 4 February 2017.

^ Russell, Stuart J.; Norvig, Peter (2010). Artificial Intelligence: A Modern Approach (Third ed.). Prentice Hall. ISBN 9780136042594.

^ Mohri, Mehryar; Rostamizadeh, Afshin; Talwalkar, Ameet (2012). Foundations of Machine Learning. The MIT Press. ISBN 9780262018258.

^ Alpaydin, Ethem (2010). Introduction to Machine Learning. MIT Press. p. 9. ISBN 978-0-262-01243-0.

^ Alex Ratner, Stephen Bach, Paroma Varma, Chris Ré And referencing work by many other members of Hazy Research. "Weak Supervision: The New Programming Paradigm for Machine Learning". hazyresearch.github.io. Retrieved 2019-06-06.CS1 maint: Multiple names: authors list (link)

^ Jordan, Michael I.; Bishop, Christopher M. (2004). "Neural Networks".  In Allen B. Tucker (ed.). Computer Science Handbook, Second Edition (Section VII: Intelligent Systems). Boca Raton, Florida: Chapman & Hall/CRC Press LLC. ISBN 978-1-58488-360-9.

^ a b c Dimitri P. Bertsekas. "Dynamic Programming and Optimal Control: Approximate Dynamic Programming, Vol.II", Athena Scientific, 2012,[1]

^ a b c Dimitri P. Bertsekas and John N. Tsitsiklis. "Neuro-Dynamic Programming", Athena Scientific, 1996,[2]

^ van Otterlo, M.; Wiering, M. (2012). Reinforcement learning and markov decision processes. Reinforcement Learning. Adaptation, Learning, and Optimization. 12. pp. 3–42. doi:10.1007/978-3-642-27645-3_1. ISBN 978-3-642-27644-6.

^ Y. Bengio; A. Courville; P. Vincent (2013). "Representation Learning: A Review and New Perspectives". IEEE Trans. PAMI, Special Issue Learning Deep Architectures. 35 (8): 1798–1828. arXiv:1206.5538. doi:10.1109/tpami.2013.50. PMID 23787338.

^ Nathan Srebro; Jason D. M. Rennie; Tommi S. Jaakkola (2004). Maximum-Margin Matrix Factorization. NIPS.

^ Coates, Adam; Lee, Honglak; Ng, Andrew Y. (2011). An analysis of single-layer networks in unsupervised feature learning (PDF). Int'l Conf. on AI and Statistics (AISTATS).

^ Csurka, Gabriella; Dance, Christopher C.; Fan, Lixin; Willamowski, Jutta; Bray, Cédric (2004). Visual categorization with bags of keypoints (PDF). ECCV Workshop on Statistical Learning in Computer Vision.

^ Daniel Jurafsky; James H. Martin (2009). Speech and Language Processing. Pearson Education International. pp. 145–146.

^ Lu, Haiping; Plataniotis, K.N.; Venetsanopoulos, A.N. (2011). "A Survey of Multilinear Subspace Learning for Tensor Data" (PDF). Pattern Recognition. 44 (7): 1540–1551. doi:10.1016/j.patcog.2011.01.004.

^ Yoshua Bengio (2009). Learning Deep Architectures for AI. Now Publishers Inc. pp. 1–3. ISBN 978-1-60198-294-0.

^ Tillmann, A. M. (2015). "On the Computational Intractability of Exact and Approximate Dictionary Learning". IEEE Signal Processing Letters. 22 (1): 45–49. arXiv:1405.6664. Bibcode:2015ISPL...22...45T. doi:10.1109/LSP.2014.2345761.

^ Aharon, M, M Elad, and A Bruckstein. 2006. "K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation." Signal Processing, IEEE Transactions on 54 (11): 4311–4322

^ Zimek, Arthur; Schubert, Erich (2017), "Outlier Detection", Encyclopedia of Database Systems, Springer New York, pp. 1–5, doi:10.1007/978-1-4899-7993-3_80719-1, ISBN 9781489979933

^ Hodge, V. J.; Austin, J. (2004). "A Survey of Outlier Detection Methodologies" (PDF). Artificial Intelligence Review. 22 (2): 85–126. CiteSeerX 10.1.1.318.4023. doi:10.1007/s10462-004-4304-y.

^ Dokas, Paul; Ertoz, Levent; Kumar, Vipin; Lazarevic, Aleksandar; Srivastava, Jaideep; Tan, Pang-Ning (2002). "Data mining for network intrusion detection" (PDF). Proceedings NSF Workshop on Next Generation Data Mining.

^ Chandola, V.; Banerjee, A.; Kumar, V. (2009). "Anomaly detection: A survey". ACM Computing Surveys. 41 (3): 1–58. doi:10.1145/1541880.1541882.

^ Piatetsky-Shapiro, Gregory (1991), Discovery, analysis, and presentation of strong rules, in Piatetsky-Shapiro, Gregory; and Frawley, William J.; eds., Knowledge Discovery in Databases, AAAI/MIT Press, Cambridge, MA.

^ Bassel, George W.; Glaab, Enrico; Marquez, Julietta; Holdsworth, Michael J.; Bacardit, Jaume (2011-09-01). "Functional Network Construction in Arabidopsis Using Rule-Based Machine Learning on Large-Scale Data Sets". The Plant Cell. 23 (9): 3101–3116. doi:10.1105/tpc.111.088153. ISSN 1532-298X. PMC 3203449. PMID 21896882.

^ Agrawal, R.; Imieliński, T.; Swami, A. (1993). "Mining association rules between sets of items in large databases". Proceedings of the 1993 ACM SIGMOD international conference on Management of data - SIGMOD '93. p. 207. CiteSeerX 10.1.1.40.6984. doi:10.1145/170035.170072. ISBN 978-0897915922.

^ Urbanowicz, Ryan J.; Moore, Jason H. (2009-09-22). "Learning Classifier Systems: A Complete Introduction, Review, and Roadmap". Journal of Artificial Evolution and Applications. 2009: 1–25. doi:10.1155/2009/736398. ISSN 1687-6229.

^ Plotkin G.D. Automatic Methods of Inductive Inference, PhD thesis, University of Edinburgh, 1970.

^ Shapiro, Ehud Y. Inductive inference of theories from facts, Research Report 192, Yale University, Department of Computer Science, 1981. Reprinted in J.-L. Lassez, G. Plotkin (Eds.), Computational Logic, The MIT Press, Cambridge, MA, 1991, pp. 199–254.

^ Shapiro, Ehud Y. (1983). Algorithmic program debugging. Cambridge, Mass: MIT Press. ISBN 0-262-19218-7

^ Shapiro, Ehud Y. "The model inference system." Proceedings of the 7th international joint conference on Artificial intelligence-Volume 2. Morgan Kaufmann Publishers Inc., 1981.

^ Honglak Lee, Roger Grosse, Rajesh Ranganath, Andrew Y. Ng. "Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations" Proceedings of the 26th Annual International Conference on Machine Learning, 2009.

^ Cortes, Corinna; Vapnik, Vladimir N. (1995). "Support-vector networks". Machine Learning. 20 (3): 273–297. doi:10.1007/BF00994018.

^ Goldberg, David E.; Holland, John H. (1988). "Genetic algorithms and machine learning". Machine Learning. 3 (2): 95–99. doi:10.1007/bf00113892.

^ Michie, D.; Spiegelhalter, D. J.; Taylor, C. C. (1994). "Machine Learning, Neural and Statistical Classification". Ellis Horwood Series in Artificial Intelligence. Bibcode:1994mlns.book.....M.

^ Zhang, Jun; Zhan, Zhi-hui; Lin, Ying; Chen, Ni; Gong, Yue-jiao; Zhong, Jing-hui; Chung, Henry S.H.; Li, Yun; Shi, Yu-hui (2011). "Evolutionary Computation Meets Machine Learning: A Survey" (PDF). Computational Intelligence Magazine. 6 (4): 68–75. doi:10.1109/mci.2011.942584.

^ "Federated Learning: Collaborative Machine Learning without Centralized Training Data". Google AI Blog. Retrieved 2019-06-08.

^ "BelKor Home Page" research.att.com

^ "The Netflix Tech Blog: Netflix Recommendations: Beyond the 5 stars (Part 1)". 2012-04-06. Retrieved 8 August 2015.

^ Scott Patterson (13 July 2010). "Letting the Machines Decide". The Wall Street Journal. Retrieved 24 June 2018.

^ Vinod Khosla (January 10, 2012). "Do We Need Doctors or Algorithms?". Tech Crunch.

^ When A Machine Learning Algorithm Studied Fine Art Paintings, It Saw Things Art Historians Had Never Noticed, The Physics at ArXiv blog

^ Vincent, James (2019-04-10). "The first AI-generated textbook shows what robot writers are actually good at". The Verge. Retrieved 2019-05-05.

^ "Why Machine Learning Models Often Fail to Learn: QuickTake Q&A". Bloomberg.com. 2016-11-10. Retrieved 2017-04-10.

^ "The First Wave of Corporate AI Is Doomed to Fail". Harvard Business Review. 2017-04-18. Retrieved 2018-08-20.

^ "Why the A.I. euphoria is doomed to fail". VentureBeat. 2016-09-18. Retrieved 2018-08-20.

^ "9 Reasons why your machine learning project will fail". www.kdnuggets.com. Retrieved 2018-08-20.

^ "Why Uber's self-driving car killed a pedestrian". The Economist. Retrieved 2018-08-20.

^ "IBM's Watson recommended 'unsafe and incorrect' cancer treatments - STAT". STAT. 2018-07-25. Retrieved 2018-08-21.

^ Hernandez, Daniela; Greenwald, Ted (2018-08-11). "IBM Has a Watson Dilemma". Wall Street Journal. ISSN 0099-9660. Retrieved 2018-08-21.

^ Garcia, Megan (2016). "Racist in the Machine". World Policy Journal. 33 (4): 111–117. doi:10.1215/07402775-3813015. ISSN 0740-2775.

^ Caliskan, Aylin; Bryson, Joanna J.; Narayanan, Arvind (2017-04-14). "Semantics derived automatically from language corpora contain human-like biases". Science. 356 (6334): 183–186. arXiv:1608.07187. Bibcode:2017Sci...356..183C. doi:10.1126/science.aal4230. ISSN 0036-8075. PMID 28408601.

^ Wang, Xinan; Dasgupta, Sanjoy (2016),  Lee, D. D.; Sugiyama, M.; Luxburg, U. V.; Guyon, I. (eds.), "An algorithm for L1 nearest neighbor search via monotonic embedding" (PDF), Advances in Neural Information Processing Systems 29, Curran Associates, Inc., pp. 983–991, retrieved 2018-08-20

^ "Machine Bias". ProPublica. Julia Angwin, Jeff Larson, Lauren Kirchner, Surya Mattu. 2016-05-23. Retrieved 2018-08-20.CS1 maint: others (link)

^ "Opinion | When an Algorithm Helps Send You to Prison". New York Times. Retrieved 2018-08-20.

^ "Google apologises for racist blunder". BBC News. 2015-07-01. Retrieved 2018-08-20.

^ "Google 'fixed' its racist algorithm by removing gorillas from its image-labeling tech". The Verge. Retrieved 2018-08-20.

^ "Opinion | Artificial Intelligence's White Guy Problem". New York Times. Retrieved 2018-08-20.

^ Metz, Rachel. "Why Microsoft's teen chatbot, Tay, said lots of awful things online". MIT Technology Review. Retrieved 2018-08-20.

^ Simonite, Tom. "Microsoft says its racist chatbot illustrates how AI isn't adaptable enough to help most businesses". MIT Technology Review. Retrieved 2018-08-20.

^ Hempel, Jessi (2018-11-13). "Fei-Fei Li's Quest to Make Machines Better for Humanity". Wired. ISSN 1059-1028. Retrieved 2019-02-17.

^ Kohavi, Ron (1995). "A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection" (PDF). International Joint Conference on Artificial Intelligence.

^ Pontius, Robert Gilmore; Si, Kangping (2014). "The total operating characteristic to measure diagnostic ability for multiple thresholds". International Journal of Geographical Information Science. 28 (3): 570–583. doi:10.1080/13658816.2013.862623.

^ Bostrom, Nick (2011). "The Ethics of Artificial Intelligence" (PDF). Retrieved 11 April 2016.

^ Edionwe, Tolulope. "The fight against racist algorithms". The Outline. Retrieved 17 November 2017.

^ Jeffries, Adrianne. "Machine learning is racist because the internet is racist". The Outline. Retrieved 17 November 2017.

^ Narayanan, Arvind (August 24, 2016). "Language necessarily contains human biases, and so will machines trained on language corpora". Freedom to Tinker.

^ Char, D. S.; Shah, N. H.; Magnus, D. (2018). "Implementing Machine Learning in Health Care—Addressing Ethical Challenges". New England Journal of Medicine. 378 (11): 981–983. doi:10.1056/nejmp1714229. PMC 5962261. PMID 29539284.


Further reading[edit]
.mw-parser-output .refbegin{font-size:90%;margin-bottom:0.5em}.mw-parser-output .refbegin-hanging-indents>ul{list-style-type:none;margin-left:0}.mw-parser-output .refbegin-hanging-indents>ul>li,.mw-parser-output .refbegin-hanging-indents>dl>dd{margin-left:0;padding-left:3.2em;text-indent:-3.2em;list-style:none}.mw-parser-output .refbegin-100{font-size:100%}
Nils J. Nilsson, Introduction to Machine Learning.
Trevor Hastie, Robert Tibshirani and Jerome H. Friedman (2001). The Elements of Statistical Learning, Springer. ISBN 0-387-95284-5.
Pedro Domingos (September 2015), The Master Algorithm, Basic Books, ISBN 978-0-465-06570-7
Ian H. Witten and Eibe Frank (2011). Data Mining: Practical machine learning tools and techniques Morgan Kaufmann, 664pp., ISBN 978-0-12-374856-0.
Ethem Alpaydin (2004). Introduction to Machine Learning, MIT Press, ISBN 978-0-262-01243-0.
David J. C. MacKay. Information Theory, Inference, and Learning Algorithms Cambridge: Cambridge University Press, 2003. ISBN 0-521-64298-1
Richard O. Duda, Peter E. Hart, David G. Stork (2001) Pattern classification (2nd edition), Wiley, New York, ISBN 0-471-05669-3.
Christopher Bishop (1995). Neural Networks for Pattern Recognition, Oxford University Press. ISBN 0-19-853864-2.
Stuart Russell & Peter Norvig, (2009). Artificial Intelligence – A Modern Approach. Pearson, ISBN 9789332543515.
Ray Solomonoff, An Inductive Inference Machine, IRE Convention Record, Section on Information Theory, Part 2, pp., 56–62, 1957.
Ray Solomonoff, An Inductive Inference Machine A privately circulated report from the 1956 Dartmouth Summer Research Conference on AI.

External links[edit]



Wikimedia Commons has media related to Machine learning.

International Machine Learning Society
mloss is an academic database of open-source machine learning software.
Machine Learning Crash Course by Google. This is a free course on machine learning through the use of TensorFlow.
vteComputer scienceNote: This template roughly follows the 2012 ACM Computing Classification System.Hardware
Printed circuit board
Peripheral
Integrated circuit
Very Large Scale Integration
Systems on Chip (SoCs)
Energy consumption (Green computing)
Electronic design automation
Hardware acceleration
Computer systemsorganization
Computer architecture
Embedded system
Real-time computing
Dependability
Networks
Network architecture
Network protocol
Network components
Network scheduler
Network performance evaluation
Network service
Software organization
Interpreter
Middleware
Virtual machine
Operating system
Software quality
Software notationsand tools
Programming paradigm
Programming language
Compiler
Domain-specific language
Modeling language
Software framework
Integrated development environment
Software configuration management
Software library
Software repository
Software development
Software development process
Requirements analysis
Software design
Software construction
Software deployment
Software maintenance
Programming team
Open-source model
Theory of computation
Model of computation
Formal language
Automata theory
Computational complexity theory
Logic
Semantics
Algorithms
Algorithm design
Analysis of algorithms
Algorithmic efficiency
Randomized algorithm
Computational geometry
Mathematicsof computing
Discrete mathematics
Probability
Statistics
Mathematical software
Information theory
Mathematical analysis
Numerical analysis
Informationsystems
Database management system
Information storage systems
Enterprise information system
Social information systems
Geographic information system
Decision support system
Process control system
Multimedia information system
Data mining
Digital library
Computing platform
Digital marketing
World Wide Web
Information retrieval
Security
Cryptography
Formal methods
Security services
Intrusion detection system
Hardware security
Network security
Information security
Application security
Human–computerinteraction
Interaction design
Social computing
Ubiquitous computing
Visualization
Accessibility
Concurrency
Concurrent computing
Parallel computing
Distributed computing
Multithreading
Multiprocessing
Artificialintelligence
Natural language processing
Knowledge representation and reasoning
Computer vision
Automated planning and scheduling
Search methodology
Control method
Philosophy of artificial intelligence
Distributed artificial intelligence
Machine learning
Supervised learning
Unsupervised learning
Reinforcement learning
Multi-task learning
Cross-validation
Graphics
Animation
Rendering
Image manipulation
Graphics processing unit
Mixed reality
Virtual reality
Image compression
Solid modeling
Appliedcomputing
E-commerce
Enterprise software
Computational mathematics
Computational physics
Computational chemistry
Computational biology
Computational social science
Computational engineering
Computational healthcare
Digital art
Electronic publishing
Cyberwarfare
Electronic voting
Video games
Word processing
Operations research
Educational technology
Document management

 Book
 Category
 Portal
WikiProject
 Commons




Machine learning anddata mining
Problems
Classification
Clustering
Regression
Anomaly detection
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction


Supervised learning.mw-parser-output .nobold{font-weight:normal}(classification • regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)


Clustering
BIRCH
CURE
Hierarchical
k-means
Expectation–maximization (EM)
DBSCAN
OPTICS
Mean-shift


Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
t-SNE


Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov


Anomaly detection
k-NN
Local outlier factor


Artificial neural networks
Autoencoder
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net


Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)


Theory
Bias–variance dilemma
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory


Machine-learning venues
NIPS
ICML
ML
JMLR
ArXiv:cs.LG


Glossary of artificial intelligence
Glossary of artificial intelligence


Related articles
List of datasets for machine-learning research
Outline of machine learning


 Machine learning portalvte
 The Long Short-Term Memory (LSTM) cell can process data sequentially and keep its hidden state through time.
Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture[1] used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections that make it a "general purpose computer" (that is, it can compute anything that a Turing machine can).[2] It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition[3] or speech recognition.[4][5]
Bloomberg Business Week wrote: "These powers make LSTM arguably the most commercial AI achievement, used for everything from predicting diseases to composing music."[6]
A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.
LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the exploding and vanishing gradient problems that can be encountered when training traditional RNNs. Relative insensitivity to gap length is an advantage of LSTM over RNNs, hidden Markov models and other sequence learning methods in numerous applications[citation needed].

Contents

1 History
2 Idea
3 Architecture
4 Variants

4.1 LSTM with a forget gate

4.1.1 Variables
4.1.2 Activation functions


4.2 Peephole LSTM
4.3 Peephole convolutional LSTM


5 Training

5.1 CTC score function
5.2 Alternatives

5.2.1 Success




6 Applications
7 See also
8 References
9 External links


History[edit]
LSTM was proposed in 1997 by Sepp Hochreiter and Jürgen Schmidhuber.[1] By introducing Constant Error Carousel (CEC) units, LSTM deals with the exploding and vanishing gradient problems. The initial version of LSTM block included cells, input and output gates.[7]
In 1999, Felix Gers and his advisor Jürgen Schmidhuber and Fred Cummins introduced the forget gate (also called “keep gate”) into LSTM architecture,[8] 
enabling the LSTM to reset its own state.[7]
In 2000, Gers & Schmidhuber & Cummins added peephole connections (connections from the cell to the gates) into the architecture.[9] Additionally, the output activation function was omitted.[7]
In 2014, Kyunghyun Cho et al. put forward a simplified variant called Gated recurrent unit (GRU).[10]
Among other successes, LSTM achieved record results in natural language text compression,[11] unsegmented connected handwriting recognition[12] and won the ICDAR handwriting competition (2009). LSTM networks were a major component of a network that achieved a record 17.7% phoneme error rate on the classic TIMIT natural speech dataset (2013).[13]
As of 2016, major technology companies including Google, Apple, and Microsoft were using LSTM as fundamental components in new products.[14] For example, Google used LSTM for speech recognition on the smartphone,[15][16] for the smart assistant Allo[17] and for Google Translate.[18][19] Apple uses LSTM for the "Quicktype" function on the iPhone[20][21] and for Siri.[22] Amazon uses LSTM for Amazon Alexa.[23]
In 2017, Facebook performed some 4.5 billion automatic translations every day using long short-term memory networks.[24]
In 2017, researchers from Michigan State University, IBM Research, and Cornell University published a study in the Knowledge Discovery and Data Mining (KDD) conference.[25][26][27] Their study describes a novel neural network that performs better on certain data sets than the widely used long short-term memory neural network.
Further in 2017 Microsoft reported reaching 95.1% recognition accuracy on the Switchboard corpus, incorporating a vocabulary of 165,000 words. The approach used "dialog session-based long-short-term memory".[28]

Idea[edit]
In theory, classic (or "vanilla") RNNs can keep track of arbitrary long-term dependencies in the input sequences. The problem of vanilla RNNs is computational (or practical) in nature: when training a vanilla RNN using back-propagation, the gradients which are back-propagated can "vanish" (that is, they can tend to zero) or "explode" (that is, they can tend to infinity), because of the computations involved in the process, which use finite-precision numbers. RNNs using LSTM units partially solve the vanishing gradient problem, because LSTM units allow gradients to also flow unchanged. However, LSTM networks can still suffer from the exploding gradient problem.[29]

Architecture[edit]
There are several architectures of LSTM units. A common architecture is composed of a cell (the memory part of the LSTM unit) and three "regulators", usually called gates, of the flow of information inside the LSTM unit: an input gate, an output gate and a forget gate. Some variations of the LSTM unit do not have one or more of these gates or maybe have other gates. For example, gated recurrent units (GRUs) do not have an output gate.
Intuitively, the cell is responsible for keeping track of the dependencies between the elements in the input sequence. The input gate controls the extent to which a new value flows into the cell, the forget gate controls the extent to which a value remains in the cell and the output gate controls the extent to which the value in the cell is used to compute the output activation of the LSTM unit. The activation function of the LSTM gates is often the logistic function. 
There are connections into and out of the LSTM gates, a few of which are recurrent. The weights of these connections, which need to be learned during training, determine how the gates operate.

Variants[edit]
In the equations below, the lowercase variables represent vectors. Matrices 




W

q




{\displaystyle W_{q}}

 and 




U

q




{\displaystyle U_{q}}

 contain, respectively, the weights of the input and recurrent connections, where the subscript 






q




{\displaystyle _{q}}

 can either be the input gate 



i


{\displaystyle i}

, output gate 



o


{\displaystyle o}

, the forget gate 



f


{\displaystyle f}

 or the memory cell 



c


{\displaystyle c}

, depending on the activation being calculated. In this section, we are thus using a "vector notation". So, for example, 




c

t


∈


R


h




{\displaystyle c_{t}\in \mathbb {R} ^{h}}

 is not just one cell of one LSTM unit, but contains 



h


{\displaystyle h}

 LSTM unit's cells.

LSTM with a forget gate[edit]
The compact forms of the equations for the forward pass of an LSTM unit with a forget gate are:[1][9]










f

t





=

σ

g


(

W

f



x

t


+

U

f



h

t
−
1


+

b

f


)





i

t





=

σ

g


(

W

i



x

t


+

U

i



h

t
−
1


+

b

i


)





o

t





=

σ

g


(

W

o



x

t


+

U

o



h

t
−
1


+

b

o


)





c

t





=

f

t


∘

c

t
−
1


+

i

t


∘

σ

c


(

W

c



x

t


+

U

c



h

t
−
1


+

b

c


)





h

t





=

o

t


∘

σ

h


(

c

t


)






{\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}x_{t}+U_{f}h_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}x_{t}+U_{i}h_{t-1}+b_{i})\\o_{t}&=\sigma _{g}(W_{o}x_{t}+U_{o}h_{t-1}+b_{o})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}x_{t}+U_{c}h_{t-1}+b_{c})\\h_{t}&=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}


where the initial values are 




c

0


=
0


{\displaystyle c_{0}=0}

 and 




h

0


=
0


{\displaystyle h_{0}=0}

 and the operator 



∘


{\displaystyle \circ }

 denotes the Hadamard product (element-wise product). The subscript 



t


{\displaystyle t}

 indexes the time step.

Variables[edit]





x

t


∈


R


d




{\displaystyle x_{t}\in \mathbb {R} ^{d}}

: input vector to the LSTM unit





f

t


∈


R


h




{\displaystyle f_{t}\in \mathbb {R} ^{h}}

: forget gate's activation vector





i

t


∈


R


h




{\displaystyle i_{t}\in \mathbb {R} ^{h}}

: input gate's activation vector





o

t


∈


R


h




{\displaystyle o_{t}\in \mathbb {R} ^{h}}

: output gate's activation vector





h

t


∈


R


h




{\displaystyle h_{t}\in \mathbb {R} ^{h}}

: hidden state vector also known as output vector of the LSTM unit





c

t


∈


R


h




{\displaystyle c_{t}\in \mathbb {R} ^{h}}

: cell state vector




W
∈


R


h
×
d




{\displaystyle W\in \mathbb {R} ^{h\times d}}

, 



U
∈


R


h
×
h




{\displaystyle U\in \mathbb {R} ^{h\times h}}

 and 



b
∈


R


h




{\displaystyle b\in \mathbb {R} ^{h}}

: weight matrices and bias vector parameters which need to be learned during training
where the superscripts 



d


{\displaystyle d}

 and 



h


{\displaystyle h}

 refer to the number of input features and number of hidden units, respectively.

Activation functions[edit]





σ

g




{\displaystyle \sigma _{g}}

: sigmoid function.





σ

c




{\displaystyle \sigma _{c}}

: hyperbolic tangent function.





σ

h




{\displaystyle \sigma _{h}}

: hyperbolic tangent function or, as the peephole LSTM paper[30][31] suggests, 




σ

h


(
x
)
=
x


{\displaystyle \sigma _{h}(x)=x}

.
Peephole LSTM[edit]
 A peephole LSTM unit with input (i.e. 



i


{\displaystyle i}

), output (i.e. 



o


{\displaystyle o}

), and forget (i.e. 



f


{\displaystyle f}

) gates. Each of these gates can be thought as a "standard" neuron in a feed-forward (or multi-layer) neural network: that is, they compute an activation (using an activation function) of a weighted sum. 




i

t


,

o

t




{\displaystyle i_{t},o_{t}}

 and 




f

t




{\displaystyle f_{t}}

 represent the activations of respectively the input, output and forget gates, at time step 



t


{\displaystyle t}

.  The 3 exit arrows from the memory cell 



c


{\displaystyle c}

 to the 3 gates 



i
,
o


{\displaystyle i,o}

 and 



f


{\displaystyle f}

 represent the peephole connections. These peephole connections actually denote the contributions of the activation of the memory cell 



c


{\displaystyle c}

 at time step 



t
−
1


{\displaystyle t-1}

, i.e. the contribution of 




c

t
−
1




{\displaystyle c_{t-1}}

 (and not 




c

t




{\displaystyle c_{t}}

, as the picture may suggest). In other words, the gates 



i
,
o


{\displaystyle i,o}

 and 



f


{\displaystyle f}

 calculate their activations at time step 



t


{\displaystyle t}

 (i.e., respectively, 




i

t


,

o

t




{\displaystyle i_{t},o_{t}}

 and 




f

t




{\displaystyle f_{t}}

) also considering the activation of the memory cell 



c


{\displaystyle c}

 at time step 



t
−
1


{\displaystyle t-1}

, i.e. 




c

t
−
1




{\displaystyle c_{t-1}}

.  The single left-to-right arrow exiting the memory cell is not a peephole connection and denotes 




c

t




{\displaystyle c_{t}}

.  The little circles containing a 



×


{\displaystyle \times }

 symbol represent an element-wise multiplication between its inputs. The big circles containing an S-like curve represent the application of a differentiable function (like the sigmoid function) to a weighted sum.  There are many other kinds of LSTMs as well.[7]
The figure on the right is a graphical representation of an LSTM unit with peephole connections (i.e. a peephole LSTM).[30][31] Peephole connections allow the gates to access the constant error carousel (CEC), whose activation is the cell state.[32] 




h

t
−
1




{\displaystyle h_{t-1}}

 is not used, 




c

t
−
1




{\displaystyle c_{t-1}}

 is used instead in most places.










f

t





=

σ

g


(

W

f



x

t


+

U

f



c

t
−
1


+

b

f


)





i

t





=

σ

g


(

W

i



x

t


+

U

i



c

t
−
1


+

b

i


)





o

t





=

σ

g


(

W

o



x

t


+

U

o



c

t
−
1


+

b

o


)





c

t





=

f

t


∘

c

t
−
1


+

i

t


∘

σ

c


(

W

c



x

t


+

b

c


)





h

t





=

σ

h


(

o

t


∘

c

t


)






{\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}x_{t}+U_{f}c_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}x_{t}+U_{i}c_{t-1}+b_{i})\\o_{t}&=\sigma _{g}(W_{o}x_{t}+U_{o}c_{t-1}+b_{o})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}x_{t}+b_{c})\\h_{t}&=\sigma _{h}(o_{t}\circ c_{t})\end{aligned}}}


Peephole convolutional LSTM[edit]
Peephole convolutional LSTM.[33] The 



∗


{\displaystyle *}

 denotes the convolution operator.










f

t





=

σ

g


(

W

f


∗

x

t


+

U

f


∗

h

t
−
1


+

V

f


∘

c

t
−
1


+

b

f


)





i

t





=

σ

g


(

W

i


∗

x

t


+

U

i


∗

h

t
−
1


+

V

i


∘

c

t
−
1


+

b

i


)





c

t





=

f

t


∘

c

t
−
1


+

i

t


∘

σ

c


(

W

c


∗

x

t


+

U

c


∗

h

t
−
1


+

b

c


)





o

t





=

σ

g


(

W

o


∗

x

t


+

U

o


∗

h

t
−
1


+

V

o


∘

c

t


+

b

o


)





h

t





=

o

t


∘

σ

h


(

c

t


)






{\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}*x_{t}+U_{f}*h_{t-1}+V_{f}\circ c_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}*x_{t}+U_{i}*h_{t-1}+V_{i}\circ c_{t-1}+b_{i})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}*x_{t}+U_{c}*h_{t-1}+b_{c})\\o_{t}&=\sigma _{g}(W_{o}*x_{t}+U_{o}*h_{t-1}+V_{o}\circ c_{t}+b_{o})\\h_{t}&=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}


Training[edit]
A RNN using LSTM units can be trained in a supervised fashion, on a set of training sequences, using an optimization algorithm, like gradient descent, combined with backpropagation through time to compute the gradients needed during the optimization process, in order to change each weight of the LSTM network in proportion to the derivative of the error (at the output layer of the LSTM network) with respect to corresponding weight. 
A problem with using gradient descent for standard RNNs is that error gradients vanish exponentially quickly with the size of the time lag between important events. This is due to 




lim

n
→
∞



W

n


=
0


{\displaystyle \lim _{n\to \infty }W^{n}=0}

 if the spectral radius of 



W


{\displaystyle W}

 is smaller than 1.[34][35]
However, with LSTM units, when error values are back-propagated from the output layer, the error remains in the LSTM unit's cell. This "error carousel" continuously feeds error back to each of the LSTM unit's gates, until they learn to cut off the value.

CTC score function[edit]
Many applications use stacks of LSTM RNNs[36] and train them by connectionist temporal classification (CTC)[37] to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.

Alternatives[edit]
Sometimes, it can be advantageous to train (parts of) an LSTM by neuroevolution[38] or by policy gradient methods, especially when there is no "teacher" (that is, training labels).

Success[edit]
There have been several successful stories of training, in a non-supervised fashion, RNNs with LSTM units.
In 2018, Bill Gates called it a “huge milestone in advancing artificial intelligence” when bots developed by OpenAI were able to beat humans in the game of Dota 2.[39] OpenAI Five consists of five independent but coordinated neural networks. Each network is trained by a policy gradient method without supervising teacher and contains a single-layer, 1024-unit Long-Short-Term-Memory that sees the current game state and emits actions through several possible action heads.[39]
In 2018, OpenAI also trained a similar LSTM by policy gradients to control a human-like robot hand that manipulates physical objects with unprecedented dexterity.[40]
In 2019, DeepMind's program AlphaStar used a deep LSTM core to excel at the complex video game Starcraft.[41] This was viewed as significant progress towards Artificial General Intelligence.[41]

Applications[edit]
Applications of LSTM include:

Robot control[42]
Time series prediction[38][43]
Speech recognition[44][45][46]
Rhythm learning[31]
Music composition[47]
Grammar learning[48][30][49]
Handwriting recognition[50][51]
Human action recognition[52]
Sign Language Translation[53]
Protein Homology Detection[54]
Predicting subcellular localization of proteins[55]
Time series anomaly detection[56]
Several prediction tasks in the area of business process management[57]
Prediction in medical care pathways[58]
Semantic parsing[59]
Object Co-segmentation[60][61]
See also[edit]
1 the Road
Recurrent neural network
Deep learning
Gated recurrent unit
Differentiable neural computer
Long-term potentiation
Prefrontal cortex basal ganglia working memory
Time series
References[edit]


^ a b c Sepp Hochreiter; Jürgen Schmidhuber (1997). "Long short-term memory". Neural Computation. 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735. PMID 9377276..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ Siegelmann, Hava T.; Sontag, Eduardo D. (1992). On the Computational Power of Neural Nets. ACM. COLT '92. pp. 440–449. doi:10.1145/130385.130432. ISBN 978-0897914970.

^ Graves, A.; Liwicki, M.; Fernandez, S.; Bertolami, R.; Bunke, H.; Schmidhuber, J. (2009). "A Novel Connectionist System for Improved Unconstrained Handwriting Recognition" (PDF). IEEE Transactions on Pattern Analysis and Machine Intelligence. 31 (5): 855–868. CiteSeerX 10.1.1.139.4502. doi:10.1109/tpami.2008.137. PMID 19299860.

^ Sak, Hasim; Senior, Andrew; Beaufays, Francoise (2014). "Long Short-Term Memory recurrent neural network architectures for large scale acoustic modeling" (PDF).

^ Li, Xiangang; Wu, Xihong (2014-10-15). "Constructing Long Short-Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition". arXiv:1410.4281 [cs.CL].

^ Vance, Ashlee (May 15, 2018). "Quote: These powers make LSTM arguably the most commercial AI achievement, used for everything from predicting diseases to composing music". Bloomberg Business Week. Retrieved 2019-01-16.

^ a b c d Klaus Greff; Rupesh Kumar Srivastava; Jan Koutník; Bas R. Steunebrink; Jürgen Schmidhuber (2015). "LSTM: A Search Space Odyssey". IEEE Transactions on Neural Networks and Learning Systems. 28 (10): 2222–2232. arXiv:1503.04069. doi:10.1109/TNNLS.2016.2582924. PMID 27411231.

^ Felix Gers; Jürgen Schmidhuber; Fred Cummins (1999). "Learning to Forget: Continual Prediction with LSTM". Proc. ICANN'99, IEE, London: 850–855.

^ a b Felix A. Gers; Jürgen Schmidhuber; Fred Cummins (2000). "Learning to Forget: Continual Prediction with LSTM". Neural Computation. 12 (10): 2451–2471. CiteSeerX 10.1.1.55.5709. doi:10.1162/089976600300015015.

^ Cho, Kyunghyun; van Merrienboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua (2014). "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation". arXiv:1406.1078 [cs.CL].

^ "The Large Text Compression Benchmark". Retrieved 2017-01-13.

^ Graves, A.; Liwicki, M.; Fernández, S.; Bertolami, R.; Bunke, H.; Schmidhuber, J. (May 2009). "A Novel Connectionist System for Unconstrained Handwriting Recognition". IEEE Transactions on Pattern Analysis and Machine Intelligence. 31 (5): 855–868. CiteSeerX 10.1.1.139.4502. doi:10.1109/tpami.2008.137. ISSN 0162-8828. PMID 19299860.

^ Graves, Alex; Mohamed, Abdel-rahman; Hinton, Geoffrey (2013-03-22). "Speech Recognition with Deep Recurrent Neural Networks". arXiv:1303.5778 [cs.NE].

^ Metz, Cade (2016-06-14). "Apple is bringing the AI revolution to your iphone". WIRED. Retrieved 2016-06-16.

^ Beaufays, Françoise (August 11, 2015). "The neural networks behind Google Voice transcription". Research Blog. Retrieved 2017-06-27.

^ Sak, Haşim; Senior, Andrew; Rao, Kanishka; Beaufays, Françoise; Schalkwyk, Johan (September 24, 2015). "Google voice search: faster and more accurate". Research Blog. Retrieved 2017-06-27.

^ Khaitan, Pranav (May 18, 2016). "Chat Smarter with Allo". Research Blog. Retrieved 2017-06-27.

^ Wu, Yonghui; Schuster, Mike; Chen, Zhifeng; Le, Quoc V.; Norouzi, Mohammad; Macherey, Wolfgang; Krikun, Maxim; Cao, Yuan; Gao, Qin (2016-09-26). "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation". arXiv:1609.08144 [cs.CL].

^ Metz, Cade (September 27, 2016). "An Infusion of AI Makes Google Translate More Powerful Than Ever | WIRED". Wired. Retrieved 2017-06-27.

^ Efrati, Amir (June 13, 2016). "Apple's Machines Can Learn Too". The Information. Retrieved 2017-06-27.

^ Ranger, Steve (June 14, 2016). "iPhone, AI and big data: Here's how Apple plans to protect your privacy | ZDNet". ZDNet. Retrieved 2017-06-27.

^ Smith, Chris (2016-06-13). "iOS 10: Siri now works in third-party apps, comes with extra AI features". BGR. Retrieved 2017-06-27.

^ Vogels, Werner (30 November 2016). "Bringing the Magic of Amazon AI and Alexa to Apps on AWS. - All Things Distributed". www.allthingsdistributed.com. Retrieved 2017-06-27.

^ Ong, Thuy (4 August 2017). "Facebook's translations are now powered completely by AI". www.allthingsdistributed.com. Retrieved 2019-02-15.

^ "Patient Subtyping via Time-Aware LSTM Networks" (PDF). msu.edu. Retrieved 21 Nov 2018.

^ "Patient Subtyping via Time-Aware LSTM Networks". Kdd.org. Retrieved 24 May 2018.

^ "SIGKDD". Kdd.org. Retrieved 24 May 2018.

^ Haridy, Rich (August 21, 2017). "Microsoft's speech recognition system is now as good as a human". newatlas.com. Retrieved 2017-08-27.

^ bro, n. "Why can RNNs with LSTM units also suffer from "exploding gradients"?". Cross Validated. Retrieved 25 December 2018.

^ a b c Gers, F. A.; Schmidhuber, J. (2001). "LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages" (PDF). IEEE Transactions on Neural Networks. 12 (6): 1333–1340. doi:10.1109/72.963769. PMID 18249962.

^ a b c Gers, F.; Schraudolph, N.; Schmidhuber, J. (2002). "Learning precise timing with LSTM recurrent networks" (PDF). Journal of Machine Learning Research. 3: 115–143.

^ Gers, F. A.; Schmidhuber, E. (November 2001). "LSTM recurrent networks learn simple context-free and context-sensitive languages" (PDF). IEEE Transactions on Neural Networks. 12 (6): 1333–1340. doi:10.1109/72.963769. ISSN 1045-9227. PMID 18249962.

^ Xingjian Shi; Zhourong Chen; Hao Wang; Dit-Yan Yeung; Wai-kin Wong; Wang-chun Woo (2015). "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting". Proceedings of the 28th International Conference on Neural Information Processing Systems: 802–810. arXiv:1506.04214. Bibcode:2015arXiv150604214S.

^ S. Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut f. Informatik, Technische Univ. Munich, 1991.

^ Hochreiter, S.; Bengio, Y.; Frasconi, P.; Schmidhuber, J. (2001). "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies (PDF Download Available)".  In Kremer and, S. C.; Kolen, J. F. (eds.). A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press.

^ Fernández, Santiago; Graves, Alex; Schmidhuber, Jürgen (2007). "Sequence labelling in structured domains with hierarchical recurrent neural networks". Proc. 20th Int. Joint Conf. On Artificial Intelligence, Ijcai 2007: 774–779. CiteSeerX 10.1.1.79.1887.

^ Graves, Alex; Fernández, Santiago; Gomez, Faustino (2006). "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks". In Proceedings of the International Conference on Machine Learning, ICML 2006: 369–376. CiteSeerX 10.1.1.75.6306.

^ a b Wierstra, Daan; Schmidhuber, J.; Gomez, F. J. (2005). "Evolino: Hybrid Neuroevolution/Optimal Linear Search for Sequence Learning". Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI), Edinburgh: 853–858.

^ a b Rodriguez, Jesus (July 2, 2018). "The Science Behind OpenAI Five that just Produced One of the Greatest Breakthrough in the History of AI". Towards Data Science. Retrieved 2019-01-15.

^ "Learning Dexterity". OpenAI Blog. July 30, 2018. Retrieved 2019-01-15.

^ a b Stanford, Stacy (January 25, 2019). "DeepMind's AI, AlphaStar Showcases Significant Progress Towards AGI". Medium ML Memoirs. Retrieved 2019-01-15.

^ Mayer, H.; Gomez, F.; Wierstra, D.; Nagy, I.; Knoll, A.; Schmidhuber, J. (October 2006). A System for Robotic Heart Surgery that Learns to Tie Knots Using Recurrent Neural Networks. 2006 IEEE/RSJ International Conference on Intelligent Robots and Systems. pp. 543–548. CiteSeerX 10.1.1.218.3399. doi:10.1109/IROS.2006.282190. ISBN 978-1-4244-0258-8.

^ Petneházi, Gábor (2019-01-01). "Recurrent neural networks for time series forecasting". arXiv:1901.00069 [cs.LG].

^ Graves, A.; Schmidhuber, J. (2005). "Framewise phoneme classification with bidirectional LSTM and other neural network architectures". Neural Networks. 18 (5–6): 602–610. CiteSeerX 10.1.1.331.5800. doi:10.1016/j.neunet.2005.06.042. PMID 16112549.

^ Fernández, Santiago; Graves, Alex; Schmidhuber, Jürgen (2007). An Application of Recurrent Neural Networks to Discriminative Keyword Spotting. Proceedings of the 17th International Conference on Artificial Neural Networks. ICANN'07. Berlin, Heidelberg: Springer-Verlag. pp. 220–229. ISBN 978-3540746935.

^ Graves, Alex; Mohamed, Abdel-rahman; Hinton, Geoffrey (2013). "Speech Recognition with Deep Recurrent Neural Networks". Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on: 6645–6649.

^ Eck, Douglas; Schmidhuber, Jürgen (2002-08-28). Learning the Long-Term Structure of the Blues. Artificial Neural Networks — ICANN 2002. Lecture Notes in Computer Science. 2415. Springer, Berlin, Heidelberg. pp. 284–289. CiteSeerX 10.1.1.116.3620. doi:10.1007/3-540-46084-5_47. ISBN 978-3540460848.

^ Schmidhuber, J.; Gers, F.; Eck, D.; Schmidhuber, J.; Gers, F. (2002). "Learning nonregular languages: A comparison of simple recurrent networks and LSTM". Neural Computation. 14 (9): 2039–2041. CiteSeerX 10.1.1.11.7369. doi:10.1162/089976602320263980. PMID 12184841.

^ Perez-Ortiz, J. A.; Gers, F. A.; Eck, D.; Schmidhuber, J. (2003). "Kalman filters improve LSTM network performance in problems unsolvable by traditional recurrent nets". Neural Networks. 16 (2): 241–250. CiteSeerX 10.1.1.381.1992. doi:10.1016/s0893-6080(02)00219-8. PMID 12628609.

^ A. Graves, J. Schmidhuber. Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks. Advances in Neural Information Processing Systems 22, NIPS'22, pp 545–552, Vancouver, MIT Press, 2009.

^ Graves, Alex; Fernández, Santiago; Liwicki, Marcus; Bunke, Horst; Schmidhuber, Jürgen (2007). Unconstrained Online Handwriting Recognition with Recurrent Neural Networks. Proceedings of the 20th International Conference on Neural Information Processing Systems. NIPS'07. USA: Curran Associates Inc. pp. 577–584. ISBN 9781605603520.

^ M. Baccouche, F. Mamalet, C Wolf, C. Garcia, A. Baskurt. Sequential Deep Learning for Human Action Recognition. 2nd International Workshop on Human Behavior Understanding (HBU), A.A. Salah, B. Lepri ed. Amsterdam, Netherlands. pp. 29–39. Lecture Notes in Computer Science 7065. Springer. 2011

^ Huang, Jie; Zhou, Wengang; Zhang, Qilin; Li, Houqiang; Li, Weiping (2018-01-30). "Video-based Sign Language Recognition without Temporal Segmentation". arXiv:1801.10111 [cs.CV].

^ Hochreiter, S.; Heusel, M.; Obermayer, K. (2007). "Fast model-based protein homology detection without alignment". Bioinformatics. 23 (14): 1728–1736. doi:10.1093/bioinformatics/btm247. PMID 17488755.

^ Thireou, T.; Reczko, M. (2007). "Bidirectional Long Short-Term Memory Networks for predicting the subcellular localization of eukaryotic proteins". IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB). 4 (3): 441–446. doi:10.1109/tcbb.2007.1015. PMID 17666763.

^ Malhotra, Pankaj; Vig, Lovekesh; Shroff, Gautam; Agarwal, Puneet (April 2015). "Long Short Term Memory Networks for Anomaly Detection in Time Series" (PDF). European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning — ESANN 2015.

^ Tax, N.; Verenich, I.; La Rosa, M.; Dumas, M. (2017). Predictive Business Process Monitoring with LSTM neural networks. Proceedings of the International Conference on Advanced Information Systems Engineering (CAiSE). Lecture Notes in Computer Science. 10253. pp. 477–492. arXiv:1612.02130. doi:10.1007/978-3-319-59536-8_30. ISBN 978-3-319-59535-1.

^ Choi, E.; Bahadori, M.T.; Schuetz, E.; Stewart, W.; Sun, J. (2016). "Doctor AI: Predicting Clinical Events via Recurrent Neural Networks". Proceedings of the 1st Machine Learning for Healthcare Conference: 301–318. arXiv:1511.05942. Bibcode:2015arXiv151105942C.

^ Jia, Robin; Liang, Percy (2016-06-11). "Data Recombination for Neural Semantic Parsing". arXiv:1606.03622 [cs].

^ Wang, Le; Duan, Xuhuan; Zhang, Qilin; Niu, Zhenxing; Hua, Gang; Zheng, Nanning (2018-05-22). "Segment-Tube: Spatio-Temporal Action Localization in Untrimmed Videos with Per-Frame Segmentation" (PDF). Sensors. 18 (5): 1657. doi:10.3390/s18051657. ISSN 1424-8220. PMC 5982167. PMID 29789447.

^ Duan, Xuhuan; Wang, Le; Zhai, Changbo; Zheng, Nanning; Zhang, Qilin; Niu, Zhenxing; Hua, Gang (2018). Joint Spatio-Temporal Action Localization in Untrimmed Videos with Per-Frame Segmentation. 25th IEEE International Conference on Image Processing (ICIP). doi:10.1109/icip.2018.8451692. ISBN 978-1-4799-7061-2.


External links[edit]
Recurrent Neural Networks with over 30 LSTM papers by Jürgen Schmidhuber's group at IDSIA
Gers, Felix (2001). "Long Short-Term Memory in Recurrent Neural Networks" (PDF). PhD thesis.
Gers, Felix A.; Schraudolph, Nicol N.; Schmidhuber, Jürgen (Aug 2002). "Learning precise timing with LSTM recurrent networks" (PDF). Journal of Machine Learning Research. 3: 115–143.
Abidogun, Olusola Adeniyi (2005). "Data Mining, Fraud Detection and Mobile Telecommunications: Call Pattern Analysis with Unsupervised Neural Networks". Master's Thesis. hdl:11394/249. Archived (PDF) from the original on May 22, 2012.
original with two chapters devoted to explaining recurrent neural networks, especially LSTM.
Monner, Derek D.; Reggia, James A. (2010). "A generalized LSTM-like training algorithm for second-order recurrent neural networks" (PDF). High-performing extension of LSTM that has been simplified to a single node type and can train arbitrary architectures
Herta, Christian. "How to implement LSTM in Python with Theano". Tutorial.
Chevalier, Guillaume. Tutorial: How to use LSTMs with TensorFlow in Python on cellphone sensor data on GitHub



Overview of and topical guide to machine learning
Machine learning anddata mining
Problems
Classification
Clustering
Regression
Anomaly detection
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction


Supervised learning.mw-parser-output .nobold{font-weight:normal}(classification • regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)


Clustering
BIRCH
CURE
Hierarchical
k-means
Expectation–maximization (EM)
DBSCAN
OPTICS
Mean-shift


Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
t-SNE


Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov


Anomaly detection
k-NN
Local outlier factor


Artificial neural networks
Autoencoder
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net


Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)


Theory
Bias–variance dilemma
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory


Machine-learning venues
NIPS
ICML
ML
JMLR
ArXiv:cs.LG


Glossary of artificial intelligence
Glossary of artificial intelligence


Related articles
List of datasets for machine-learning research
Outline of machine learning


 Machine learning portalvte
The following outline is provided as an overview of and topical guide to machine learning. Machine learning is a subfield of soft computing within computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] In 1959, Arthur Samuel defined machine learning as a "field of study that gives computers the ability to learn without being explicitly programmed".[2] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[3] Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions.

.mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}Contents

1 What type of thing is machine learning?
2 Branches of machine learning

2.1 Subfields of machine learning
2.2 Cross-disciplinary fields involving machine learning


3 Applications of machine learning
4 Machine learning hardware
5 Machine learning tools

5.1 Machine learning frameworks

5.1.1 Proprietary machine learning frameworks
5.1.2 Open source machine learning frameworks


5.2 Machine learning libraries
5.3 Machine learning algorithms

5.3.1 Types of machine learning algorithms




6 Machine learning methods

6.1 Dimensionality reduction
6.2 Ensemble learning
6.3 Meta learning
6.4 Reinforcement learning
6.5 Supervised learning

6.5.1 Bayesian
6.5.2 Decision tree algorithms
6.5.3 Linear classifier


6.6 Unsupervised learning

6.6.1 Artificial neural networks
6.6.2 Association rule learning
6.6.3 Hierarchical clustering
6.6.4 Cluster analysis
6.6.5 Anomaly detection


6.7 Semi-supervised learning
6.8 Deep learning
6.9 Other machine learning methods and problems


7 Machine learning research
8 History of machine learning
9 Machine learning projects
10 Machine learning organizations

10.1 Machine learning conferences and workshops


11 Machine learning publications

11.1 Books on machine learning
11.2 Machine learning journals


12 Persons influential in machine learning
13 See also

13.1 Other


14 Further reading
15 References
16 External links



What type of thing is machine learning?[edit]
An academic discipline
A branch of science
An applied science
A subfield of computer science
A branch of artificial intelligence
A subfield of soft computing
Branches of machine learning[edit]
Subfields of machine learning[edit]
Subfields of machine learning

Computational learning theory – studying the design and analysis of machine learning algorithms.[4]
Grammar induction
Meta learning
Cross-disciplinary fields involving machine learning[edit]
Cross-disciplinary fields involving machine learning

Adversarial machine learning
Predictive analytics
Quantum machine learning
Robot learning
Developmental robotics
Applications of machine learning[edit]
Applications of machine learning

Biomedical informatics
Computer vision
Customer relationship management –
Data mining
Email filtering
Inverted pendulum – balance and equilibrium system.
Natural language processing (NLP)
Automatic summarization
Automatic taxonomy construction
Dialog system
Grammar checker
Language recognition
Handwriting recognition
Optical character recognition
Speech recognition
Machine translation
Question answering
Speech synthesis
Text mining
Term frequency–inverse document frequency (tf–idf)
Text simplification
Pattern recognition
Facial recognition system
Handwriting recognition
Image recognition
Optical character recognition
Speech recognition
Recommendation system
Collaborative filtering
Content-based filtering
Hybrid recommender systems (Collaborative and content-based filtering)
Search engine
Search engine optimization
Social Engineering
Machine learning hardware[edit]
Machine learning hardware

Graphics processing unit
Tensor processing unit
Vision processing unit
Machine learning tools[edit]
Machine learning tools   (list)

Comparison of deep learning software
Comparison of deep learning software/Resources
Machine learning frameworks[edit]
Machine learning framework

Proprietary machine learning frameworks[edit]
Proprietary machine learning frameworks

Amazon Machine Learning
 Microsoft Azure Machine Learning Studio
DistBelief – replaced by TensorFlow

Open source machine learning frameworks[edit]
Open source machine learning frameworks

Apache Singa
Caffe
H2O
PyTorch
mlpack
TensorFlow

Torch
CNTK
Accord.Net

Machine learning libraries[edit]
Machine learning library   

Deeplearning4j
Theano
Scikit-learn
Machine learning algorithms[edit]
Machine learning algorithm

Types of machine learning algorithms[edit]
Almeida–Pineda recurrent backpropagation
ALOPEX
Backpropagation
Bootstrap aggregating
CN2 algorithm
Constructing skill trees
Dehaene–Changeux model
Diffusion map
Dominance-based rough set approach
Dynamic time warping
Error-driven learning
Evolutionary multimodal optimization
Expectation–maximization algorithm
FastICA
Forward–backward algorithm
GeneRec
Genetic Algorithm for Rule Set Production
Growing self-organizing map
HEXQ
Hyper basis function network
IDistance
K-nearest neighbors algorithm
Kernel methods for vector output
Kernel principal component analysis
Leabra
Linde–Buzo–Gray algorithm
Local outlier factor
Logic learning machine
LogitBoost
Manifold alignment
Minimum redundancy feature selection
Mixture of experts
Multiple kernel learning
Non-negative matrix factorization
Online machine learning
Out-of-bag error
Prefrontal cortex basal ganglia working memory
PVLV
Q-learning
Quadratic unconstrained binary optimization
Query-level feature
Quickprop
Radial basis function network
Randomized weighted majority algorithm
Reinforcement learning
Repeated incremental pruning to produce error reduction (RIPPER)
Rprop
Rule-based machine learning
Skill chaining
Sparse PCA
State–action–reward–state–action
Stochastic gradient descent
Structured kNN
T-distributed stochastic neighbor embedding
Temporal difference learning
Wake-sleep algorithm
Weighted majority algorithm (machine learning)
Machine learning methods[edit]
Machine learning method   (list)

Instance-based algorithm
K-nearest neighbors algorithm (KNN)
Learning vector quantization (LVQ)
Self-organizing map (SOM)
Regression analysis
Logistic regression
Ordinary least squares regression (OLSR)
Linear regression
Stepwise regression
Multivariate adaptive regression splines (MARS)
Regularization algorithm
Ridge regression
Least Absolute Shrinkage and Selection Operator (LASSO)
Elastic net
Least-angle regression (LARS)
Classifiers
Probabilistic classifier
Naive Bayes classifier
Binary classifier
Linear classifier
Hierarchical classifier
Dimensionality reduction[edit]
Dimensionality reduction

Canonical correlation analysis (CCA)
Factor analysis
Feature extraction
Feature selection
Independent component analysis (ICA)
Linear discriminant analysis (LDA)
Multidimensional scaling (MDS)
Non-negative matrix factorization (NMF)
Partial least squares regression (PLSR)
Principal component analysis (PCA)
Principal component regression (PCR)
Projection pursuit
Sammon mapping
t-distributed stochastic neighbor embedding (t-SNE)
Ensemble learning[edit]
Ensemble learning

AdaBoost
Boosting
Bootstrap aggregating (Bagging)
Ensemble averaging – process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model. Frequently an ensemble of models performs better than any individual model, because the various errors of the models "average out."
Gradient boosted decision tree (GBDT)
Gradient boosting machine (GBM)
Random Forest
Stacked Generalization (blending)
Meta learning[edit]
Meta learning

Inductive bias
Metadata
Reinforcement learning[edit]
Reinforcement learning

Q-learning
State–action–reward–state–action (SARSA)
Temporal difference learning (TD)
Learning Automata
Supervised learning[edit]
Supervised learning

AODE
Artificial neural network
Association rule learning algorithms
Apriori algorithm
Eclat algorithm
Case-based reasoning
Gaussian process regression
Gene expression programming
Group method of data handling (GMDH)
Inductive logic programming
Instance-based learning
Lazy learning
Learning Automata
Learning Vector Quantization
Logistic Model Tree
Minimum message length (decision trees, decision graphs, etc.)
Nearest Neighbor Algorithm
Analogical modeling
Probably approximately correct learning (PAC) learning
Ripple down rules, a knowledge acquisition methodology
Symbolic machine learning algorithms
Support vector machines
Random Forests
Ensembles of classifiers
Bootstrap aggregating (bagging)
Boosting (meta-algorithm)
Ordinal classification
Information fuzzy networks (IFN)
Conditional Random Field
ANOVA
Quadratic classifiers
k-nearest neighbor
Boosting
SPRINT
Bayesian networks
Naive Bayes
Hidden Markov models
Hierarchical hidden Markov model
Bayesian[edit]
Bayesian statistics

Bayesian knowledge base
Naive Bayes
Gaussian Naive Bayes
Multinomial Naive Bayes
Averaged One-Dependence Estimators (AODE)
Bayesian Belief Network (BBN)
Bayesian Network (BN)
Decision tree algorithms[edit]
Decision tree algorithm

Decision tree
Classification and regression tree (CART)
Iterative Dichotomiser 3 (ID3)
C4.5 algorithm
C5.0 algorithm
Chi-squared Automatic Interaction Detection (CHAID)
Decision stump
Conditional decision tree
ID3 algorithm
Random forest
SLIQ
Linear classifier[edit]
Linear classifier

Fisher's linear discriminant
Linear regression
Logistic regression
Multinomial logistic regression
Naive Bayes classifier
Perceptron
Support vector machine
Unsupervised learning[edit]
Unsupervised learning

Expectation-maximization algorithm
Vector Quantization
Generative topographic map
Information bottleneck method
Artificial neural networks[edit]
Artificial neural network

Feedforward neural network
Extreme learning machine
Convolutional neural network
Recurrent neural network
Long short-term memory (LSTM)
Logic learning machine
Self-organizing map
Association rule learning[edit]
Association rule learning

Apriori algorithm
Eclat algorithm
FP-growth algorithm
Hierarchical clustering[edit]
Hierarchical clustering

Single-linkage clustering
Conceptual clustering
Cluster analysis[edit]
Cluster analysis

BIRCH
DBSCAN
Expectation-maximization (EM)
Fuzzy clustering
Hierarchical Clustering
K-means clustering
K-medians
Mean-shift
OPTICS algorithm
Anomaly detection[edit]
Anomaly detection

k-nearest neighbors classification (k-NN)
Local outlier factor
Semi-supervised learning[edit]
Semi-supervised learning

Active learning – special case of semi-supervised learning in which a learning algorithm is able to interactively query the user (or some other information source) to obtain the desired outputs at new data points.[5] [6]
Generative models
Low-density separation
Graph-based methods
Co-training
Transduction
Deep learning[edit]
Deep learning

Deep belief networks
Deep Boltzmann machines
Deep Convolutional neural networks
Deep Recurrent neural networks
Hierarchical temporal memory
Generative Adversarial Networks
Deep Boltzmann Machine (DBM)
Stacked Auto-Encoders
Other machine learning methods and problems[edit]
Anomaly detection
Association rules
Bias-variance dilemma
Classification
Multi-label classification
Clustering
Data Pre-processing
Empirical risk minimization
Feature engineering
Feature learning
Learning to rank
Occam learning
Online machine learning
PAC learning
Regression
Reinforcement Learning
Semi-supervised learning
Statistical learning
Structured prediction
Graphical models
Bayesian network
Conditional random field (CRF)
Hidden Markov model (HMM)
Unsupervised learning
VC theory
Machine learning research[edit]
List of artificial intelligence projects
List of datasets for machine learning research
History of machine learning[edit]
History of machine learning

Timeline of machine learning
Machine learning projects[edit]
Machine learning projects

DeepMind
Google Brain
Machine learning organizations[edit]
Machine learning organizations

Knowledge Engineering and Machine Learning Group
Machine learning conferences and workshops[edit]
Artificial Intelligence and Security (AISec) (co-located workshop with CCS)
Conference on Neural Information Processing Systems (NIPS)
ECML PKDD
International Conference on Machine Learning (ICML)
ML4ALL (Machine Learning For All)
Machine learning publications[edit]
Books on machine learning[edit]
This section needs expansion with: content. You can help by adding to it. (November 2018)
Books about machine learning

Machine learning journals[edit]
Machine Learning
Journal of Machine Learning Research (JMLR)
Neural Computation
Persons influential in machine learning[edit]
Alberto Broggi
Andrei Knyazev
Andrew McCallum
Andrew Ng
Anuraag Jain
Armin B. Cremers
Ayanna Howard
Barney Pell
Ben Goertzel
Ben Taskar
Bernhard Schölkopf
Brian D. Ripley
Christopher G. Atkeson
Corinna Cortes
Demis Hassabis
Douglas Lenat
Eric Xing
Ernst Dickmanns
Geoffrey Hinton – co-inventor of the backpropagation and contrastive divergence training algorithms
Hans-Peter Kriegel
Hartmut Neven
Heikki Mannila
Ian Goodfellow – Father of Generative & adversarial networks [7]
Jacek M. Zurada
Jaime Carbonell
Jeremy Slovak
Jerome H. Friedman
John D. Lafferty
John Platt – invented SMO and Platt scaling
Julie Beth Lovins
Jürgen Schmidhuber
Karl Steinbuch
Katia Sycara
Leo Breiman – invented bagging and random forests
Lise Getoor
Luca Maria Gambardella
Léon Bottou
Marcus Hutter
Mehryar Mohri
Michael Collins
Michael I. Jordan
Michael L. Littman
Nando de Freitas
Ofer Dekel
Oren Etzioni
Pedro Domingos
Peter Flach
Pierre Baldi
Pushmeet Kohli
Ray Kurzweil
Rayid Ghani
Ross Quinlan
Salvatore J. Stolfo
Sebastian Thrun
Selmer Bringsjord
Sepp Hochreiter
Shane Legg
Stephen Muggleton
Steve Omohundro
Tom M. Mitchell
Trevor Hastie
Vasant Honavar
Vladimir Vapnik – co-inventor of the SVM and VC theory
Yann LeCun – invented convolutional neural networks
Yasuo Matsuyama
Yoshua Bengio
Zoubin Ghahramani
See also[edit]


Machine learning portal
Outline of artificial intelligence
Outline of computer vision
Outline of robotics
Accuracy paradox
Action model learning
Activation function
Activity recognition
ADALINE
Adaptive neuro fuzzy inference system
Adaptive resonance theory
Additive smoothing
Adjusted mutual information
Aika (software)
AIVA
AIXI
AlchemyAPI
AlexNet
Algorithm selection
Algorithmic inference
Algorithmic learning theory
AlphaGo
AlphaGo Zero
Alternating decision tree
Apprenticeship learning
Causal Markov condition
Competitive learning
Concept learning
Decision tree learning
Distribution learning theory
Eager learning
End-to-end reinforcement learning
Error tolerance (PAC learning)
Explanation-based learning
Feature
GloVe
Hyperparameter
IBM Machine Learning Hub
Inferential theory of learning
Learning automata
Learning classifier system
Learning rule
Learning with errors
M-Theory (learning framework)
Machine learning control
Machine learning in bioinformatics
Margin
Markov chain geostatistics
Markov chain Monte Carlo (MCMC)
Markov information source
Markov logic network
Markov model
Markov random field
Markovian discrimination
Maximum-entropy Markov model
Multi-armed bandit
Multi-task learning
Multilinear subspace learning
Multimodal learning
Multiple instance learning
Multiple-instance learning
Never-Ending Language Learning
Offline learning
Parity learning
Population-based incremental learning
Predictive learning
Preference learning
Proactive learning
Proximal gradient methods for learning
Semantic analysis
Similarity learning
Sparse dictionary learning
Stability (learning theory)
Statistical learning theory
Statistical relational learning
Tanagra
Transfer learning
Variable-order Markov model
Version space learning
Waffles
Weka
Loss function
Loss functions for classification
Mean squared error (MSE)
Mean squared prediction error (MSPE)
Taguchi loss function
Low-energy adaptive clustering hierarchy
Other[edit]
Anne O'Tate
Ant colony optimization algorithms
Anthony Levandowski
Anti-unification (computer science)
Apache Flume
Apache Giraph
Apache Mahout
Apache SINGA
Apache Spark
Apache SystemML
Aphelion (software)
Arabic Speech Corpus
Archetypal analysis
Arthur Zimek
Artificial ants
Artificial bee colony algorithm
Artificial development
Artificial immune system
Astrostatistics
Averaged one-dependence estimators
Bag-of-words model
Balanced clustering
Ball tree
Base rate
Bat algorithm
Baum–Welch algorithm
Bayesian hierarchical modeling
Bayesian interpretation of kernel regularization
Bayesian optimization
Bayesian structural time series
Bees algorithm
Behavioral clustering
Bernoulli scheme
Bias–variance tradeoff
Biclustering
BigML
Binary classification
Bing Predicts
Bio-inspired computing
Biogeography-based optimization
Biplot
Bondy's theorem
Bongard problem
Bradley–Terry model
BrownBoost
Brown clustering
Burst error
CBCL (MIT)
CIML community portal
CMA-ES
CURE data clustering algorithm
Cache language model
Calibration (statistics)
Canonical correspondence analysis
Canopy clustering algorithm
Cascading classifiers
Category utility
CellCognition
Cellular evolutionary algorithm
Chi-square automatic interaction detection
Chromosome (genetic algorithm)
Classifier chains
Cleverbot
Clonal selection algorithm
Cluster-weighted modeling
Clustering high-dimensional data
Clustering illusion
CoBoosting
Cobweb (clustering)
Cognitive computer
Cognitive robotics
Collostructional analysis
Common-method variance
Complete-linkage clustering
Computer-automated design
Concept class
Concept drift
Conference on Artificial General Intelligence
Conference on Knowledge Discovery and Data Mining
Confirmatory factor analysis
Confusion matrix
Congruence coefficient
Connect (computer system)
Consensus clustering
Constrained clustering
Constrained conditional model
Constructive cooperative coevolution
Correlation clustering
Correspondence analysis
Cortica
Coupled pattern learner
Cross-entropy method
Cross-validation (statistics)
Crossover (genetic algorithm)
Cuckoo search
Cultural algorithm
Cultural consensus theory
Curse of dimensionality
DADiSP
DARPA LAGR Program
Darkforest
Dartmouth workshop
DarwinTunes
Data Mining Extensions
Data exploration
Data pre-processing
Data stream clustering
Dataiku
Davies–Bouldin index
Decision boundary
Decision list
Decision tree model
Deductive classifier
DeepArt
DeepDream
Deep Web Technologies
Defining length
Dendrogram
Dependability state model
Detailed balance
Determining the number of clusters in a data set
Detrended correspondence analysis
Developmental robotics
Diffbot
Differential evolution
Discrete phase-type distribution
Discriminative model
Dissociated press
Distributed R
Dlib
Document classification
Documenting Hate
Domain adaptation
Doubly stochastic model
Dual-phase evolution
Dunn index
Dynamic Bayesian network
Dynamic Markov compression
Dynamic topic model
Dynamic unobserved effects model
EDLUT
ELKI
Edge recombination operator
Effective fitness
Elastic map
Elastic matching
Elbow method (clustering)
Emergent (software)
Encog
Entropy rate
Erkki Oja
Eurisko
European Conference on Artificial Intelligence
Evaluation of binary classifiers
Evolution strategy
Evolution window
Evolutionary Algorithm for Landmark Detection
Evolutionary algorithm
Evolutionary art
Evolutionary music
Evolutionary programming
Evolvability (computer science)
Evolved antenna
Evolver (software)
Evolving classification function
Expectation propagation
Exploratory factor analysis
F1 score
FLAME clustering
Factor analysis of mixed data
Factor graph
Factor regression model
Factored language model
Farthest-first traversal
Fast-and-frugal trees
Feature Selection Toolbox
Feature hashing
Feature scaling
Feature vector
Firefly algorithm
First-difference estimator
First-order inductive learner
Fish School Search
Fisher kernel
Fitness approximation
Fitness function
Fitness proportionate selection
Fluentd
Folding@home
Formal concept analysis
Forward algorithm
Fowlkes–Mallows index
Frederick Jelinek
Frrole
Functional principal component analysis
GATTO
GLIMMER
Gary Bryce Fogel
Gaussian adaptation
Gaussian process
Gaussian process emulator
Gene prediction
General Architecture for Text Engineering
Generalization error
Generalized canonical correlation
Generalized filtering
Generalized iterative scaling
Generalized multidimensional scaling
Generative adversarial network
Generative model
Genetic algorithm
Genetic algorithm scheduling
Genetic algorithms in economics
Genetic fuzzy systems
Genetic memory (computer science)
Genetic operator
Genetic programming
Genetic representation
Geographical cluster
Gesture Description Language
Geworkbench
Glossary of artificial intelligence
Glottochronology
Golem (ILP)
Google matrix
Grafting (decision trees)
Gramian matrix
Grammatical evolution
Granular computing
GraphLab
Graph kernel
Gremlin (programming language)
Growth function
HUMANT (HUManoid ANT) algorithm
Hammersley–Clifford theorem
Harmony search
Hebbian theory
Hidden Markov random field
Hidden semi-Markov model
Hierarchical hidden Markov model
Higher-order factor analysis
Highway network
Hinge loss
Holland's schema theorem
Hopkins statistic
Hoshen–Kopelman algorithm
Huber loss
IRCF360
Ian Goodfellow
Ilastik
Ilya Sutskever
Immunocomputing
Imperialist competitive algorithm
Inauthentic text
Incremental decision tree
Induction of regular languages
Inductive bias
Inductive probability
Inductive programming
Influence diagram
Information Harvesting
Information fuzzy networks
Information gain in decision trees
Information gain ratio
Inheritance (genetic algorithm)
Instance selection
Intel RealSense
Interacting particle system
Interactive machine translation
International Joint Conference on Artificial Intelligence
International Meeting on Computational Intelligence Methods for Bioinformatics and Biostatistics
International Semantic Web Conference
Iris flower data set
Island algorithm
Isotropic position
Item response theory
Iterative Viterbi decoding
JOONE
Jabberwacky
Jaccard index
Jackknife variance estimates for random forest
Java Grammatical Evolution
Joseph Nechvatal
Jubatus
Julia (programming language)
Junction tree algorithm
K-SVD
K-means++
K-medians clustering
K-medoids
KNIME
KXEN Inc.
K q-flats
Kaggle
Kalman filter
Katz's back-off model
Keras
Kernel adaptive filter
Kernel density estimation
Kernel eigenvoice
Kernel embedding of distributions
Kernel method
Kernel perceptron
Kernel random forest
Kinect
Klaus-Robert Müller
Kneser–Ney smoothing
Knowledge Vault
Knowledge integration
LIBSVM
LPBoost
Labeled data
LanguageWare
Language Acquisition Device (computer)
Language identification in the limit
Language model
Large margin nearest neighbor
Latent Dirichlet allocation
Latent class model
Latent semantic analysis
Latent variable
Latent variable model
Lattice Miner
Layered hidden Markov model
Learnable function class
Least squares support vector machine
Leave-one-out error
Leslie P. Kaelbling
Linear genetic programming
Linear predictor function
Linear separability
Lingyun Gu
Linkurious
Lior Ron (business executive)
List of genetic algorithm applications
List of metaphor-based metaheuristics
List of text mining software
Local case-control sampling
Local independence
Local tangent space alignment
Locality-sensitive hashing
Log-linear model
Logistic model tree
Low-rank approximation
Low-rank matrix approximations
MATLAB
MIMIC (immunology)
MXNet
Mallet (software project)
Manifold regularization
Margin-infused relaxed algorithm
Margin classifier
Mark V. Shaney
Massive Online Analysis
Matrix regularization
Matthews correlation coefficient
Mean shift
Mean squared error
Mean squared prediction error
Measurement invariance
Medoid
MeeMix
Melomics
Memetic algorithm
Meta-optimization
Mexican International Conference on Artificial Intelligence
Michael Kearns (computer scientist)
MinHash
Mixture model
Mlpy
Models of DNA evolution
Moral graph
Mountain car problem
Movidius
Multi-armed bandit
Multi-label classification
Multi expression programming
Multiclass classification
Multidimensional analysis
Multifactor dimensionality reduction
Multilinear principal component analysis
Multiple correspondence analysis
Multiple discriminant analysis
Multiple factor analysis
Multiple sequence alignment
Multiplicative weight update method
Multispectral pattern recognition
Mutation (genetic algorithm)
MysteryVibe
N-gram
NOMINATE (scaling method)
Native-language identification
Natural Language Toolkit
Natural evolution strategy
Nearest-neighbor chain algorithm
Nearest centroid classifier
Nearest neighbor search
Neighbor joining
Nest Labs
NetMiner
NetOwl
Neural Designer
Neural Engineering Object
Neural Lab
Neural modeling fields
Neural network software
NeuroSolutions
Neuro Laboratory
Neuroevolution
Neuroph
Niki.ai
Noisy channel model
Noisy text analytics
Nonlinear dimensionality reduction
Novelty detection
Nuisance variable
Numenta
One-class classification
Onnx
OpenNLP
Optimal discriminant analysis
Oracle Data Mining
Orange (software)
Ordination (statistics)
Overfitting
PROGOL
PSIPRED
Pachinko allocation
PageRank
Parallel metaheuristic
Parity benchmark
Part-of-speech tagging
Particle swarm optimization
Path dependence
Pattern language (formal languages)
Peltarion Synapse
Perplexity
Persian Speech Corpus
Picas (app)
Pietro Perona
Pipeline Pilot
Piranha (software)
Pitman–Yor process
Plate notation
Polynomial kernel
Pop music automation
Population process
Portable Format for Analytics
Predictive Model Markup Language
Predictive state representation
Preference regression
Premature convergence
Principal geodesic analysis
Prior knowledge for pattern recognition
Prisma (app)
Probabilistic Action Cores
Probabilistic context-free grammar
Probabilistic latent semantic analysis
Probabilistic soft logic
Probability matching
Probit model
Product of experts
Programming with Big Data in R
Proper generalized decomposition
Pruning (decision trees)
Pushpak Bhattacharyya
Q methodology
Qloo
Quality control and genetic algorithms
Quantum Artificial Intelligence Lab
Queueing theory
Quick, Draw!
R (programming language)
Rada Mihalcea
Rademacher complexity
Radial basis function kernel
Rand index
Random indexing
Random projection
Random subspace method
Ranking SVM
RapidMiner
Rattle GUI
Raymond Cattell
Reasoning system
Regularization perspectives on support vector machines
Relational data mining
Relationship square
Relevance vector machine
Relief (feature selection)
Renjin
Repertory grid
Representer theorem
Reward-based selection
Richard Zemel
Right to explanation
RoboEarth
Robust principal component analysis
RuleML Symposium
Rule induction
Rules extraction system family
SAS (software)
SNNS
SPSS Modeler
SUBCLU
Sample complexity
Sample exclusion dimension
Santa Fe Trail problem
Savi Technology
Schema (genetic algorithms)
Search-based software engineering
Selection (genetic algorithm)
Self-Service Semantic Suite
Semantic folding
Semantic mapping (statistics)
Semidefinite embedding
Sense Networks
Sensorium Project
Sequence labeling
Sequential minimal optimization
Shattered set
Shogun (toolbox)
Silhouette (clustering)
SimHash
SimRank
Similarity measure
Simple matching coefficient
Simultaneous localization and mapping
Sinkov statistic
Sliced inverse regression
SmartMatch
Snakes and Ladders
Soft independent modelling of class analogies
Soft output Viterbi algorithm
Solomonoff's theory of inductive inference
SolveIT Software
Spectral clustering
Spike-and-slab variable selection
Statistical machine translation
Statistical parsing
Statistical semantics
Stefano Soatto
Stephen Wolfram
Stochastic block model
Stochastic cellular automaton
Stochastic diffusion search
Stochastic grammar
Stochastic matrix
Stochastic universal sampling
Stress majorization
String kernel
Structural equation modeling
Structural risk minimization
Structured sparsity regularization
Structured support vector machine
Subclass reachability
Sufficient dimension reduction
Sukhotin's algorithm
Sum of absolute differences
Sum of absolute transformed differences
Swarm intelligence
Switching Kalman filter
Symbolic regression
Synchronous context-free grammar
Syntactic pattern recognition
TD-Gammon
TIMIT
Teaching dimension
Teuvo Kohonen
Textual case-based reasoning
Theory of conjoint measurement
Thomas G. Dietterich
Thurstonian model
Topic model
Tournament selection
Training, test, and validation sets
Transiogram
Trax Image Recognition
Trigram tagger
Truncation selection
Tucker decomposition
UIMA
UPGMA
Ugly duckling theorem
Uncertain data
Uniform convergence in probability
Unique negative dimension
Universal portfolio algorithm
User behavior analytics
VC dimension
VIGRA
Validation set
Vapnik–Chervonenkis theory
Variable-order Bayesian network
Variable kernel density estimation
Variable rules analysis
Variational message passing
Varimax rotation
Vector quantization
Vicarious (company)
Viterbi algorithm
Vowpal Wabbit
WACA clustering algorithm
WPGMA
Ward's method
Weasel program
Whitening transformation
Winnow (algorithm)
Win–stay, lose–switch
Witness set
Wolfram Language
Wolfram Mathematica
Writer invariant
Xgboost
Yooreeka
Zeroth (software)
Further reading[edit]
Trevor Hastie, Robert Tibshirani and Jerome H. Friedman (2001). The Elements of Statistical Learning, Springer. .mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}ISBN 0-387-95284-5.
Pedro Domingos (September 2015), The Master Algorithm, Basic Books, ISBN 978-0-465-06570-7
Mehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar (2012). Foundations of Machine Learning, The MIT Press. ISBN 978-0-262-01825-8.
Ian H. Witten and Eibe Frank (2011). Data Mining: Practical machine learning tools and techniques Morgan Kaufmann, 664pp., ISBN 978-0-12-374856-0.
David J. C. MacKay. Information Theory, Inference, and Learning Algorithms Cambridge: Cambridge University Press, 2003. ISBN 0-521-64298-1
Richard O. Duda, Peter E. Hart, David G. Stork (2001) Pattern classification (2nd edition), Wiley, New York, ISBN 0-471-05669-3.
Christopher Bishop (1995). Neural Networks for Pattern Recognition, Oxford University Press. ISBN 0-19-853864-2.
Vladimir Vapnik (1998). Statistical Learning Theory. Wiley-Interscience, ISBN 0-471-03003-1.
Ray Solomonoff, An Inductive Inference Machine, IRE Convention Record, Section on Information Theory, Part 2, pp., 56-62, 1957.
Ray Solomonoff, "An Inductive Inference Machine" A privately circulated report from the 1956 Dartmouth Summer Research Conference on AI.
References[edit]


^ http://www.britannica.com/EBchecked/topic/1116194/machine-learning  This tertiary source  reuses information from other sources but does not name them.

^ Phil Simon (March 18, 2013). Too Big to Ignore: The Business Case for Big Data. Wiley. p. 89. ISBN 978-1-118-63817-0.

^ Ron Kohavi; Foster Provost (1998). "Glossary of terms". Machine Learning. 30: 271–274.

^ http://www.learningtheory.org/

^ Settles, Burr (2010), "Active Learning Literature Survey" (PDF), Computer Sciences Technical Report 1648. University of Wisconsin–Madison, retrieved 2014-11-18

^ Rubens, Neil; Elahi, Mehdi; Sugiyama, Masashi; Kaplan, Dain (2016). "Active Learning in Recommender Systems".  In Ricci, Francesco; Rokach, Lior; Shapira, Bracha (eds.). Recommender Systems Handbook (2 ed.). Springer US. doi:10.1007/978-1-4899-7637-6. ISBN 978-1-4899-7637-6.

^ https://en.wikipedia.org/wiki/Generative_adversarial_network#cite_note-GANs-1


External links[edit]

Machine learningat Wikipedia's sister projects


Definitions from Wiktionary
Media from Wikimedia Commons
News from Wikinews
Quotations from Wikiquote
Texts from Wikisource
Textbooks from Wikibooks
Resources from Wikiversity



Data Science: Data to Insights from MIT (machine learning)
Popular online course by Andrew Ng, at Coursera. It uses GNU Octave. The course is a free version of Stanford University's actual course taught by Ng, see.stanford.edu/Course/CS229 available for free].
mloss is an academic database of open-source machine learning software.
vteWikipedia OutlinesGeneral reference
Culture and the arts
Geography and places
Health and fitness
History and events
Mathematics and logic
Natural and physical sciences
People and self
Philosophy and thinking
Religion and belief systems
Society and social sciences
Technology and applied sciences



Deep Learning StudioDeveloper(s)Deep Cognition Inc.Written inPythonOperating systemMicrosoft Windows, Ubuntu LinuxTypeDeep learningLicenseProprietary softwareWebsitewww.deepcognition.ai
Deep Learning Studio is a software tool that aims to simplify the creation of deep learning models used in artificial intelligence.[1] It is compatible with a number of open-source programming frameworks popularly used in artificial neural networks, including MXNet and Google's TensorFlow.[1]
Prior to the release of Deep Learning Studio in January 2017, proficiency in Python, among other programming languages, was essential in developing effective deep learning models.[1] Deep Learning Studio sought to simplify the model creation process through a visual, drag-and-drop interface and the application of pre-trained learning models on available data.[1]
Irving, TX-based Deep Cognition Inc. is the developer behind Deep Learning Studio. In 2017, the software allowed Deep Cognition to become a finalist for Best Innovation in Deep Learning in the Alconics Awards, which are given annually to the best artificial intelligence software.[2]
Deep Cognition launched version 2.0 of Deep Learning Studio at NVIDIA's GTC 2018 Conference in San Jose, CA.[3]
Fremont, CA-based computing products supplier Exxact Corp provides desktop computers specifically built to handle Deep Learning Studio workloads.[4]

Contents

1 Features[1]
2 See also
3 References
4 External links


Features[1][edit]
Deep Learning Studio is available in two versions: Desktop and Cloud, both of which are free software. The Desktop version is available on Windows and Ubuntu. The Cloud version is available in single-user and multi-user configurations.[5] A Deep Cognition account is needed to access the Cloud version. Account registration is free.
Deep Learning Studio can import existing Keras models; it also takes a data set as an input.
Deep Learning Studio's AutoML feature allows automatic generation of deep learning models. More advanced users may choose to generate their own models using various types of layers and neural networks.
Deep Learning Studio also has a library of loss functions and optimizers for use in hyperparameter tuning, a traditionally complicated area in neural network programming.
Generated models can be trained using either CPUs or GPUs. Trained models can then be used for predictive analytics.
Deep Learning Studio has been cited for being a user-friendly deep learning tool.[6]

See also[edit]
Artificial intelligence
Artificial neural network
Data mining
Deep learning
Machine learning
Predictive analytics
References[edit]


^ a b c d e "Deep Learning Made Easy with Deep Cognition". www.kdnuggets.com. Retrieved 2018-03-08..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ Innovates, Dallas (2017-09-25). "Deep Cognition Among Finalists for Alconics Award » Dallas Innovates". Dallas Innovates. Retrieved 2018-03-08.

^ "AI Democratization: IBM, Deep Cognition, and Cloudera - Wikibon Research". wikibon.com. Retrieved 2018-03-28.

^ "Deep Learning Studio Solutions | Exxact". www.exxactcorp.com. Retrieved 2018-03-15.

^ "AWS Marketplace: Deep Cognition". aws.amazon.com. Retrieved 2018-03-15.

^ "Making Deep Learning User-Friendly, Possible? – Towards Data Science". Towards Data Science. 2018-04-04. Retrieved 2018-04-11.


External links[edit]
Official website
Official blog


This artificial intelligence-related article is a stub. You can help Wikipedia by expanding it.vte



For other uses, see CNN (disambiguation).
This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources: "Convolutional neural network" – news · newspapers · books · scholar · JSTOR (June 2019) (Learn how and when to remove this template message)
Machine learning anddata mining
Problems
Classification
Clustering
Regression
Anomaly detection
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction


Supervised learning.mw-parser-output .nobold{font-weight:normal}(classification • regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)


Clustering
BIRCH
CURE
Hierarchical
k-means
Expectation–maximization (EM)
DBSCAN
OPTICS
Mean-shift


Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
t-SNE


Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov


Anomaly detection
k-NN
Local outlier factor


Artificial neural networks
Autoencoder
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net


Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)


Theory
Bias–variance dilemma
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory


Machine-learning venues
NIPS
ICML
ML
JMLR
ArXiv:cs.LG


Glossary of artificial intelligence
Glossary of artificial intelligence


Related articles
List of datasets for machine-learning research
Outline of machine learning


 Machine learning portalvte
In deep learning,  a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery.
CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually refer to fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The "fully-connectedness" of these networks make them prone to overfitting data. Typical ways of regularization includes adding some form of magnitude measurement of weights to the loss function. However, CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns. Therefore, on the scale of connectedness and complexity, CNNs are on the lower extreme.
They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics.[1][2]
Convolutional networks were inspired by biological processes[3][4][5][6] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.
CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns the filters that in traditional algorithms were hand-engineered. This independence from prior knowledge and human effort in feature design is a major advantage.

They have applications in image and video recognition, recommender systems,[7] image classification, medical image analysis, and natural language processing.[8].mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}Contents

1 Design

1.1 Convolutional
1.2 Pooling
1.3 Fully connected
1.4 Receptive field
1.5 Weights


2 History

2.1 Receptive fields in the visual cortex
2.2 Neocognitron, origin of the CNN architecture
2.3 Time delay neural networks
2.4 Image recognition with CNNs trained by gradient descent

2.4.1 LeNet-5


2.5 Shift-invariant neural network
2.6 Neural abstraction pyramid
2.7 GPU implementations


3 Distinguishing features
4 Building blocks

4.1 Convolutional layer

4.1.1 Local connectivity
4.1.2 Spatial arrangement
4.1.3 Parameter sharing


4.2 Pooling layer
4.3 ReLU layer
4.4 Fully connected layer
4.5 Loss layer


5 Choosing hyperparameters

5.1 Number of filters
5.2 Filter shape
5.3 Max pooling shape


6 Regularization methods

6.1 Empirical

6.1.1 Dropout
6.1.2 DropConnect
6.1.3 Stochastic pooling
6.1.4 Artificial data


6.2 Explicit

6.2.1 Early stopping
6.2.2 Number of parameters
6.2.3 Weight decay
6.2.4 Max norm constraints




7 Hierarchical coordinate frames
8 Applications

8.1 Image recognition
8.2 Video analysis
8.3 Natural language processing
8.4 Drug discovery
8.5 Health risk assessment and biomarkers of aging discovery
8.6 Checkers game
8.7 Go


9 Fine-tuning
10 Human interpretable explanations
11 Related Architectures

11.1 Deep Q-networks
11.2 Deep belief networks


12 Notable libraries
13 Notable APIs
14 See also
15 Notes
16 References
17 External links



Design[edit]
A convolutional neural network consists of an input and an output layer, as well as multiple hidden layers. The hidden layers of a CNN typically consist of a series of convolutional layers that convolve with a multiplication or other dot product. The activation function is commonly a RELU layer, and is subsequently followed by additional convolutions such as pooling layers, fully connected layers and normalization layers, referred to as hidden layers because their inputs and outputs are masked by the activation function and final convolution. The final convolution, in turn, often involves backpropagation in order to more accurately weight the end product.[9]
Though the layers are colloquially referred to as convolutions, this is only by convention. Mathematically, it is technically a sliding dot product or cross-correlation. This has significance for the indices in the matrix, in that it affects how weight is determined at a specific index point.[citation needed]

Convolutional[edit]
When programming a CNN, each convolutional layer within a neural network should have the following attributes:

Input is a tensor with shape (number of images) x (image width) x (image height) x (image depth).
Convolutional kernels whose width and height are hyper-parameters, and whose depth must be equal to that of the image. Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus.[10]
Each convolutional neuron processes data only for its receptive field. Although fully connected feedforward neural networks can be used to learn features as well as classify data, it is not practical to apply this architecture to images. A very high number of neurons would be necessary, even in a shallow (opposite of deep) architecture, due to the very large input sizes associated with images, where each pixel is a relevant variable. For instance, a fully connected layer for a (small) image of size 100 x 100 has 10,000 weights for each neuron in the second layer. The convolution operation brings a solution to this problem as it reduces the number of free parameters, allowing the network to be deeper with fewer parameters.[11]  For instance, regardless of image size, tiling regions of size 5 x 5, each with the same shared weights, requires only 25 learnable parameters. In this way, it resolves the vanishing or exploding gradients problem in training traditional multi-layer neural networks with many layers by using backpropagation.[citation needed]

Pooling[edit]
Convolutional networks may include local or global pooling layers to streamline the underlying computation. Pooling layers reduce the dimensions of the data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, typically 2 x 2.  Global pooling acts on all the neurons of the convolutional layer.[12][13]  In addition, pooling may compute a max or an average. Max pooling uses the maximum value from each of a cluster of neurons at the prior layer.[14] Average pooling uses the average value from each of a cluster of neurons at the prior layer.[15]

Fully connected[edit]
Fully connected layers connect every neuron in one layer to every neuron in another layer. It is in principle the same as the traditional multi-layer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.

Receptive field[edit]
In neural networks, each neuron receives input from some number of locations in the previous layer. In a fully connected layer, each neuron receives input from every element of the previous layer. In a convolutional layer, neurons receive input from only a restricted subarea of the previous layer. Typically the subarea is of a square shape (e.g., size 5 by 5). The input area of a neuron is called its receptive field.  So, in a fully connected layer, the receptive field is the entire previous layer. In a convolutional layer, the receptive area is smaller than the entire previous layer.

Weights[edit]
Each neuron in a neural network computes an output value by applying a specific function to the input values coming from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning, in a neural network, progresses by making iterative adjustments to these biases and weights. 
The vector of weights and the bias are called filters and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces memory footprint because a single bias and a single vector of weights is used across all receptive fields sharing that filter, as opposed to each receptive field having its own bias and vector weighting.[16]

History[edit]
CNN design follows vision processing in living organisms.[citation needed]

Receptive fields in the visual cortex[edit]
Work by Hubel and Wiesel in the 1950s and 1960s showed that cat and monkey visual cortexes contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field.[citation needed] Neighboring cells have similar and overlapping receptive fields.[citation needed] Receptive field size and location varies systematically across the cortex to form a complete map of visual space.[citation needed] The cortex in each hemisphere represents the contralateral visual field.[citation needed]
Their 1968 paper identified two basic visual cell types in the brain:[4]

simple cells, whose output is maximized by straight edges having particular orientations within their receptive field
complex cells, which have larger receptive fields, whose output is insensitive to the exact position of the edges in the field.
Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.[17][18]

Neocognitron, origin of the CNN architecture[edit]
The "neocognitron"[3] was introduced by Kunihiko Fukushima in 1980.[5][14][19]
It was inspired by the above-mentioned work of Hubel and Wiesel. The neocognitron introduced the two basic types of layers in CNNs: convolutional layers, and downsampling layers. A convolutional layer contains units whose receptive fields cover a patch of the previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted.
In a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging, J. Weng et al. introduced a method called max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch.[20] Max-pooling is often used in modern CNNs.[21]
Several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron.[3] Today, however, the CNN architecture is usually trained through backpropagation.
The neocognitron is the first CNN which requires units located at multiple network positions to have shared weights. Neocognitrons were adapted in 1988 to analyze time-varying signals.[22]

Time delay neural networks[edit]
The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. and was the first convolutional network, as it achieved shift invariance.[23] It did so by utilizing weight sharing in combination with back propagation training.[24] Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights, instead of a local one.[23]
TDNNs are convolutional networks that share weights along the temporal dimension.[25] They allow speech signals to be processed time-invariantly. This inspired translation invariance in image processing with CNNs.[24]  The tiling of neuron outputs can cover timed stages.[26]
TDNNs now achieve the best performance in far distance speech recognition.[27]

Image recognition with CNNs trained by gradient descent[edit]
A system to recognize hand-written ZIP Code numbers[28] involved convolutions in which the kernel coefficients had been laboriously hand designed.[29]
Yann LeCun et al. (1989)[29] used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types.
This approach became a foundation of modern computer vision.

LeNet-5[edit]
LeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1998,[30] that classifies digits, was applied by several banks to recognize hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources.

Shift-invariant neural network[edit]
Similarly, a shift invariant neural network was proposed by W. Zhang et al. for image character recognition in 1988.[1][2] The architecture and training algorithm were modified in 1991[31] and applied for medical image processing[32] and automatic detection of breast cancer in mammograms.[33]
A different convolution-based design was proposed in 1988[34] for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.[35][36]

Neural abstraction pyramid[edit]
The feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid[37] by lateral and feedback connections.[further explanation needed] The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated.

GPU implementations[edit]
Although CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on Graphics Processing Units or GPUs.
In 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU.[38][21] In 2005, another paper also emphasised the value of GPGPU for machine learning.[39]
The first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU.[40] Subsequent work also used GPUs, initially for other types of neural networks (different from CNNs), especially unsupervised neural networks.[41][42][43][44]
In 2010, Dan Ciresan et al. at IDSIA showed that even deep standard neural networks with many layers can be quickly trained on GPU by supervised learning through the old method known as backpropagation. Their network outperformed previous machine learning methods on the MNIST handwritten digits benchmark.[45] In 2011, they extended this GPU approach to CNNs, achieving an acceleration factor of 60, with impressive results.[12] In 2011, they used such CNNs on GPU to win an image recognition contest where they achieved superhuman performance for the first time.[46] Between May 15, 2011 and September 30, 2012, their CNNs won no less than four image competitions.[47][21] In 2012, they also significantly improved on the best performance in the literature for multiple image databases, including the MNIST database, the NORB database, the HWDB1.0 dataset (Chinese characters) and the CIFAR10 dataset (dataset of 60000 32x32 labeled RGB images).[14]
Subsequently, a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012.[48] A very deep CNN with over 100 layers by Microsoft won the ImageNet 2015 contest.[49]

Distinguishing features[edit]
In the past, traditional multilayer perceptron (MLP) models have been used for image recognition.[example  needed] However, due to the full connectivity between nodes, they suffered from the curse of dimensionality, and did not scale well with higher resolution images. A 1000×1000-pixel image with RGB color channels has 3 million weights, which is too high to feasibly process efficiently at scale with full connectivity.

 CNN layers arranged in 3 dimensions
For example, in CIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in a first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.
Also, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in image data, both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.
Convolutional neural networks are biologically inspired variants of multilayer perceptrons that are designed to emulate the behavior of a visual cortex.[citation needed] These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:

3D volumes of neurons. The layers of a CNN have neurons arranged in 3 dimensions: width, height and depth.[citation needed] The neurons inside a layer are connected to only a small region of the layer before it, called a receptive field. Distinct types of layers, both locally and completely connected, are stacked to form a CNN architecture.
Local connectivity: following the concept of receptive fields, CNNs exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent layers. The architecture thus ensures that the learned "filters" produce the strongest response to a spatially local input pattern. Stacking many such layers leads to non-linear filters that become increasingly global (i.e. responsive to a larger region of pixel space) so that the network first creates representations of small parts of the input, then from them assembles representations of larger areas.
Shared weights: In CNNs, each filter is replicated across the entire visual field. These replicated units share the same parameterization (weight vector and bias) and form a feature map. This means that all the neurons in a given convolutional layer respond to the same feature within their specific response field. Replicating units in this way allows for features to be detected regardless of their position in the visual field, thus constituting a property of translation invariance.
Together, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.

Building blocks[edit]
This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources: "Convolutional neural network" – news · newspapers · books · scholar · JSTOR (June 2017) (Learn how and when to remove this template message)
A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below. Neurons of a convolutional layer (blue), connected to their receptive field (red)
Convolutional layer[edit]
The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the entries of the filter and the input and producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.[nb 1]
Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input and shares parameters with neurons in the same activation map.

Local connectivity[edit]
 Typical CNN architecture
When dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.
The extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.

Spatial arrangement[edit]
Three hyperparameters control the size of the output volume of the convolutional layer: the depth, stride and zero-padding.

The depth of the output volume controls the number of neurons in a layer that connect to the same region of the input volume. These neurons learn to activate for different features in the input. For example, if the first convolutional layer takes the raw image as input, then different neurons along the depth dimension may activate in the presence of various oriented edges, or blobs of color.
Stride controls how depth columns around the spatial dimensions (width and height) are allocated. When the stride is 1 then we move the filters one pixel at a time. This leads to heavily overlapping receptive fields between the columns, and also to large output volumes. When the stride is 2 then the filters jump 2 pixels at a time as they slide around. Similarly, for any integer 



S
>
0
,


{\textstyle S>0,}

 a stride of S causes the filter to be translated by S units at a time per output. In practice, stride lengths of 



S
≥
3


{\textstyle S\geq 3}

 are rare. The receptive fields overlap less and the resulting output volume has smaller spatial dimensions when stride length is increased.[50]
Sometimes it is convenient to pad the input with zeros on the border of the input volume. The size of this padding is a third hyperparameter. Padding provides control of the output volume spatial size. In particular, sometimes it is desirable to exactly preserve the spatial size of the input volume.
The spatial size of the output volume can be computed as a function of the input volume size 



W


{\displaystyle W}

, the kernel field size of the convolutional layer neurons 



K


{\displaystyle K}

, the stride with which they are applied 



S


{\displaystyle S}

, and the amount of zero padding 



P


{\displaystyle P}

 used on the border. The formula for calculating how many neurons "fit" in a given volume is given by







W
−
K
+
2
P

S


+
1.


{\displaystyle {\frac {W-K+2P}{S}}+1.}


If this number is not an integer, then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general, setting zero padding to be 



P
=
(
K
−
1
)

/

2


{\textstyle P=(K-1)/2}

 when the stride is 



S
=
1


{\displaystyle S=1}

 ensures that the input volume and output volume will have the same size spatially. However, it's not always completely necessary to use all of the neurons of the previous layer. For example, a neural network designer may decide to use just a portion of padding.

Parameter sharing[edit]
A parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on one reasonable assumption: if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. In other words, denoting a single 2-dimensional slice of depth as a depth slice, we constrain the neurons in each depth slice to use the same weights and bias.
Since all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as a convolution of the neuron's weights with the input volume.[nb 2] Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.
Sometimes, the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a "locally connected layer".

Pooling layer[edit]
 Max pooling with a 2x2 filter and stride = 2
Another important concept of CNNs is pooling, which is a form of non-linear down-sampling. There are several non-linear functions to implement pooling among which max pooling is the most common. It partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum.
Intuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting. It is common to periodically insert a pooling layer between successive convolutional layers in a CNN architecture.[citation needed] The pooling operation provides another form of translation invariance.
The pooling layer operates independently on every depth slice of the input and resizes it spatially. The most common form is a pooling layer with filters of size 2×2 applied with a stride of 2 downsamples at every depth slice in the input by 2 along both width and height, discarding 75% of the activations. In this case, every max operation is over 4 numbers. The depth dimension remains unchanged.
In addition to max pooling, pooling units can use other functions, such as average pooling or ℓ2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which performs better in practice.[51]
Due to the aggressive reduction in the size of the representation,[which?] there is a recent trend towards using smaller filters[52] or discarding pooling layers altogether.[53]

 RoI pooling to size 2x2. In this example region proposal (an input parameter) has size 7x5.
"Region of Interest" pooling (also known as RoI pooling) is a variant of max pooling, in which output size is fixed and input rectangle is a parameter.[54]
Pooling is an important component of convolutional neural networks for object detection based on Fast R-CNN[55] architecture.

ReLU layer[edit]
ReLU is the abbreviation of rectified linear unit, which applies the non-saturating activation function 



f
(
x
)
=
max
(
0
,
x
)


{\textstyle f(x)=\max(0,x)}

.[48] It effectively removes negative values from an activation map by setting them to zero.[56] It increases the nonlinear properties of the decision function and of the overall network without affecting the receptive fields of the convolution layer.
Other functions are also used to increase nonlinearity, for example the saturating hyperbolic tangent 



f
(
x
)
=
tanh
⁡
(
x
)


{\displaystyle f(x)=\tanh(x)}

, 



f
(
x
)
=

|

tanh
⁡
(
x
)

|



{\displaystyle f(x)=|\tanh(x)|}

, and the sigmoid function 



σ
(
x
)
=
(
1
+

e

−
x



)

−
1




{\textstyle \sigma (x)=(1+e^{-x})^{-1}}

. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy.[57]

Fully connected layer[edit]
Finally, after several convolutional and max pooling layers, the high-level reasoning in the neural network is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation, with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term).

Loss layer[edit]
Main articles: Loss function and Loss functions for classification
The "loss layer" specifies how training penalizes the deviation between the predicted (output) and true labels and is normally the final layer of a neural network. Various loss functions appropriate for different tasks may be used.
Softmax loss is used for predicting a single class of K mutually exclusive classes.[nb 3] Sigmoid cross-entropy loss is used for predicting K independent probability values in 



[
0
,
1
]


{\displaystyle [0,1]}

. Euclidean loss is used for regressing to real-valued labels 



(
−
∞
,
∞
)


{\displaystyle (-\infty ,\infty )}

.

Choosing hyperparameters[edit]
This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources: "Convolutional neural network" – news · newspapers · books · scholar · JSTOR (June 2017) (Learn how and when to remove this template message)
CNNs use more hyperparameters than a standard multilayer perceptron (MLP). While the usual rules for learning rates and regularization constants still apply, the following should be kept in mind when optimizing.

Number of filters[edit]
Since feature map size decreases with depth, layers near the input layer will tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature values va with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.
The number of feature maps directly controls the capacity and depends on the number of available examples and task complexity.

Filter shape[edit]
Common filter shapes found in the literature vary greatly, and are usually chosen based on the dataset.
The challenge is, thus, to find the right level of granularity so as to create abstractions at the proper scale, given a particular dataset, and without overfitting.

Max pooling shape[edit]
Typical values are 2×2. Very large input volumes may warrant 4×4 pooling in the lower layers.[58] However, choosing larger shapes will dramatically reduce the dimension of the signal, and may result in excess information loss. Often, non-overlapping pooling windows perform best.[51]

Regularization methods[edit]
Main article: Regularization (mathematics)This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources: "Convolutional neural network" – news · newspapers · books · scholar · JSTOR (June 2017) (Learn how and when to remove this template message)
Regularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.

Empirical[edit]
Dropout[edit]
Because a fully connected layer occupies most of the parameters, it is prone to overfitting. One method to reduce overfitting is dropout.[59][60] At each training stage, individual nodes are either "dropped out" of the net with probability 



1
−
p


{\displaystyle 1-p}

 or kept with probability 



p


{\displaystyle p}

, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed.  Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.
In the training stages, the probability that a hidden node will be dropped is usually 0.5; for input nodes, this should be much lower, intuitively because information is directly lost when input nodes are ignored.
At testing time after training has finished, we would ideally like to find a sample average of all possible 




2

n




{\displaystyle 2^{n}}

 dropped-out networks; unfortunately this is unfeasible for large values of 



n


{\displaystyle n}

.  However, we can find an approximation by using the full network with each node's output weighted by a factor of 



p


{\displaystyle p}

, so the expected value of the output of any node is the same as in the training stages.  This is the biggest contribution of the dropout method: although it effectively generates 




2

n




{\displaystyle 2^{n}}

 neural nets, and as such allows for model combination, at test time only a single network needs to be tested.
By avoiding training all nodes on all training data, dropout decreases overfitting.  The method also significantly improves training speed.  This makes the model combination practical, even for deep neural networks. The technique seems to reduce node interactions, leading them to learn more robust features[clarification needed] that better generalize to new data.

DropConnect[edit]
DropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability 



1
−
p


{\displaystyle 1-p}

. Each unit thus receives input from a random subset of units in the previous layer.[61]
DropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.

Stochastic pooling[edit]
A major drawback to Dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.
In stochastic pooling,[62] the conventional deterministic pooling operations are replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches, such as dropout and data augmentation.
An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images,[63] which delivers excellent performance on the MNIST data set.[63]  Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.

Artificial data[edit]
Since the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Since these networks are usually trained with all available data, one approach is to either generate new data from scratch (if possible) or perturb existing data to create new ones.  For example, input images could be asymmetrically cropped by a few percent to create new examples with the same label as the original.[64]

Explicit[edit]
Early stopping[edit]
Main article: Early stopping
One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur.  It comes with the disadvantage that the learning process is halted.

Number of parameters[edit]
Another simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a "zero norm".

Weight decay[edit]
A simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant, thus increasing the penalty for large weight vectors.
L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.
L1 regularization is another common form. It is possible to combine L1 with L2 regularization (this is called Elastic net regularization). The L1 regularization leads the weight vectors to become sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs.

Max norm constraints[edit]
Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector 






w
→





{\displaystyle {\vec {w}}}

 of every neuron to satisfy 



‖



w
→




‖

2


<
c


{\displaystyle \|{\vec {w}}\|_{2}<c}

. Typical values of 



c


{\displaystyle c}

 are order of 3–4. Some papers report improvements[65] when using this form of regularization.

Hierarchical coordinate frames[edit]
Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.[66]
Currently, the common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and to use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.[67]
Thus, one way of representing something is to embed the coordinate frame within it. Once this is done, large features can be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). Using this approach ensures that the higher level entity (e.g. face) is present when the lower level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose ("pose vectors") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.[68]

Applications[edit]
Image recognition[edit]
CNNs are often used in image recognition systems. In 2012 an error rate of 0.23 percent on the MNIST database was reported.[14] Another paper on using CNN for image classification reported that the learning process was "surprisingly fast"; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database.[12] Subsequently, a similar CNN called 
AlexNet[69] won the ImageNet Large Scale Visual Recognition Challenge 2012.
When applied to facial recognition, CNNs achieved a large decrease in error rate.[70] Another paper reported a 97.6 percent recognition rate on "5,600 still images of more than 10 subjects".[6] CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error.[26]
The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014,[71] a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet[72] (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans.[73] The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.[citation needed]
In 2015 a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.[74]

Video analysis[edit]
Compared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space.[75][76] Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream.[77][78][79] Long short-term memory (LSTM) recurrent units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies.[80][81] Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines[82] and Independent Subspace Analysis.[83]

Natural language processing[edit]
CNNs have also been explored for natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing,[84] search query retrieval,[85] sentence modeling,[86] classification,[87] prediction[88] and other traditional NLP tasks.[89]

Drug discovery[edit]
CNNs have been used in drug discovery.  Predicting the interaction between molecules and biological proteins can identify potential treatments.  In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based rational drug design.[90]  The system trains directly on 3-dimensional representations of chemical interactions.  Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures,[91] AtomNet discovers chemical features, such as aromaticity, sp3 carbons and hydrogen bonding.  Subsequently, AtomNet was used to predict novel candidate biomolecules for multiple disease targets, most notably treatments for the Ebola virus[92] and multiple sclerosis.[93]

Health risk assessment and biomarkers of aging discovery[edit]
CNNs can be naturally tailored to analyze a sufficiently large collection of time series data representing one-week-long human physical activity streams augmented by the rich clinical data (including the death register, as provided by, e.g., the NHANES study).  A simple CNN was combined with Cox-Gompertz proportional hazards model and used to produce a proof-of-concept example of digital biomarkers of aging in the form of all-causes-mortality predictor.[94]

Checkers game[edit]
CNNs have been used in the game of checkers. From 1999 to 2001, Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checkers using co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the difference in number of pieces between the two sides.  Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%.[95][96] It also earned a win against the program Chinook at its "expert" level of play.[97]

Go[edit]
CNNs have been used in computer Go. In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play.[98] Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move.[99]
A couple of CNNs for choosing moves to try ("policy network") and evaluating positions ("value network") driving MCTS were used by AlphaGo, the first to beat the best human player at the time.[100]

Fine-tuning[edit]
For many applications, little training data is available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights. This allows convolutional networks to be successfully applied to problems with small training sets.[101]

Human interpretable explanations[edit]
End-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars.[102] With recent advances in visual salience, spatial and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.[103][104]

Related Architectures[edit]
Deep Q-networks[edit]
A deep Q-network (DQN) is a type of deep learning model that combines a deep CNN with Q-learning, a form of reinforcement learning. Unlike earlier reinforcement learning agents, DQNs can learn directly from high-dimensional sensory inputs.[citation needed]
Preliminary results were presented in 2014, with an accompanying paper in February 2015.[105] The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it.[106]

Deep belief networks[edit]
Main article: Deep belief network
Convolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like deep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR[107] have been obtained using CDBNs.[108]

Notable libraries[edit]
Caffe: A library for convolutional neural networks. Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in C++, and has Python and MATLAB wrappers.
Deeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark. A general-purpose deep learning library for the JVM production stack running on a C++ scientific computing engine. Allows the creation of custom layers. Integrates with Hadoop and Kafka.
Dlib: A toolkit for making real world machine learning and data analysis applications in C++.
Microsoft Cognitive Toolkit: A deep learning toolkit written by Microsoft with several unique features enhancing scalability over multiple nodes. It supports full-fledged interfaces for training in C++ and Python and with additional support for model inference in C# and Java.
TensorFlow: Apache 2.0-licensed Theano-like library with support for CPU, GPU, Google's proprietary tensor processing unit (TPU),[109] and mobile devices.
Theano: The reference deep-learning library for Python with an API largely compatible with the popular NumPy library. Allows user to write symbolic mathematical expressions, then automatically generates their derivatives, saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast, on-the-GPU implementation.
Torch: A scientific computing framework with wide support for machine learning algorithms, written in C and Lua. The main author is Ronan Collobert, and it is now used at Facebook AI Research and Twitter.
Notable APIs[edit]
Keras: A high level API written in Python for TensorFlow and Theano convolutional neural networks.[110]
See also[edit]
Convolution
Deep learning
Natural-language processing
Neocognitron
Scale-invariant feature transform
Time delay neural network
Vision processing unit
Notes[edit]


^ When applied to other types of data than image data, such as sound data, "spatial position" may variously correspond to different points in the time domain, frequency domain or other mathematical spaces.

^ hence the name "convolutional layer"

^ So-called categorical data.


References[edit]


^ a b Zhang, Wei (1988). "Shift-invariant pattern recognition neural network and its optical architecture". Proceedings of Annual Conference of the Japan Society of Applied Physics..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ a b Zhang, Wei (1990). "Parallel distributed processing model with local space-invariant interconnections and its optical architecture". Applied Optics. 29 (32): 4790–7. Bibcode:1990ApOpt..29.4790Z. doi:10.1364/AO.29.004790. PMID 20577468.

^ a b c Fukushima, K. (2007). "Neocognitron". Scholarpedia. 2 (1): 1717. doi:10.4249/scholarpedia.1717.

^ a b Hubel, D. H.; Wiesel, T. N. (1968-03-01). "Receptive fields and functional architecture of monkey striate cortex". The Journal of Physiology. 195 (1): 215–243. doi:10.1113/jphysiol.1968.sp008455. ISSN 0022-3751. PMC 1557912. PMID 4966457.

^ a b Fukushima, Kunihiko (1980). "Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position" (PDF). Biological Cybernetics. 36 (4): 193–202. doi:10.1007/BF00344251. PMID 7370364. Retrieved 16 November 2013.

^ a b Matusugu, Masakazu; Katsuhiko Mori; Yusuke Mitari; Yuji Kaneda (2003). "Subject independent facial expression recognition with robust face detection using a convolutional neural network" (PDF). Neural Networks. 16 (5): 555–559. doi:10.1016/S0893-6080(03)00115-1. PMID 12850007. Retrieved 17 November 2013.

^ van den Oord, Aaron; Dieleman, Sander; Schrauwen, Benjamin (2013-01-01).  Burges, C. J. C.; Bottou, L.; Welling, M.; Ghahramani, Z.; Weinberger, K. Q. (eds.). Deep content-based music recommendation (PDF). Curran Associates, Inc. pp. 2643–2651.

^ Collobert, Ronan; Weston, Jason (2008-01-01). A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. Proceedings of the 25th International Conference on Machine Learning. ICML '08. New York, NY, USA: ACM. pp. 160–167. doi:10.1145/1390156.1390177. ISBN 978-1-60558-205-4.

^ "CS231n Convolutional Neural Networks for Visual Recognition". cs231n.github.io. Retrieved 2018-12-13.

^ "Convolutional Neural Networks (LeNet) – DeepLearning 0.1 documentation". DeepLearning 0.1. LISA Lab. Retrieved 31 August 2013.

^ Habibi, Aghdam, Hamed (2017-05-30). Guide to convolutional neural networks : a practical application to traffic-sign detection and classification. Heravi, Elnaz Jahani. Cham, Switzerland. ISBN 9783319575490. OCLC 987790957.

^ a b c Ciresan, Dan; Ueli Meier; Jonathan Masci; Luca M. Gambardella; Jurgen Schmidhuber (2011). "Flexible, High Performance Convolutional Neural Networks for Image Classification" (PDF). Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence-Volume Volume Two. 2: 1237–1242. Retrieved 17 November 2013.

^ Krizhevsky, Alex. "ImageNet Classification with Deep Convolutional Neural Networks" (PDF). Retrieved 17 November 2013.

^ a b c d Ciresan, Dan; Meier, Ueli; Schmidhuber, Jürgen (June 2012). Multi-column deep neural networks for image classification. 2012 IEEE Conference on Computer Vision and Pattern Recognition. New York, NY: Institute of Electrical and Electronics Engineers (IEEE). pp. 3642–3649. arXiv:1202.2745. CiteSeerX 10.1.1.300.3283. doi:10.1109/CVPR.2012.6248110. ISBN 978-1-4673-1226-4. OCLC 812295155.

^ "A Survey of FPGA-based Accelerators for Convolutional Neural Networks", NCAA, 2018

^ LeCun, Yann. "LeNet-5, convolutional neural networks". Retrieved 16 November 2013.

^ 
David H. Hubel and Torsten N. Wiesel (2005). Brain and visual perception: the story of a 25-year collaboration. Oxford University Press US. p. 106. ISBN 978-0-19-517618-6.

^ Hubel, DH; Wiesel, TN (October 1959). "Receptive fields of single neurones in the cat's striate cortex". J. Physiol. 148 (3): 574–91. doi:10.1113/jphysiol.1959.sp006308. PMC 1363130. PMID 14403679.

^ LeCun, Yann; Bengio, Yoshua; Hinton, Geoffrey (2015). "Deep learning". Nature. 521 (7553): 436–444. Bibcode:2015Natur.521..436L. doi:10.1038/nature14539. PMID 26017442.

^ Weng, J; Ahuja, N; Huang, TS (1993). "Learning recognition and segmentation of 3-D objects from 2-D images". Proc. 4th International Conf. Computer Vision: 121–128.

^ a b c Schmidhuber, Jürgen (2015). "Deep Learning". Scholarpedia. 10 (11): 1527–54. CiteSeerX 10.1.1.76.1541. doi:10.1162/neco.2006.18.7.1527. PMID 16764513.

^ Homma, Toshiteru; Les Atlas; Robert Marks II (1988). "An Artificial Neural Network for Spatio-Temporal Bipolar Patters: Application to Phoneme Classification" (PDF). Advances in Neural Information Processing Systems. 1: 31–40.

^ a b Waibel, Alex (December 1987). Phoneme Recognition Using Time-Delay Neural Networks. Meeting of the Institute of Electrical, Information and Communication Engineers (IEICE). Tokyo, Japan.

^ a b Alexander Waibel et al., Phoneme Recognition Using Time-Delay Neural Networks IEEE Transactions on Acoustics, Speech, and Signal Processing, Volume 37, No. 3, pp. 328. - 339 March 1989.

^ LeCun, Yann; Bengio, Yoshua (1995). "Convolutional networks for images, speech, and time series".  In Arbib, Michael A. (ed.). The handbook of brain theory and neural networks (Second ed.). The MIT press. pp. 276–278.

^ a b Le Callet, Patrick; Christian Viard-Gaudin; Dominique Barba (2006). "A Convolutional Neural Network Approach for Objective Video Quality Assessment" (PDF). IEEE Transactions on Neural Networks. 17 (5): 1316–1327. doi:10.1109/TNN.2006.879766. PMID 17001990. Retrieved 17 November 2013.

^ Ko, Tom; Peddinti, Vijayaditya; Povey, Daniel; Seltzer, Michael L.; Khudanpur, Sanjeev (March 2018). A Study on Data Augmentation of Reverberant Speech for Robust Speech Recognition. The 42nd IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2017). New Orleans, LA, USA.

^ Denker, J S , Gardner, W R., Graf, H. P, Henderson, D, Howard, R E, Hubbard, W, Jackel, L D , BaIrd, H S, and Guyon (1989) Neural network recognizer for hand-written zip code digits, AT&T Bell Laboratories

^ a b Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, L. D. Jackel, Backpropagation Applied to Handwritten Zip Code Recognition; AT&T Bell Laboratories

^ LeCun, Yann; Léon Bottou; Yoshua Bengio; Patrick Haffner (1998). "Gradient-based learning applied to document recognition" (PDF). Proceedings of the IEEE. 86 (11): 2278–2324. CiteSeerX 10.1.1.32.9552. doi:10.1109/5.726791. Retrieved October 7, 2016.

^ Zhang, Wei (1991). "Error Back Propagation with Minimum-Entropy Weights: A Technique for Better Generalization of 2-D Shift-Invariant NNs". Proceedings of the International Joint Conference on Neural Networks.

^ Zhang, Wei (1991). "Image processing of human corneal endothelium based on a learning network". Applied Optics. 30 (29): 4211–7. Bibcode:1991ApOpt..30.4211Z. doi:10.1364/AO.30.004211. PMID 20706526.

^ Zhang, Wei (1994). "Computerized detection of clustered microcalcifications in digital mammograms using a shift-invariant artificial neural network". Medical Physics. 21 (4): 517–24. Bibcode:1994MedPh..21..517Z. doi:10.1118/1.597177. PMID 8058017.

^ Daniel Graupe, Ruey Wen Liu, George S Moschytz."Applications of neural networks to medical signal processing". In Proc. 27th IEEE Decision and Control Conf.,  pp. 343–347, 1988.

^ Daniel Graupe, Boris Vern, G. Gruener, Aaron Field, and Qiu Huang. "Decomposition of surface EMG signals into single fiber action potentials by means of neural network". Proc. IEEE International Symp. on Circuits and Systems, pp. 1008–1011, 1989.

^ Qiu Huang, Daniel Graupe, Yi Fang Huang, Ruey Wen Liu."Identification of firing patterns of neuronal signals." In Proc. 28th IEEE Decision and Control Conf., pp. 266–271, 1989.

^ Behnke, Sven (2003). Hierarchical Neural Networks for Image Interpretation (PDF). Lecture Notes in Computer Science. 2766. Springer. doi:10.1007/b11963. ISBN 978-3-540-40722-5.

^ Oh, KS; Jung, K (2004). "GPU implementation of neural networks". Pattern Recognition. 37 (6): 1311–1314. doi:10.1016/j.patcog.2004.01.013.

^ Dave Steinkraus; Patrice Simard; Ian Buck (2005). "Using GPUs for Machine Learning Algorithms". 12th International Conference on Document Analysis and Recognition (ICDAR 2005). pp. 1115–1119.

^ Kumar Chellapilla; Sid Puri; Patrice Simard (2006). "High Performance Convolutional Neural Networks for Document Processing".  In Lorette, Guy (ed.). Tenth International Workshop on Frontiers in Handwriting Recognition. Suvisoft.

^ Hinton, GE; Osindero, S; Teh, YW (Jul 2006). "A fast learning algorithm for deep belief nets". Neural Computation. 18 (7): 1527–54. CiteSeerX 10.1.1.76.1541. doi:10.1162/neco.2006.18.7.1527. PMID 16764513.

^ Bengio, Yoshua; Lamblin, Pascal; Popovici, Dan; Larochelle, Hugo (2007). "Greedy Layer-Wise Training of Deep Networks". Advances in Neural Information Processing Systems: 153–160.

^ Ranzato, MarcAurelio; Poultney, Christopher; Chopra, Sumit; LeCun, Yann (2007). "Efficient Learning of Sparse Representations with an Energy-Based Model" (PDF). Advances in Neural Information Processing Systems.

^ Raina, R; Madhavan, A; Ng, Andrew (2009). "Large-scale deep unsupervised learning using graphics processors". ICML: 873–880.

^ Ciresan, Dan; Meier, Ueli; Gambardella, Luca; Schmidhuber, Jürgen (2010). "Deep big simple neural nets for handwritten digit recognition". Neural Computation. 22 (12): 3207–3220. arXiv:1003.0358. doi:10.1162/NECO_a_00052. PMID 20858131.

^ "IJCNN 2011 Competition result table". OFFICIAL IJCNN2011 COMPETITION. 2010. Retrieved 2019-01-14.

^ Schmidhuber, Jürgen (17 March 2017). "History of computer vision contests won by deep CNNs on GPU". Retrieved 14 January 2019.

^ a b Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey E. (2017-05-24). "ImageNet classification with deep convolutional neural networks" (PDF). Communications of the ACM. 60 (6): 84–90. doi:10.1145/3065386. ISSN 0001-0782.

^ He, Kaiming; Zhang, Xiangyu; Ren, Shaoqing; Sun, Jian (2016). "Deep Residual Learning for Image Recognition". 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

^ "CS231n Convolutional Neural Networks for Visual Recognition". cs231n.github.io. Retrieved 2017-04-25.

^ a b Scherer, Dominik; Müller, Andreas C.; Behnke, Sven (2010). "Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition" (PDF). Artificial Neural Networks (ICANN), 20th International Conference on. Thessaloniki, Greece: Springer. pp. 92–101.

^ Graham, Benjamin (2014-12-18). "Fractional Max-Pooling". arXiv:1412.6071 [cs.CV].

^ Springenberg, Jost Tobias; Dosovitskiy, Alexey; Brox, Thomas; Riedmiller, Martin (2014-12-21). "Striving for Simplicity: The All Convolutional Net". arXiv:1412.6806 [cs.LG].

^ Grel, Tomasz (2017-02-28). "Region of interest pooling explained". deepsense.io.

^ Girshick, Ross (2015-09-27). "Fast R-CNN". arXiv:1504.08083 [cs.CV].

^ Romanuke, Vadim (2017). "Appropriate number and allocation of ReLUs in convolutional neural networks" (PDF). Research Bulletin of NTUU "Kyiv Polytechnic Institute". 1: 69–78. doi:10.20535/1810-0546.2017.1.88156. Retrieved 17 February 2019.

^ Krizhevsky, A.; Sutskever, I.; Hinton, G. E. (2012). "Imagenet classification with deep convolutional neural networks" (PDF). Advances in Neural Information Processing Systems. 1: 1097–1105.

^ Deshpande, Adit. "The 9 Deep Learning Papers You Need To Know About (Understanding CNNs Part 3)". adeshpande3.github.io. Retrieved 2018-12-04.

^ Srivastava, Nitish; C. Geoffrey Hinton; Alex Krizhevsky; Ilya Sutskever; Ruslan Salakhutdinov (2014). "Dropout: A Simple Way to Prevent Neural Networks from overfitting" (PDF). Journal of Machine Learning Research. 15 (1): 1929–1958.

^ Carlos E. Perez. "A Pattern Language for Deep Learning".

^ "Regularization of Neural Networks using DropConnect | ICML 2013 | JMLR W&CP". jmlr.org. 2013-02-13. pp. 1058–1066. Retrieved 2015-12-17.

^ Zeiler, Matthew D.; Fergus, Rob (2013-01-15). "Stochastic Pooling for Regularization of Deep Convolutional Neural Networks". arXiv:1301.3557 [cs.LG].

^ a b Platt, John; Steinkraus, Dave; Simard, Patrice Y. (August 2003). "Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis – Microsoft Research". Microsoft Research. Retrieved 2015-12-17.

^ Hinton, Geoffrey E.; Srivastava, Nitish; Krizhevsky, Alex; Sutskever, Ilya; Salakhutdinov, Ruslan R. (2012). "Improving neural networks by preventing co-adaptation of feature detectors". arXiv:1207.0580 [cs.NE].

^ "Dropout: A Simple Way to Prevent Neural Networks from Overfitting". jmlr.org. Retrieved 2015-12-17.

^ Hinton, Geoffrey (1979). "Some demonstrations of the effects of structural descriptions in mental imagery". Cognitive Science. 3 (3): 231–250. doi:10.1016/s0364-0213(79)80008-7.

^ Rock, Irvin. "The frame of reference." The legacy of Solomon Asch: Essays in cognition and social psychology (1990): 243–268.

^ J. Hinton, Coursera lectures on Neural Networks, 2012, Url: https://www.coursera.org/learn/neural-networks

^ Dave Gershgorn (18 June 2018). "The inside story of how AI got good enough to dominate Silicon Valley". Quartz. Retrieved 5 October 2018.

^ Lawrence, Steve; C. Lee Giles; Ah Chung Tsoi; Andrew D. Back (1997). "Face Recognition: A Convolutional Neural Network Approach". IEEE Transactions on Neural Networks. 8 (1): 98–113. CiteSeerX 10.1.1.92.5813. doi:10.1109/72.554195.

^ "ImageNet Large Scale Visual Recognition Competition 2014 (ILSVRC2014)". Retrieved 30 January 2016.

^ Szegedy, Christian; Liu, Wei; Jia, Yangqing; Sermanet, Pierre; Reed, Scott; Anguelov, Dragomir; Erhan, Dumitru; Vanhoucke, Vincent; Rabinovich, Andrew (2014). "Going Deeper with Convolutions". Computing Research Repository. arXiv:1409.4842. Bibcode:2014arXiv1409.4842S.

^ Russakovsky, Olga; Deng, Jia; Su, Hao; Krause, Jonathan; Satheesh, Sanjeev; Ma, Sean; Huang, Zhiheng; Karpathy, Andrej; Khosla, Aditya; Bernstein, Michael; Berg, Alexander C.; Fei-Fei, Li (2014). "Image Net Large Scale Visual Recognition Challenge". arXiv:1409.0575 [cs.CV].

^ "The Face Detection Algorithm Set To Revolutionize Image Search". Technology Review. February 16, 2015. Retrieved 27 October 2017.

^ Baccouche, Moez; Mamalet, Franck; Wolf, Christian; Garcia, Christophe; Baskurt, Atilla (2011-11-16). "Sequential Deep Learning for Human Action Recognition".  In Salah, Albert Ali; Lepri, Bruno (eds.). Human Behavior Unterstanding. Lecture Notes in Computer Science. 7065. Springer Berlin Heidelberg. pp. 29–39. CiteSeerX 10.1.1.385.4740. doi:10.1007/978-3-642-25446-8_4. ISBN 978-3-642-25445-1.

^ Ji, Shuiwang; Xu, Wei; Yang, Ming; Yu, Kai (2013-01-01). "3D Convolutional Neural Networks for Human Action Recognition". IEEE Transactions on Pattern Analysis and Machine Intelligence. 35 (1): 221–231. CiteSeerX 10.1.1.169.4046. doi:10.1109/TPAMI.2012.59. ISSN 0162-8828. PMID 22392705.

^ Huang, Jie; Zhou, Wengang; Zhang, Qilin; Li, Houqiang; Li, Weiping (2018). "Video-based Sign Language Recognition without Temporal Segmentation". arXiv:1801.10111 [cs.CV].

^ Karpathy, Andrej, et al. "Large-scale video classification with convolutional neural networks." IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2014.

^ Simonyan, Karen; Zisserman, Andrew (2014). "Two-Stream Convolutional Networks for Action Recognition in Videos". arXiv:1406.2199 [cs.CV]. (2014).

^ Wang, Le; Duan, Xuhuan; Zhang, Qilin; Niu, Zhenxing; Hua, Gang; Zheng, Nanning (2018-05-22). "Segment-Tube: Spatio-Temporal Action Localization in Untrimmed Videos with Per-Frame Segmentation" (PDF). Sensors. 18 (5): 1657. doi:10.3390/s18051657. ISSN 1424-8220. PMC 5982167. PMID 29789447.

^ Duan, Xuhuan; Wang, Le; Zhai, Changbo; Zheng, Nanning; Zhang, Qilin; Niu, Zhenxing; Hua, Gang (2018). Joint Spatio-Temporal Action Localization in Untrimmed Videos with Per-Frame Segmentation. 25th IEEE International Conference on Image Processing (ICIP). doi:10.1109/icip.2018.8451692. ISBN 978-1-4799-7061-2.

^ Taylor, Graham W.; Fergus, Rob; LeCun, Yann; Bregler, Christoph (2010-01-01). Convolutional Learning of Spatio-temporal Features. Proceedings of the 11th European Conference on Computer Vision: Part VI. ECCV'10. Berlin, Heidelberg: Springer-Verlag. pp. 140–153. ISBN 978-3-642-15566-6.

^ Le, Q. V.; Zou, W. Y.; Yeung, S. Y.; Ng, A. Y. (2011-01-01). Learning Hierarchical Invariant Spatio-temporal Features for Action Recognition with Independent Subspace Analysis. Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition. CVPR '11. Washington, DC, USA: IEEE Computer Society. pp. 3361–3368. CiteSeerX 10.1.1.294.5948. doi:10.1109/CVPR.2011.5995496. ISBN 978-1-4577-0394-2.

^ Grefenstette, Edward; Blunsom, Phil; de Freitas, Nando; Hermann, Karl Moritz (2014-04-29). "A Deep Architecture for Semantic Parsing". arXiv:1404.7296 [cs.CL].

^ Mesnil, Gregoire; Deng, Li; Gao, Jianfeng; He, Xiaodong; Shen, Yelong (April 2014). "Learning Semantic Representations Using Convolutional Neural Networks for Web Search – Microsoft Research". Microsoft Research. Retrieved 2015-12-17.

^ Kalchbrenner, Nal; Grefenstette, Edward; Blunsom, Phil (2014-04-08). "A Convolutional Neural Network for Modelling Sentences". arXiv:1404.2188 [cs.CL].

^ Kim, Yoon (2014-08-25). "Convolutional Neural Networks for Sentence Classification". arXiv:1408.5882 [cs.CL].

^ Collobert, Ronan, and Jason Weston. "A unified architecture for natural language processing: Deep neural networks with multitask learning."Proceedings of the 25th international conference on Machine learning. ACM, 2008.

^ Collobert, Ronan; Weston, Jason; Bottou, Leon; Karlen, Michael; Kavukcuoglu, Koray; Kuksa, Pavel (2011-03-02). "Natural Language Processing (almost) from Scratch". arXiv:1103.0398 [cs.LG].

^ Wallach, Izhar; Dzamba, Michael; Heifets, Abraham (2015-10-09). "AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug Discovery". arXiv:1510.02855 [cs.LG].

^ Yosinski, Jason; Clune, Jeff; Nguyen, Anh; Fuchs, Thomas; Lipson, Hod (2015-06-22). "Understanding Neural Networks Through Deep Visualization". arXiv:1506.06579 [cs.CV].

^ "Toronto startup has a faster way to discover effective medicines". The Globe and Mail. Retrieved 2015-11-09.

^ "Startup Harnesses Supercomputers to Seek Cures". KQED Future of You. 2015-05-27. Retrieved 2015-11-09.

^ Tim Pyrkov, Konstantin Slipensky, Mikhail Barg, Alexey Kondrashin, Boris Zhurov, Alexander Zenin, Mikhail Pyatnitskiy, Leonid Menshikov, Sergei Markov, and Peter O. Fedichev (2018). "Extracting biological age from biomedical data via deep learning: too much of a good thing?". Scientific Reports. 8 (1): 5210. doi:10.1038/s41598-018-23534-9. PMC 5980076. PMID 29581467.CS1 maint: Multiple names: authors list (link)

^ Chellapilla, K; Fogel, DB (1999). "Evolving neural networks to play checkers without relying on expert knowledge". IEEE Trans Neural Netw. 10 (6): 1382–91. doi:10.1109/72.809083. PMID 18252639.

^ Chellapilla, K.; Fogel, D.B. (2001). "Evolving an expert checkers playing program without using human expertise". IEEE Transactions on Evolutionary Computation. 5 (4): 422–428. doi:10.1109/4235.942536.

^ Fogel, David (2001). Blondie24: Playing at the Edge of AI. San Francisco, CA: Morgan Kaufmann. ISBN 978-1558607835.

^ Clark, Christopher; Storkey, Amos (2014). "Teaching Deep Convolutional Neural Networks to Play Go". arXiv:1412.3409 [cs.AI].

^ Maddison, Chris J.; Huang, Aja; Sutskever, Ilya; Silver, David (2014). "Move Evaluation in Go Using Deep Convolutional Neural Networks". arXiv:1412.6564 [cs.LG].

^ "AlphaGo – Google DeepMind". Retrieved 30 January 2016.

^ Durjoy Sen Maitra; Ujjwal Bhattacharya; S.K. Parui, "CNN based common approach to handwritten character recognition of multiple scripts," in Document Analysis and Recognition (ICDAR), 2015 13th International Conference on, vol., no., pp.1021–1025, 23–26 Aug. 2015

^ "NIPS 2017". Interpretable ML Symposium. 2017-10-20. Retrieved 2018-09-12.

^ Zang, Jinliang; Wang, Le; Liu, Ziyi; Zhang, Qilin; Hua, Gang; Zheng, Nanning (2018). "Attention-Based Temporal Weighted Convolutional Neural Network for Action Recognition". IFIP Advances in Information and Communication Technology. Cham: Springer International Publishing. pp. 97–108. arXiv:1803.07179. doi:10.1007/978-3-319-92007-8_9. ISBN 978-3-319-92006-1. ISSN 1868-4238.

^ Wang, Le; Zang, Jinliang; Zhang, Qilin; Niu, Zhenxing; Hua, Gang; Zheng, Nanning (2018-06-21). "Action Recognition by an Attention-Aware Temporal Weighted Convolutional Neural Network" (PDF). Sensors. 18 (7): 1979. doi:10.3390/s18071979. ISSN 1424-8220. PMC 6069475. PMID 29933555.

^ Mnih, Volodymyr;  et al. (2015). "Human-level control through deep reinforcement learning". Nature. 518 (7540): 529–533. Bibcode:2015Natur.518..529M. doi:10.1038/nature14236. PMID 25719670.

^ Sun, R.; Sessions, C. (June 2000). "Self-segmentation of sequences: automatic formation of hierarchies of sequential behaviors". IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics). 30 (3): 403–418. CiteSeerX 10.1.1.11.226. doi:10.1109/3477.846230. ISSN 1083-4419. PMID 18252373.

^ "Convolutional Deep Belief Networks on CIFAR-10" (PDF).

^ Lee, Honglak; Grosse, Roger; Ranganath, Rajesh; Ng, Andrew Y. (1 January 2009). Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations. Proceedings of the 26th Annual International Conference on Machine Learning – ICML '09. ACM. pp. 609–616. CiteSeerX 10.1.1.149.6800. doi:10.1145/1553374.1553453. ISBN 9781605585161.

^ Cade Metz (May 18, 2016). "Google Built Its Very Own Chips to Power Its AI Bots". Wired.

^ "Keras Documentation". keras.io.


External links[edit]
CS231n: Convolutional Neural Networks for Visual Recognition — Andrej Karpathy's Stanford computer science course on CNNs in computer vision
An Intuitive Explanation of Convolutional Neural Networks — A beginner level introduction to what Convolutional Neural Networks are and how they work
Convolutional Neural Networks for Image Classification — Literature Survey



This article contains content that is written like an advertisement. Please help improve it by removing promotional content and inappropriate external links, and by adding encyclopedic content written from a neutral point of view. (February 2019) (Learn how and when to remove this template message)
Google BrainCommercial?YesType of projectArtificial intelligence and machine learningLocationMountain View, CaliforniaWebsiteai.google/brain-team/
Google Brain is a deep learning artificial intelligence research team at Google. Formed in the early 2010s, Google Brain combines open-ended machine learning research with systems engineering and Google-scale computing resources.[1][2][3]

Contents

1 History
2 Projects

2.1 Artificial-intelligence-devised encryption system
2.2 Image enhancement
2.3 Google Translate
2.4 Robotics


3 In Google products
4 Team and location
5 Reception
6 See also
7 References


History[edit]
The so-called "Google Brain" project began in 2011 as a part-time research collaboration between Google Fellow Jeff Dean, Google Researcher Greg Corrado, and Stanford University professor Andrew Ng.[4][5][6] Ng had been interested in using deep learning techniques to crack the problem of artificial intelligence since 2006, and in 2011 began collaborating with Dean and Corrado to build a large-scale deep learning software system, DistBelief,[7] on top of Google's cloud computing infrastructure. Google Brain started as a Google X project and became so successful that it was graduated back to Google: Astro Teller has said that Google Brain paid for the entire cost of Google X.[8]
In June 2012, the New York Times reported that a cluster of 16,000 computers dedicated to mimicking some aspects of human brain activity had successfully trained itself to recognize a cat based on 10 million digital images taken from YouTube videos.[6] The story was also covered by National Public Radio[9] and SmartPlanet.[10]
In March 2013, Google hired Geoffrey Hinton, a leading researcher in the deep learning field, and acquired the company DNNResearch Inc. headed by Hinton. Hinton said that he would be dividing his future time between his university research and his work at Google.[11]

Projects[edit]
Artificial-intelligence-devised encryption system[edit]
In October 2016, the Google Brain ran an experiment concerning the encrypting of communications. In it, two sets of AI's devised their own cryptographic algorithms to protect their communications from another AI, which at the same time aimed at evolving its own system to crack the AI-generated encryption. The study proved to be successful, with the two initial AIs being able to learn and further develop their communications from scratch.[12]
In this experiment, three AIs were created: Alice, Bob and Eve. The goal of the experiment was for Alice to send a message to Bob, which would decrypt it, while in the meantime Eve would try to intercept the message. In it, the AIs were not given specific instructions on how to encrypt their messages, they were solely given a loss function. The consequence was that during the experiment, if communications between Alice and Bob were not successful, with Bob misinterpreting Alice's message or Eve intercepting the communications, the following rounds would show an evolution in the cryptography so that Alice and Bob could communicate safely. Indeed, this study allowed for concluding that it is possible for AIs to devise their own encryption system without having any cryptographic algorithms prescribed beforehand, which would reveal a breakthrough for message encryption in the future.[13]

Image enhancement[edit]
In February 2017, Google Brain announced an image enhancement system using neural networks to fill in details in very low resolution pictures. The examples provided would transform pictures with an 8x8 resolution into 32x32 ones.
The software utilizes two different neural networks to generate the images. The first, called a "conditioning network," maps the pixels of the low-resolution picture to a similar high-resolution one, lowering the resolution of the latter to 8×8 and trying to make a match. The second is a "prior network", which analyzes the pixelated image and tries to add details based on a large number of high resolution pictures. Then, upon upscaling of the original 8×8 picture, the system adds pixels based on its knowledge of what the picture should be. Lastly, the outputs from the two networks are combined to create the final image.[14]
This represents a breakthrough in the enhancement of low resolution pictures. Despite the fact that the added details are not part of the real image, but only best guesses, the technology has shown impressive results when facing real-world testing. Upon being shown the enhanced picture and the real one, humans were fooled 10% of the time in case of celebrity faces, and 28% in case of bedroom pictures. This compares to previous disappointing results from normal bicubic scaling, which did not fool any human.[15][16][17]

Google Translate[edit]
The Google Brain project contributed to Google Translate. In September 2016, Google Neural Machine Translation (GNMT) was launched, an end-to-end learning framework, able to learn from a large number of examples. While its introduction has increased the quality of Google Translate's translations for the pilot languages, it was very difficult to create such improvements for all of its 103 languages. Addressing this problem, the Google Brain Team was able to develop a Multilingual GNMT system, which extended the previous one by enabling translations between multiple languages. Furthermore, it allows for Zero-Shot Translations, which are translations between two languages that the system has never explicitly seen before.[18] Google announced that Google Translate can now also translate without transcribing, using neural networks. This means that it is possible to translate speech in one language directly into text in another language, without first transcribing it to text. According to the Researchers at Google Brain, this intermediate step can be avoided using neural networks. In order for the system to learn this, they exposed it to many hours of Spanish audio together with the corresponding English text. The different layers of neural networks, replicating the human brain, were able to link the corresponding parts and subsequently manipulate the audio waveform until it was transformed to English text.[19]

Robotics[edit]
Different from the traditional robotics, robotics researched by the Google Brain Team could automatically learn to acquire new skills by machine learning. In 2016, the Google Brain Team collaborated with researchers at Google X to demonstrate how robotics could use their experiences to teach themselves more efficiently. Robots made about 800,000 grasping attempts during research.[20] Later in 2017, the team explored three approaches for learning new skills: through reinforcement learning, through their own interaction with objects, and through human demonstration.[20] To build on the goal of the Google Brain Team, they would continue making robots that are able to learn new tasks through learning and practice, as well as deal with complex tasks.

In Google products[edit]
The project's technology is currently used in the Android Operating System's speech recognition system,[21] photo search for Google+[22] and video recommendations in YouTube.[23]

Team and location[edit]
Google Brain was initially established by Google Fellow Jeff Dean and visiting Stanford professor Andrew Ng.[5] In 2014, the team included Jeff Dean, Quoc Le, Ilya Sutskever, Alex Krizhevsky, Samy Bengio and Vincent Vanhoucke. In 2017, team members include Anelia Angelova, Samy Bengio, Greg Corrado, George Dahl, Michael Isard, Anjuli Kannan, Hugo Larochelle, Quoc Le, Chris Olah, Vincent Vanhoucke, Vijay Vasudevan and Fernanda Viegas.[24] Chris Lattner, who created Apple's new programming language Swift and then ran Tesla's autonomy team for six months joined Google Brain's team in August 2017.[25]
Google Brain is based in Mountain View, California and has satellite groups in Accra, Amsterdam, Beijing, Berlin, Cambridge (Massachusetts), London, Montreal, New York City, Paris, Pittsburgh, Princeton, San Francisco, Tokyo, Toronto, and Zurich.[26]

Reception[edit]
Google Brain has received coverage in Wired Magazine,[27][28][29] the New York Times,[29] Technology Review,[30][31] National Public Radio,[9] and Big Think.[32]

See also[edit]
Artificial intelligence
Glossary of artificial intelligence
Quantum Artificial Intelligence Lab – run by Google in collaboration with NASA and Universities Space Research Association
TensorFlow
References[edit]


^ "Brain Team mission – Google AI". Google AI..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ Machine Learning Algorithms and Techniques Research at Google. Retrieved May 18, 2017

^ "Research at Google". research.google.com. Retrieved 2018-02-16.

^ "Google's Large Scale Deep Neural Networks Project". Retrieved 25 October 2015.

^ a b Jeff Dean and Andrew Ng (26 June 2012). "Using large-scale brain simulations for machine learning and A.I." Official Google Blog. Retrieved 26 January 2015.

^ a b Markoff, John (June 25, 2012). "How Many Computers to Identify a Cat? 16,000". The New York Times. New York Times. Retrieved February 11, 2014.

^ Jeffrey Dean;  et al. (December 2012). "Large Scale Distributed Deep Networks" (PDF). Retrieved 25 October 2015.

^ Conor Dougherty (16 February 2015). "Astro Teller, Google's 'Captain of Moonshots,' on Making Profits at Google X". Retrieved 25 October 2015.

^ a b "A Massive Google Network Learns To Identify — Cats". National Public Radio. June 26, 2012. Retrieved February 11, 2014.

^ Shin, Laura (June 26, 2012). "Google brain simulator teaches itself to recognize cats". SmartPlanet. Retrieved February 11, 2014.

^ "U of T neural networks start-up acquired by Google" (Press release). Toronto, ON. 12 March 2013. Retrieved 13 March 2013.

^ "Google AI invents its own cryptographic algorithm; no one knows how it works". arstechnica.co.uk. 2016-10-28. Retrieved 2017-05-15.

^ Abadi, Martín; Andersen, David G. (2016). "Learning to Protect Communications with Adversarial Neural Cryptography". arXiv:1610.06918. Bibcode:2016arXiv161006918A.

^ Dahl, Ryan; Norouzi, Mohammad; Shlens, Jonathon (2017). "Pixel Recursive Super Resolution". arXiv:1702.00783. Bibcode:2017arXiv170200783D.

^ "Google Brain super-resolution image tech makes "zoom, enhance!" real". arstechnica.co.uk. 2017-02-07. Retrieved 2017-05-15.

^ "Google just made 'zoom and enhance' a reality -- kinda". cnet.com. Retrieved 2017-05-15.

^ "Google uses AI to sharpen low-res images". engadget.com. Retrieved 2017-05-15.

^ Schuster, Mike; Johnson, Melvin; Thorat, Nikhil. "Zero-Shot Translation with Google's Multilingual Neural Machine Translation System". Google Research Blog. Retrieved 15 May 2017.

^ Reynolds, Matt. "Google uses neural networks to translate without transcribing". New Scientist. Retrieved 15 May 2017.

^ a b "The Google Brain team — Looking Back on 2016". Research Blog. Retrieved 2017-12-18.

^ "Speech Recognition and Deep Learning". Google Research Blog. August 6, 2012. Retrieved February 11, 2014.

^ "Improving Photo Search: A Step Across the Semantic Gap". Google Research Blog. June 12, 2013.

^ "This Is Google's Plan to Save YouTube". Time. May 18, 2015.

^ Google Brain team website.  Accessed 13.05.2017. https://research.google.com/teams/brain/

^ Etherington, Darrell (Aug 14, 2017). "Swift creator Chris Lattner joins Google Brain after Tesla Autopilot stint". TechCrunch. Retrieved 11 October 2017.

^ "Research at Google". research.google.com. Retrieved 2017-08-01.

^ Levy, Steven (April 25, 2013). "How Ray Kurzweil Will Help Google Make the Ultimate AI Brain". Wired. Retrieved February 11, 2014.

^ Wohlsen, Marcus (January 27, 2014). "Google's Grand Plan to Make Your Brain Irrelevant". Wired. Retrieved February 11, 2014.

^ a b Hernandez, Daniela (May 7, 2013). "The Man Behind the Google Brain: Andrew Ng and the Quest for the New AI". Wired. Retrieved February 11, 2014.

^ Hof, Robert (April 23, 2013). "Deep Learning: With massive amounts of computational power, machines can now recognize objects and translate speech in real time. Artificial intelligence is finally getting smart". Technology Review. Retrieved February 11, 2014.

^ Regalado, Antonio (January 29, 2014). "Is Google Cornering the Market on Deep Learning? A cutting-edge corner of science is being wooed by Silicon Valley, to the dismay of some academics". Technology Review. Retrieved February 11, 2014.

^ "Ray Kurzweil and the Brains Behind the Google Brain". Big Think. December 8, 2013. Retrieved February 11, 2014.



vteGoogle
Alphabet Inc.
Android
Devices
Nexus
Pixel
Maps
YouTube
Company
Overview
Alphabet Inc.
History
List of mergers and acquisitions by Alphabet
List of products
Criticism
Privacy concerns
Censorship
Litigation
Don't be evil
Advertising
AdMob
Adscape
Ads
Ad Manager
AdSense
Analytics
Contributor
Mediabot
Google My Business
Partners
Communication
Alerts
Apps Script
Duo
Calendar
Contacts
Gmail
history
interface
Groups
Hangouts
Sync
Text-to-Speech
Translate
Transliteration
Voice
Software
Assistant
Camera
Lens
Goggles
Chrome
for Android
Web Store
Cloud Print
Earth
Gadgets
Gboard
IME
Japanese
Pinyin
Pay
Pay Send
Photos
Keep
News
Now
OpenRefine
Snapseed
Waze
Operating systems
Android
version history
software development
Android Automotive
Android TV
Wear OS
List of devices
Chrome OS
Apps
Fuchsia

Platforms
Account
Authenticator
Android Auto
ZygoteBody
Cast
Cloud Platform
App Engine
BigQuery
Bigtable
Compute Engine
Storage
Fit
GFS
G Suite
Classroom
Marketplace
Native Client
OpenSocial
Pay
Send
Primer
Play
Books
Games
Movies & TV
Music
Public DNS
Safe Browsing
Stadia
Tango
YouTube TV
Hardware
Cardboard
Daydream
Glass
Contact Lens
Home
Jamboard
Nest Learning Thermostat
Nexus
Pixel
Chrome
Chromebit
Chromecast
Chromebook
Chromebox
Wifi
OnHub
Development
App Inventor
Caja
Closure Tools
Developers
Firebase
GData
KML
Kythe
MapReduce
Owlchemy Labs
Sitemaps
Web Toolkit
Search Console
Website Optimizer
Swiffy
Programming languages
Dart
Go
Sawzall

Frameworks
Angular
AngularJS
AJAX APIs
Dialogflow
Flutter
Guava
Guice
TensorFlow

Publishing
AMP
Blogger
Bookmarks
Domains
Drive
Docs
Sheets
Slides
Drawings
Forms
Fusion Tables
FeedBurner
Sites
My Maps
YouTube
Instant
Premium
Vevo
Zagat
Search(timeline)
Blog Search
Books
Ngram Viewer
Custom Search
Dataset Search
Dictionary
Finance
Flights
Googlebot
Images
Maps
Street View
Coverage
Privacy concerns
News
Archive
Patents
Public Data
Scholar
Shopping
Tenor
Usenet
Videos
Algorithms
PageRank
Panda
Penguin
Hummingbird
Features
Personalized
SafeSearch
Voice Search
Analysis
Insights for Search
Trends
Knowledge Graph and Vault

Events
Code-in
Code Jam
Developer Day
I/O
Science Fair
Summer of Code
People
Larry Page
Sergey Brin
Al Gore
Alan Eustace
Alan Mulally
Amit Singhal
Ann Mather
David Drummond
Eric Schmidt
Jeff Dean
John Doerr
John L. Hennessy
Krishna Bharat
Matt Cutts
Patrick Pichette
Paul Otellini
Omid Kordestani
Rachel Whetstone
Rajen Sheth
Ram Shriram
Ray Kurzweil
Ruth Porat
Salar Kamangar
Sanjay Ghemawat
Shirley M. Tilghman
Sundar Pichai
Susan Wojcicki
Urs Hölzle
Vint Cerf
Hal Varian
Gayglers
Real estate
111 Eighth Avenue
Chelsea Market
Googleplex
Data centers
Other
.google
Arts & Culture
ATAP
Chrome Zone
Data Transfer Project
Dragonfly
Current
Chrome Experiments
Google Business Groups
Made with Code
Data Liberation
Takeout
Google Developer Expert
Google (verb)
Google China
Google Express
Googlization
Grants
Google.org
Logo and related
Doodle4Google
Google Doodles
Product Sans
Lunar X Prize
Google Fi
Google Station
Material Design
Motorola Mobility
Nest Labs
reCAPTCHA
Sunroof
WiFi
AI Challenge
Easter eggs
elgooG
Related
Google bombing
Goojje
Monopoly City Streets
Unity
Documentaries
Google: Behind the Screen
Google: The Thinking Factory
Google and the World Brain


 Category
 Portal
Discontinued products and services




Deepfake (a portmanteau of "deep learning" and "fake"[1]) is a technique for human image synthesis based on artificial intelligence. It is used to combine and superimpose  existing images and videos onto source images or videos using a machine learning technique known as generative adversarial network.[2] The phrase "deepfake" was coined in 2017.
Because of these capabilities, deepfakes have been used to create fake celebrity pornographic videos or revenge porn.[3] Deepfakes can also be used to create fake news and malicious hoaxes.[4][5]

Contents

1 History

1.1 Academic research
1.2 Amateur development


2 Pornography
3 Politics
4 Deepfake software
5 Criticisms

5.1 Abuses
5.2 Effects on credibility and authenticity
5.3 Internet reaction


6 Popular culture

6.1 Picaper by Jack Wodhams


7 References
8 External links


History[edit]
The development of deepfakes has taken place to a large extent in two settings: research at academic institutions, and development by amateurs in online communities.

Academic research[edit]
Academic research related to deepfakes lies predominantly within the field of computer vision, a subfield of computer science often grounded in artificial intelligence that focuses on computer processing of digital images and videos. An early landmark project was the Video Rewrite program, published in 1997, which modified existing video footage of a person speaking to depict that person mouthing the words contained in a different audio track.[6] It was the first system to fully automate this kind of facial reanimation, and it did so using machine learning techniques to make connections between the sounds produced by a video’s subject and the shape of their face.
Contemporary academic projects have focused on creating more realistic videos and on making techniques simpler, faster, and more accessible. The “Synthesizing Obama” program, published in 2017, modifies video footage of former president Barack Obama to depict him mouthing the words contained in a separate audio track.[7] The project lists as a main research contribution its photorealistic technique for synthesizing mouth shapes from audio. The Face2Face program, published in 2016, modifies video footage of a person’s face to depict them mimicking the facial expressions of another person in real time.[8] The project lists as a main research contribution the first method for reenacting facial expressions in real time using a camera that does not capture depth, making it possible for the technique to be performed using common consumer cameras.

Amateur development[edit]
The term deepfakes originated around the end of 2017 from a Reddit user named "deepfakes."[9] He, as well as others in the Reddit community r/deepfakes, shared deepfakes they created; many videos involved celebrities’ faces swapped onto the bodies of actresses in pornographic videos,[9] while non-pornographic content included many videos with actor Nicolas Cage’s face swapped into various movies.[10] In December 2017, Samantha Cole published an article about r/deepfakes in Vice that drew the first mainstream attention to deepfakes being shared in online communities.[11] Six weeks later, Cole wrote in a follow-up article about the large increase in AI-assisted fake pornography.[9] In February 2018, r/deepfakes was banned by Reddit for sharing involuntary pornography, and other websites have also banned the use of deepfakes for involuntary pornography, including the social media platform Twitter and the pornography site Pornhub.[12]
Other online communities remain, however, including Reddit communities that do not share pornography, such as r/SFWdeepfakes (short for "safe for work deepfakes"), in which community members share deepfakes depicting celebrities, politicians, and others in non-pornographic scenarios.[13] Other online communities continue to share pornography on platforms that have not banned deepfake pornography.[12]

Pornography[edit]
Deepfake pornography surfaced on the Internet in 2017, particularly on Reddit,[14] and has been banned by sites including Reddit, Twitter, and Pornhub.[15][16][17] In autumn 2017, an anonymous Reddit user under the pseudonym "deepfakes" posted several porn videos on the Internet. The first one that captured attention was the Daisy Ridley deepfake. It was also one of the more known deepfake videos, and a prominent feature in several articles. Another one was a deepfake simulation of Gal Gadot having sex with her step-brother, while others were of celebrities like Emma Watson, Katy Perry, Taylor Swift or Scarlett Johansson. The scenes were not real, having been created with artificial intelligence. They were debunked a short time later.
As time went on, the Reddit community fixed many bugs in the faked videos, making it increasingly difficult to distinguish fake from true content. Non-pornographic photographs and videos of celebrities, which are readily available online, were used as training data for the software. The deepfake phenomenon was first reported in December 2017 in the technical and scientific section of the magazine Vice, leading to its widespread reporting in other media.[18][19]
Scarlett Johansson, a frequent subject of deepfake porn, spoke publicly about the subject to The Washington Post in December 2018. In a prepared statement, she expressed concern about the phenomenon, describing the internet as a "vast wormhole of darkness that eats itself." However, she also stated that she wouldn't attempt to remove any of her deepfakes, due to her belief that they don't affect her public image and that differing laws across countries and the nature of internet culture make any attempt to remove the deepfakes "a lost cause"; she believes that while celebrities like herself are protected by their fame, deepfakes pose a grave threat to women of lesser prominence who could have their reputations damaged by depiction in involuntary deepfake pornography or revenge porn.[20]
In the United Kingdom, producers of deepfake material can be prosecuted for harassment, but there are calls to make deepfake a specific crime;[21] in the United States, where charges as varied as identity theft, cyberstalking, and revenge porn have been pursued, the notion of a more comprehensive statute has also been discussed.[22]

Politics[edit]
Deepfakes have been used to misrepresent well-known politicians on video portals or chatrooms. For example, the face of the Argentine President Mauricio Macri was replaced by the face of Adolf Hitler, and Angela Merkel's face was replaced with Donald Trump's.[23][24] In April 2018, Jordan Peele and Jonah Peretti created a deepfake using Barack Obama as a public service announcement about the danger of deepfakes.[25] In January 2019, Fox television affiliate KCPQ aired a deepfake of Trump during his Oval Office address, mocking his appearance and skin color.[26]

Deepfake software[edit]
In January 2018, a desktop application called FakeApp was launched. The app allows users to easily create and share videos with faces swapped. The app uses an artificial neural network and the power of the graphics processor and three to four gigabytes of storage space to generate the fake video. For detailed information, the program needs a lot of visual material from the person to be inserted in order to learn which image aspects have to be exchanged, using the deep learning algorithm based on the video sequences and images.
The software uses the AI-Framework TensorFlow of Google, which among other things was already used for the program DeepDream. Celebrities are the main subjects of such fake videos, but other people also appear.[27][28][29] In August 2018, researchers at the University of California, Berkeley published a paper introducing a fake dancing app that can create the impression of masterful dancing ability using AI.[30][31]
There are also open-source alternatives to the original FakeApp program, like DeepFaceLab,[32] FaceSwap (currently hosted on GitHub)[33] and myFakeApp (currently hosted on Bitbucket).[34][35]

Criticisms[edit]
Abuses[edit]
The Aargauer Zeitung says that the manipulation of images and videos using artificial intelligence could become a dangerous mass phenomenon. However, the falsification of images and videos is even older than the advent of video editing software and image editing programs; in this case it is the realism that is a new aspect.[23]
It is also possible to use deepfakes for targeted hoaxes and revenge porn.[36][37]

Effects on credibility and authenticity[edit]
An effect of deepfakes is that it can no longer be distinguished whether content is targeted (e.g. satire) or genuine. AI researcher Alex Champandard has said everyone should know how fast things can be corrupted today with this technology, and that the problem is not a technical one, but rather one to be solved by trust in information and journalism. The primary pitfall is that humanity could fall into an age in which it can no longer be determined whether a medium's content corresponds to the truth.[23]

Internet reaction[edit]
Some websites, such as Twitter and Gfycat, announced that they would delete deepfake content and block its publishers. Previously, the chat platform Discord blocked a chat channel with fake celebrity porn videos. The pornography website, Pornhub, also plans to block such content; however, it has been reported that the site has not been enforcing its ban.[38][39] At Reddit, the situation initially remained unclear until the subreddit was suspended on February 7, 2018, due to the policy violation of "involuntary pornography".[19][40][41][42]
In September 2018, Google added "involuntary synthetic pornographic imagery” to its ban list, allowing anyone to request the block of results showing their fake nudes.[43]

Popular culture[edit]
Picaper by Jack Wodhams[edit]
The 1986 Mid-December issue of Analog magazine published Picaper by Jack Wodhams. Its plot revolves around digitally enhanced or digitally generated videos produced by skilled hackers serving unscrupulous lawyers and political figures. The main protagonist, a troubleshooter for the Systems Monitoring Department, comes across an unusually high-quality fake video.[44]
Jack Wodhams calls such fabricated videos picaper or mimepic — image animation based on "the information from the presented image, and copied through choices from an infinite number of variables that a program might supply".

.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}For instance, someone gets a record of you walking and talking, your facial expressions. These impressions can be broken down to their individual matrix composites, can be analyzed, rearranged, and can then be extrapolated through known standard human behavioral patterns, so that an image of you may be re-projected doing and saying things that you have not in fact done or said.[44]

To Wodhams, pornography is not the major danger of this technology.

There is just a chance that this could be a matter of national interest. Something bigger than pornography or unlicensed alterations to copyrighted portrayals.[44]

In the story, interactive fake video is injected in a video conference call.

These days an actual gathering of a team around a table had become very much a rarity. It was so much simpler, and more convenient, to gather by vieway upon an exclusive circuit. Sessions could be as private and enclosed as any. No system was absolutely impenetrable, however. An opponent might easily sneak a doppelganger into a top-level conference. If accepted, such a known person could glean great deal of information effortlessly and unnoticeably. Not just getting information out, but putting information in. A group of convincing surrogates could feed out opinions quite contrary to those held by their original twins. To sow drastic confusion. To achieve a dramatic coup.[44]

The sobering conclusion is that "the old idea that pictures do not lie is going to have to undergo drastic revision".

References[edit]

^ Brandon, John (2018-02-16). "Terrifying high-tech porn: Creepy 'deepfake' videos are on the rise". Fox News. Retrieved 2018-02-20..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ Schwartz, Oscar (12 November 2018). "You thought fake news was bad? Deep fakes are where truth goes to die". The Guardian. Retrieved 14 November 2018.

^ "What Are Deepfakes & Why the Future of Porn is Terrifying". Highsnobiety. 2018-02-20. Retrieved 2018-02-20.

^ "Experts fear face swapping tech could start an international showdown". The Outline. Retrieved 2018-02-28.

^ Roose, Kevin (2018-03-04). "Here Come the Fake Videos, Too". The New York Times. ISSN 0362-4331. Retrieved 2018-03-24.

^ Bregler, Christoph; Covell, Michele; Slaney, Malcolm (1997). "Video Rewrite: Driving Visual Speech with Audio". Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques. 24: 353–360  – via ACM Digital Library.

^ Suwajanakorn, Supasorn; Seitz, Steven M.; Kemelmacher-Shlizerman, Ira (July 2017). "Synthesizing Obama: Learning Lip Sync from Audio". ACM Trans. Graph. 36.4: 95:1–95:13  – via ACM Digital Library.

^ Thies, Justus; Zollhöfer, Michael; Stamminger, Marc; Theobalt, Christian; Nießner, Matthias (June 2016). "Face2Face: Real-Time Face Capture and Reenactment of RGB Videos". 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE: 2387–2395. doi:10.1109/CVPR.2016.262. ISBN 9781467388511.

^ a b c Cole, Samantha (24 January 2018). "We Are Truly Fucked: Everyone Is Making AI-Generated Fake Porn Now". Vice. Retrieved 4 May 2019.

^ Haysom, Sam (31 January 2018). "People Are Using Face-Swapping Tech to Add Nicolas Cage to Random Movies and What Is 2018". Mashable. Retrieved 4 April 2019.

^ Cole, Samantha (11 December 2017). "AI-Assisted Fake Porn Is Here and We're All Fucked". Vice. Retrieved 19 December 2018.

^ a b Hathaway, Jay (8 February 2018). "Here's where 'deepfakes,' the new fake celebrity porn, went after the Reddit ban". The Daily Dot. Retrieved 22 December 2018.

^ "r/SFWdeepfakes". Reddit. Retrieved 12 December 2018.

^ Roettgers, Janko (2018-02-21). "Porn Producers Offer to Help Hollywood Take Down Deepfake Videos". Variety. Retrieved 2018-02-28.

^ "It took us less than 30 seconds to find banned 'deepfake' AI smut on the internet". Retrieved 2018-02-20.

^ Kharpal, Arjun (2018-02-08). "Reddit, Pornhub ban videos that use A.I. to superimpose a person's face over an X-rated actor". CNBC. Retrieved 2018-02-20.

^ "PornHub, Twitter Ban 'Deepfake' AI-Modified Porn". PCMAG. Retrieved 2018-02-20.

^ AI-Assisted Fake Porn Is Here and We’re All Fucked, Motherboard, 2017-12-11

^ a b Markus Böhm (2018-02-07), "Deepfakes": Firmen gehen gegen gefälschte Promi-Pornos vor, Spiegel Online

^ https://www.washingtonpost.com/technology/2018/12/31/scarlett-johansson-fake-ai-generated-sex-videos-nothing-can-stop-someone-cutting-pasting-my-image

^ Call for upskirting bill to include 'deepfake' pornography ban The Guardian

^ Harrell, Drew. "Fake-porn videos are being weaponized to harass and humiliate women: 'Everybody is a potential target'". The Washington Post. Retrieved 2019-01-01.

^ a b c "Wenn Merkel plötzlich Trumps Gesicht trägt: die gefährliche Manipulation von Bildern und Videos". az Aargauer Zeitung. 2018-02-03.

^ Patrick Gensing. "Deepfakes: Auf dem Weg in eine alternative Realität?".

^ Romano, Aja (April 18, 2018). "Jordan Peele's simulated Obama PSA is a double-edged warning against fake news". Vox. Retrieved September 10, 2018.

^ Swenson, Kyle (January 11, 2019). "A Seattle TV station aired doctored footage of Trump's Oval Office speech. The employee has been fired". The Washington Post. Retrieved January 11, 2019.

^ Britta Bauchmüller, "Fake-App": Mit diesem Programm kann jeder im Porno landen – ob er will oder nicht!, Berliner-Kurier.de

^ Eike Kühl (2018-01-26), Künstliche Intelligenz: Auf Fake News folgt Fake Porn, Die Zeit, ISSN 0044-2070

^ heise online, Deepfakes: Neuronale Netzwerke erschaffen Fake-Porn und Hitler-Parodien

^ Farquhar, Peter (2018-08-27). "An AI program will soon be here to help your deepfake dancing – just don't call it deepfake". Business Insider Australia. Retrieved 2018-08-27.

^ "Deepfakes for dancing: you can now use AI to fake those dance moves you always wanted". The Verge. Retrieved 2018-08-27.

^ https://github.com/iperov/DeepFaceLab

^ https://github.com/deepfakes/faceswap#manifesto

^ https://bitbucket.org/radeksissues/myfakeapp/src

^ https://www.duo.uio.no/bitstream/handle/10852/66387/Master-s-thesis-Tormod-Dag-Fikse.pdf

^ Künstliche Intelligenz: Selfies sind eine gute Quelle, Die Zeit, 2018-01-26, ISSN 0044-2070

^ „Deepfake“ – FakeApp kann Personen in Pornos austauschen – Welche Rechte haben Geschädigte?, WILDE BEUGER SOLMECKE Rechtsanwälte, 2018-02-02

^ "Pornhub hasn't been actively enforcing its deepfake ban". Engadget. Retrieved 2018-04-21.

^ "Pornhub Banned Deepfake Celebrity Sex Videos, But The Site Is Still Full Of Them". BuzzFeed. Retrieved 2018-04-21.

^ barbara.wimmer, Deepfakes: Reddit löscht Forum für künstlich generierte Fake-Pornos

^ heise online. "Deepfakes: Auch Reddit verbannt Fake-Porn".

^ "Reddit verbannt Deepfake-Pornos".

^ Washington Post. "Fake-porn videos are being weaponized to harass and humiliate women: 'Everybody is a potential target'".

^ a b c d "Wodhams J. Picaper. Analog (Mid-December 1986)".


External links[edit]
"This new technology could send American politics into a tailspin"



For deep versus shallow learning in educational psychology, see Student approaches to learning. For more information, see Artificial neural network.
Machine learning anddata mining
Problems
Classification
Clustering
Regression
Anomaly detection
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction


Supervised learning.mw-parser-output .nobold{font-weight:normal}(classification • regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)


Clustering
BIRCH
CURE
Hierarchical
k-means
Expectation–maximization (EM)
DBSCAN
OPTICS
Mean-shift


Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
t-SNE


Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov


Anomaly detection
k-NN
Local outlier factor


Artificial neural networks
Autoencoder
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net


Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)


Theory
Bias–variance dilemma
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory


Machine-learning venues
NIPS
ICML
ML
JMLR
ArXiv:cs.LG


Glossary of artificial intelligence
Glossary of artificial intelligence


Related articles
List of datasets for machine-learning research
Outline of machine learning


 Machine learning portalvte
Deep learning  (also known as deep structured learning  or hierarchical learning) is part of a broader family of machine learning methods based on artificial neural networks. Learning can be supervised, semi-supervised or unsupervised.[1][2][3]
Deep learning architectures such as deep neural networks, deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases superior to human experts.[4][5][6]
Artificial Neural Networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains.  Specifically, neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.[7][8][9]

.mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}Contents

1 Definition
2 Overview
3 Interpretations
4 History

4.1 Deep learning revolution


5 Neural networks

5.1 Artificial neural networks
5.2 Deep neural networks

5.2.1 Challenges




6 Applications

6.1 Automatic speech recognition
6.2 Image recognition
6.3 Visual art processing
6.4 Natural language processing
6.5 Drug discovery and toxicology
6.6 Customer relationship management
6.7 Recommendation systems
6.8 Bioinformatics
6.9 Medical Image Analysis
6.10 Mobile advertising
6.11 Image restoration
6.12 Financial fraud detection
6.13 Military


7 Relation to human cognitive and brain development
8 Commercial activity
9 Criticism and comment

9.1 Theory
9.2 Errors
9.3 Cyber threat


10 See also
11 References
12 Further reading



Definition[edit]
Deep learning is a class of machine learning algorithms that:[10](pp199–200) use multiple layers to progressively extract higher level features from raw input. For example, in image processing, lower layers may identify edges, while higher layer may identify human-meaningful items such as digits/letters or faces.

Overview[edit]
Most modern deep learning models are based on artificial neural networks, specifically, Convolutional Neural Networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.[11]
In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place in which level on its own. (Of course, this does not completely obviate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.)[1][12]
The "deep" in "deep learning" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.[2] No universally agreed upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth > 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function.[citation needed] Beyond that more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning features.
Deep learning architectures are often constructed with a greedy layer-by-layer method.[clarification needed][further explanation needed][citation needed] Deep learning helps to disentangle these abstractions and pick out which features improve performance.[1]
For supervised learning tasks, deep learning methods obviate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.
Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than labeled data. Examples of deep structures that can be trained in an unsupervised manner are neural history compressors[13] and deep belief networks.[1][14]

Interpretations[edit]
Deep neural networks are generally interpreted in terms of the universal approximation theorem[15][16][17][18][19][20] or probabilistic inference.[10][11][1][2][14][21][22]
The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.[15][16][17][18][19] In 1989, the first proof was published by George Cybenko for sigmoid activation functions[16] and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.[17]
The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al.[20] proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; If the width is smaller or equal to the input dimension, then deep neural network is not a universal approximator.
The probabilistic interpretation[21] derives from the field of machine learning. It features inference,[10][11][1][2][14][21] as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.[21] The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks.[23] The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.[24]

History[edit]
The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986,[25][13] and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.[26][27]
The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1965.[28] A 1971 paper described a deep network with 8 layers trained by the group method of data handling algorithm.[29]
Other deep learning working architectures, specifically those built for computer vision, began with the Neocognitron introduced by Kunihiko Fukushima in 1980.[30] In 1989, Yann LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970,[31][32][33][34] to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days.[35]
By 1991 such systems were used for recognizing isolated 2-D hand-written digits, while recognizing 3-D objects was done by matching 2-D images with a handcrafted 3-D object model. Weng et al. suggested that a human brain does not use a monolithic 3-D object model and in 1992 they published Cresceptron,[36][37][38] a method for performing 3-D object recognition in cluttered scenes. Because it directly used natural images, Cresceptron started the beginning of general-purpose visual learning for natural 3D worlds. Cresceptron is a cascade of layers similar to Neocognitron. But while Neocognitron required a human programmer to hand-merge features, Cresceptron learned an open number of features in each layer without supervision, where each feature is represented by a convolution kernel. Cresceptron segmented each learned object from a cluttered scene through back-analysis through the network. Max pooling, now often adopted by deep neural networks (e.g. ImageNet tests), was first used in Cresceptron to reduce the position resolution by a factor of (2x2) to 1 through the cascade for better generalization.
In 1994, André de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.[39]
In 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton.[40] Many factors contribute to the slow speed, including the vanishing gradient problem analyzed in 1991 by Sepp Hochreiter.[41][42]
Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of artificial neural network's (ANN) computational cost and a lack of understanding of how the brain wires its biological networks.
Both shallow and deep learning (e.g., recurrent nets) of ANNs have been explored for many years.[43][44][45] These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[46] Key difficulties have been analyzed, including gradient diminishing[41] and weak temporal correlation structure in neural predictive models.[47][48] Additional difficulties were the lack of training data and limited computing power.
Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI studied deep neural networks in speech and speaker recognition. Heck's speaker recognition team achieved the first significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation.[49] While SRI experienced success with deep neural networks in speaker recognition, they were unsuccessful in demonstrating similar success in speech recognition.
The principle of elevating "raw" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the "raw" spectrogram or linear filter-bank features in the late 1990s,[49] showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.[50]
Many aspects of speech recognition were taken over by a deep learning method called long short-term memory (LSTM), a recurrent neural network published by Hochreiter and Schmidhuber in 1997.[51] LSTM RNNs avoid the vanishing gradient problem and can learn "Very Deep Learning" tasks[2] that require memories of events that happened thousands of discrete time steps before, which is important for speech. In 2003, LSTM started to become competitive with traditional speech recognizers on certain tasks.[52] Later it was combined with connectionist temporal classification (CTC)[53] in stacks of LSTM RNNs.[54] In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which they made available through Google Voice Search.[55]
In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh[56]
[57][58] showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation.[59] The papers referred to learning for deep belief nets.
Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved.[60][61][62] Convolutional neural networks (CNNs) were superseded for ASR by CTC[53] for LSTM.[51][55][63][64][65][66][67] but are more successful in computer vision.
The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun.[68] Industrial applications of deep learning to large-scale speech recognition started around 2010.
The 2009 NIPS Workshop on Deep Learning for Speech Recognition[69] was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets (DNN) might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets.[70] However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems.[60][71] The nature of the recognition errors produced by the two types of systems was characteristically different,[72][69] offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems.[10][73][74] Analysis around 2009-2010, contrasted the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition,[72][69] eventually leading to pervasive and dominant use in that industry. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.[60][72][70][75]
In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.[76][77][78][73]
Advances in hardware enabled the renewed interest. In 2009, Nvidia was involved in what was called the “big bang” of deep learning, “as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs).”[79] That year, Google Brain used Nvidia GPUs to create capable DNNs. While there, Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times.[80] In particular, GPUs are well-suited for the matrix/vector math involved in machine learning.[81][82] GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days.[83][84] Specialized hardware and algorithm optimizations can be used for efficient processing.[85]

Deep learning revolution[edit]
 How deep learning is a subset of machine learning and how machine learning is a subset of artificial intelligence (AI).
In 2012, a team led by Dahl won the "Merck Molecular Activity Challenge" using multi-task deep neural networks to predict the biomolecular target of one drug.[86][87] In 2014, Hochreiter's group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients, household products and drugs and won the "Tox21 Data Challenge" of NIH, FDA and NCATS.[88][89][90]
Significant additional impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades, and GPU implementations of NNs for years, including CNNs, fast implementations of CNNs with max-pooling on GPUs in the style of Ciresan and colleagues were needed to progress on computer vision.[81][82][35][91][2] In 2011, this approach achieved for the first time superhuman performance in a visual pattern recognition contest. Also in 2011, it won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest.[92] Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR[4] showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records. In October 2012, a similar system by Krizhevsky et al.[5] won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. In November 2012, Ciresan et al.'s system also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic.[93] In 2013 and 2014, the error rate on the ImageNet task using deep learning was further reduced, following a similar trend in large-scale speech recognition. The Wolfram Image Identification project publicized these improvements.[94]
Image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.[95][96][97][98]
Some researchers assess that the October 2012 ImageNet victory anchored the start of a "deep learning revolution" that has transformed the AI industry.[99]
In March 2019, Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.

Neural networks[edit]
Artificial neural networks[edit]
Main article: Artificial neural network
Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as "cat" or "no cat" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.
An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.
Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.
The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.
Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, playing "Go"[100] ).

Deep neural networks[edit]
This section may be too technical for most readers to understand. Please help improve it to make it understandable to non-experts, without removing the technical details.  (July 2016) (Learn how and when to remove this template message)
A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers.[11][2] The DNN finds the correct mathematical manipulation to turn the input into the output, whether it be a linear relationship or a non-linear relationship. The network moves through the layers calculating the probability of each output. For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name "deep" networks.
DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives.[101] The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.[11]
Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.
DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or "weights", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network didn’t accurately recognize a particular pattern, an algorithm would adjust the weights.[102] That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.
Recurrent neural networks (RNNs), in which data can flow in any direction, are used for applications such as language modeling.[103][104][105][106][107] Long short-term memory is particularly effective for this use.[51][108]
Convolutional deep neural networks (CNNs) are used in computer vision.[109] CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).[67]

Challenges[edit]
As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.
DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning[29] or weight decay (




ℓ

2




{\displaystyle \ell _{2}}

-regularization) or sparsity (




ℓ

1




{\displaystyle \ell _{1}}

-regularization) can be applied during training to combat overfitting.[110] Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies.[111] Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.[112]
DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples)[113] speed up computation. Large processing capabilities of many-core architectures (such as, GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.[114][115]
Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights for CMAC. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.[116][117]

Applications[edit]
Automatic speech recognition[edit]
Main article: Speech recognition
Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn "Very Deep Learning" tasks[2] that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates[108] is competitive with traditional speech recognizers on certain tasks.[52]
The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences.[118] Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.



Method
Percent phoneerror rate (PER) (%)


Randomly Initialized RNN[119]
26.1


Bayesian Triphone GMM-HMM
25.6


Hidden Trajectory (Generative) Model
24.8


Monophone Randomly Initialized DNN
23.4


Monophone DBN-DNN
22.4


Triphone GMM-HMM with BMMI Training
21.7


Monophone DBN-DNN on fbank
20.7


Convolutional DNN[120]
20.0


Convolutional DNN w. Heterogeneous Pooling
18.7


Ensemble DNN/CNN/RNN[121]
18.3


Bidirectional LSTM
17.9


Hierarchical Convolutional Deep Maxout Network[122]
16.5

The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003-2007, accelerated progress in eight major areas:[10][75][73]

Scale-up/out and acclerated DNN training and decoding
Sequence discriminative training
Feature processing by deep models with solid understanding of the underlying mechanisms
Adaptation of DNNs and related deep models
Multi-task and transfer learning by DNNs and related deep models
CNNs and how to design them to best exploit domain knowledge of speech
RNN and its rich LSTM variants
Other types of deep models including tensor-based models and integrated deep generative/discriminative models.
All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.[10][123][124][125]

Image recognition[edit]
Main article: Computer vision
A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.[126]
Deep learning-based image recognition has become "superhuman", producing more accurate results than human contestants. This first occurred in 2011.[127]
Deep learning-trained vehicles now interpret 360° camera views.[128] Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.

Visual art processing[edit]
Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of a) identifying the style period of a given painting, b) Neural Style Transfer - capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video, and c) generating striking imagery based on random visual input fields.[129][130]

Natural language processing[edit]
Main article: Natural language processing
Neural networks have been used for implementing language models since the early 2000s.[103][131] LSTM helped to improve machine translation and language modeling.[104][105][106]
Other key techniques in this field are negative sampling[132] and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN.[133] Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing.[133] Deep neural architectures provide the best results for constituency parsing,[134] sentiment analysis,[135] information retrieval,[136][137] spoken language understanding,[138] machine translation,[104][139] contextual entity linking,[139] writing style recognition,[140] Text classification and others.[141]
Recent developments generalize word embedding to sentence embedding.
Google Translate (GT) uses a large end-to-end long short-term memory network.[142][143][144][145][146][147] Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system "learns from millions of examples."[143] It translates "whole sentences at a time, rather than pieces. Google Translate supports over one hundred languages.[143] The network encodes the "semantics of the sentence rather than simply memorizing phrase-to-phrase translations".[143][148] GT uses English as an intermediate between most language pairs.[148]

Drug discovery and toxicology[edit]
For more information, see Drug discovery and Toxicology.
A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects.[149][150] Research has explored use of deep learning to predict the biomolecular targets,[86][87] off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.[88][89][90]
AtomNet is a deep learning system for structure-based rational drug design.[151] AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus[152] and multiple sclerosis.[153][154]

Customer relationship management[edit]
Main article: Customer relationship management
Deep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.[155]

Recommendation systems[edit]
Main article: Recommender system
Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music recommendations.[156] Multiview deep learning has been applied for learning user preferences from multiple domains.[157] The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.

Bioinformatics[edit]
Main article: Bioinformatics
An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.[158]
In medical informatics, deep learning was used to predict sleep quality based on data from wearables[159] and predictions of health complications from electronic health record data.[160] Deep learning has also showed efficacy in healthcare.[161]

Medical Image Analysis[edit]
Deep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement[162][163]

Mobile advertising[edit]
Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and assimilated before a target segment can be created and used in ad serving by any ad server.[164] Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.

Image restoration[edit]
Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization. These applications include learning methods such as "Shrinkage Fields for Effective Image Restoration"[165] which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.

Financial fraud detection[edit]
Deep learning is being successfully applied to financial fraud detection and anti-money laundering. "Deep anti-money laundering detection system can spot and recognize relationships and similarities between data and, further down the road, learn to detect anomalies or classify and predict specific events". The solution leverages both supervised learning techniques, such as the classification of suspicious transactions, and unsupervised learning, e.g. anomaly detection.
[166]

Military[edit]
The United States Department of Defense applied deep learning to train robots in new tasks through observation.[167]

Relation to human cognitive and brain development[edit]
Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s.[168][169][170][171] These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, "...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature."[172]
A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism.[173][174] Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality.[175][176] In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.[177]
Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons[178][179] and neural populations.[180] Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system[181] both at the single-unit[182] and at the population[183] levels.

Commercial activity[edit]
Many organizations employ deep learning for particular applications. Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.[184]
Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player.[185][186][187] Google Translate uses an LSTM to translate between more than 100 languages.
In 2015, Blippar demonstrated a mobile augmented reality application that uses deep learning to recognize objects in real time.[188]
As of 2008,[189] researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor.[167]
First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot the ability to learn new tasks through observation.[167]
Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as “good job” and “bad job.”[190]

Criticism and comment[edit]
Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science.

Theory[edit]
See also: Explainable AI
A main criticism concerns the lack of theory surrounding some methods.[191] Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear.[citation needed] (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.[192]

Others point out that deep learning should be looked at as a step towards realizing strong AI, not as an all-encompassing solution. Despite the power of deep learning methods, they still lack much of the functionality needed for realizing this goal entirely. Research psychologist Gary Marcus noted:"Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning."[193]As an alternative to this emphasis on the limits of deep learning, one author speculated that it might be possible to train a machine vision stack to perform the sophisticated task of discriminating between "old master" and amateur figure drawings, and hypothesized that such a sensitivity might represent the rudiments of a non-trivial machine empathy.[194] This same author proposed that this would be in line with anthropology, which identifies a concern with aesthetics as a key element of behavioral modernity.[195]
In further reference to the idea that artistic sensitivity might inhere within relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained[196] demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's[197] web site.

Errors[edit]
Some deep learning architectures display problematic behaviors,[198] such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images[199] and misclassifying minuscule perturbations of correctly classified images.[200] Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures.[198] These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar[201] decompositions of observed entities and events.[198] Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition[202] and artificial intelligence (AI).[203]

Cyber threat[edit]
As deep learning moves from the lab into the world, research and experience shows that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such a manipulation is termed an “adversarial attack.” In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system.[204] One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.[205]
Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.[204]
ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.[204]
Another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address that would download malware.[204]
In “data poisoning”, false data is continually smuggled into a machine learning system’s training set to prevent it from achieving mastery.[204]

See also[edit]
Applications of artificial intelligence
Comparison of deep learning software
Compressed sensing
Echo state network
List of artificial intelligence projects
Liquid state machine
List of datasets for machine learning research
Reservoir computing
Sparse coding
References[edit]


^ a b c d e f Bengio, Y.; Courville, A.; Vincent, P. (2013). "Representation Learning: A Review and New Perspectives". IEEE Transactions on Pattern Analysis and Machine Intelligence. 35 (8): 1798–1828. arXiv:1206.5538. doi:10.1109/tpami.2013.50. PMID 23787338..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ a b c d e f g h Schmidhuber, J. (2015). "Deep Learning in Neural Networks: An Overview". Neural Networks. 61: 85–117. arXiv:1404.7828. doi:10.1016/j.neunet.2014.09.003. PMID 25462637.

^ Bengio, Yoshua; LeCun, Yann; Hinton, Geoffrey (2015). "Deep Learning". Nature. 521 (7553): 436–444. Bibcode:2015Natur.521..436L. doi:10.1038/nature14539. PMID 26017442.

^ a b Ciresan, Dan; Meier, U.; Schmidhuber, J. (June 2012). "Multi-column deep neural networks for image classification". 2012 IEEE Conference on Computer Vision and Pattern Recognition: 3642–3649. arXiv:1202.2745. doi:10.1109/cvpr.2012.6248110. ISBN 978-1-4673-1228-8.

^ a b Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffry (2012). "ImageNet Classification with Deep Convolutional Neural Networks" (PDF). NIPS 2012: Neural Information Processing Systems, Lake Tahoe, Nevada.

^ "Google's AlphaGo AI wins three-match series against the world's best Go player". TechCrunch. 25 May 2017.

^ Marblestone, Adam H.; Wayne, Greg; Kording, Konrad P. (2016). "Toward an Integration of Deep Learning and Neuroscience". Frontiers in Computational Neuroscience. 10: 94. doi:10.3389/fncom.2016.00094. PMC 5021692. PMID 27683554.

^ Olshausen, B. A. (1996). "Emergence of simple-cell receptive field properties by learning a sparse code for natural images". Nature. 381 (6583): 607–609. Bibcode:1996Natur.381..607O. doi:10.1038/381607a0. PMID 8637596.

^ Bengio, Yoshua; Lee, Dong-Hyun; Bornschein, Jorg; Mesnard, Thomas; Lin, Zhouhan (2015-02-13). "Towards Biologically Plausible Deep Learning". arXiv:1502.04156 [cs.LG].

^ a b c d e f Deng, L.; Yu, D. (2014). "Deep Learning: Methods and Applications" (PDF). Foundations and Trends in Signal Processing. 7 (3–4): 1–199. doi:10.1561/2000000039.

^ a b c d e Bengio, Yoshua (2009). "Learning Deep Architectures for AI" (PDF). Foundations and Trends in Machine Learning. 2 (1): 1–127. CiteSeerX 10.1.1.701.9550. doi:10.1561/2200000006.

^ LeCun, Yann; Bengio, Yoshua; Hinton, Geoffrey (28 May 2015). "Deep learning". Nature. 521 (7553): 436–444. Bibcode:2015Natur.521..436L. doi:10.1038/nature14539. PMID 26017442.

^ a b Jürgen Schmidhuber (2015). Deep Learning. Scholarpedia, 10(11):32832. Online

^ a b c Hinton, G.E. (2009). "Deep belief networks". Scholarpedia. 4 (5): 5947. Bibcode:2009SchpJ...4.5947H. doi:10.4249/scholarpedia.5947.

^ a b Balázs Csanád Csáji (2001). Approximation with Artificial Neural Networks; Faculty of Sciences; Eötvös Loránd University, Hungary

^ a b c Cybenko (1989). "Approximations by superpositions of sigmoidal functions" (PDF). Mathematics of Control, Signals, and Systems. 2 (4): 303–314. doi:10.1007/bf02551274. Archived from the original (PDF) on 2015-10-10.

^ a b c Hornik, Kurt (1991). "Approximation Capabilities of Multilayer Feedforward Networks". Neural Networks. 4 (2): 251–257. doi:10.1016/0893-6080(91)90009-t.

^ a b Haykin, Simon S. (1999). Neural Networks: A Comprehensive Foundation. Prentice Hall. ISBN 978-0-13-273350-2.

^ a b Hassoun, Mohamad H. (1995). Fundamentals of Artificial Neural Networks. MIT Press. p. 48. ISBN 978-0-262-08239-6.

^ a b Lu, Z., Pu, H., Wang, F., Hu, Z., & Wang, L. (2017). The Expressive Power of Neural Networks: A View from the Width. Neural Information Processing Systems, 6231-6239.

^ a b c d Murphy, Kevin P. (24 August 2012). Machine Learning: A Probabilistic Perspective. MIT Press. ISBN 978-0-262-01802-9.

^ Patel, Ankit; Nguyen, Tan; Baraniuk, Richard (2016). "A Probabilistic Framework for Deep Learning" (PDF). Advances in Neural Information Processing Systems.

^ Hinton, G. E.; Srivastava, N.; Krizhevsky, A.; Sutskever, I.; Salakhutdinov, R.R. (2012). "Improving neural networks by preventing co-adaptation of feature detectors". arXiv:1207.0580 [math.LG].

^ Bishop, Christopher M. (2006). Pattern Recognition and Machine Learning (PDF). Springer. ISBN 978-0-387-31073-2.

^ Rina Dechter (1986). Learning while searching in constraint-satisfaction problems. University of California, Computer Science Department, Cognitive Systems Laboratory.Online

^ Igor Aizenberg, Naum N. Aizenberg, Joos P.L. Vandewalle (2000). Multi-Valued and Universal Binary Neurons: Theory, Learning and Applications. Springer Science & Business Media.

^ Co-evolving recurrent neurons learn deep memory POMDPs. Proc. GECCO, Washington, D. C., pp. 1795-1802, ACM Press, New York, NY, USA, 2005.

^ Ivakhnenko, A. G. (1973). Cybernetic Predicting Devices. CCM Information Corporation.

^ a b Ivakhnenko, Alexey (1971). "Polynomial theory of complex systems". IEEE Transactions on Systems, Man and Cybernetics. 1 (4): 364–378. doi:10.1109/TSMC.1971.4308320.

^ Fukushima, K. (1980). "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position". Biol. Cybern. 36 (4): 193–202. doi:10.1007/bf00344251. PMID 7370364.

^ Seppo Linnainmaa (1970). The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors. Master's Thesis (in Finnish), Univ. Helsinki, 6-7.

^ Griewank, Andreas (2012). "Who Invented the Reverse Mode of Differentiation?" (PDF). Documenta Matematica (Extra Volume ISMP): 389–400.

^ Werbos, P. (1974). "Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences". Harvard University. Retrieved 12 June 2017.

^ Werbos, Paul (1982). "Applications of advances in nonlinear sensitivity analysis" (PDF). System modeling and optimization. Springer. pp. 762–770.

^ a b LeCun et al., "Backpropagation Applied to Handwritten Zip Code Recognition," Neural Computation, 1, pp. 541–551, 1989.

^ J. Weng, N. Ahuja and T. S. Huang, "Cresceptron: a self-organizing neural network which grows adaptively," Proc. International Joint Conference on Neural Networks, Baltimore, Maryland, vol I, pp. 576-581, June, 1992.

^ J. Weng, N. Ahuja and T. S. Huang, "Learning recognition and segmentation of 3-D objects from 2-D images," Proc. 4th International Conf. Computer Vision, Berlin, Germany, pp. 121-128, May, 1993.

^ J. Weng, N. Ahuja and T. S. Huang, "Learning recognition and segmentation using the Cresceptron," International Journal of Computer Vision, vol. 25, no. 2, pp. 105-139, Nov. 1997.

^ de Carvalho, Andre C. L. F.; Fairhurst, Mike C.; Bisset, David (1994-08-08). "An integrated Boolean neural network for pattern classification". Pattern Recognition Letters. 15 (8): 807–813. doi:10.1016/0167-8655(94)90009-4.

^ Hinton, Geoffrey E.; Dayan, Peter; Frey, Brendan J.; Neal, Radford (1995-05-26). "The wake-sleep algorithm for unsupervised neural networks". Science. 268 (5214): 1158–1161. Bibcode:1995Sci...268.1158H. doi:10.1126/science.7761831.

^ a b S. Hochreiter., "Untersuchungen zu dynamischen neuronalen Netzen," Diploma thesis. Institut f. Informatik, Technische Univ. Munich. Advisor: J. Schmidhuber, 1991.

^ Hochreiter, S.;  et al. (15 January 2001). "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies".  In Kolen, John F.; Kremer, Stefan C. (eds.). A Field Guide to Dynamical Recurrent Networks. John Wiley & Sons. ISBN 978-0-7803-5369-5.

^ Morgan, Nelson; Bourlard, Hervé; Renals, Steve; Cohen, Michael; Franco, Horacio (1993-08-01). "Hybrid neural network/hidden markov model systems for continuous speech recognition". International Journal of Pattern Recognition and Artificial Intelligence. 07 (4): 899–916. doi:10.1142/s0218001493000455. ISSN 0218-0014.

^ Robinson, T. (1992). "A real-time recurrent error propagation network word recognition system". ICASSP: 617–620.

^ Waibel, A.; Hanazawa, T.; Hinton, G.; Shikano, K.; Lang, K. J. (March 1989). "Phoneme recognition using time-delay neural networks". IEEE Transactions on Acoustics, Speech, and Signal Processing. 37 (3): 328–339. doi:10.1109/29.21701. ISSN 0096-3518.

^ Baker, J.; Deng, Li; Glass, Jim; Khudanpur, S.; Lee, C.-H.; Morgan, N.; O'Shaughnessy, D. (2009). "Research Developments and Directions in Speech Recognition and Understanding, Part 1". IEEE Signal Processing Magazine. 26 (3): 75–80. Bibcode:2009ISPM...26...75B. doi:10.1109/msp.2009.932166.

^ Bengio, Y. (1991). "Artificial Neural Networks and their Application to Speech/Sequence Recognition". McGill University Ph.D. thesis.

^ Deng, L.; Hassanein, K.; Elmasry, M. (1994). "Analysis of correlation structure for a neural predictive model with applications to speech recognition". Neural Networks. 7 (2): 331–339. doi:10.1016/0893-6080(94)90027-2.

^ a b Heck, L.; Konig, Y.; Sonmez, M.; Weintraub, M. (2000). "Robustness to Telephone Handset Distortion in Speaker Recognition by Discriminative Feature Design". Speech Communication. 31 (2): 181–192. doi:10.1016/s0167-6393(99)00077-1.

^ "Acoustic Modeling with Deep Neural Networks Using Raw Time Signal for LVCSR (PDF Download Available)". ResearchGate. Retrieved 2017-06-14.

^ a b c Hochreiter, Sepp; Schmidhuber, Jürgen (1997-11-01). "Long Short-Term Memory". Neural Computation. 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735. ISSN 0899-7667. PMID 9377276.

^ a b Graves, Alex; Eck, Douglas; Beringer, Nicole; Schmidhuber, Jürgen (2003). "Biologically Plausible Speech Recognition with LSTM Neural Nets" (PDF). 1st Intl. Workshop on Biologically Inspired Approaches to Advanced Information Technology, Bio-ADIT 2004, Lausanne, Switzerland. pp. 175–184.

^ a b Graves, Alex; Fernández, Santiago; Gomez, Faustino (2006). "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks". Proceedings of the International Conference on Machine Learning, ICML 2006: 369–376. CiteSeerX 10.1.1.75.6306.

^ Santiago Fernandez, Alex Graves, and Jürgen Schmidhuber (2007). An application of recurrent neural networks to discriminative keyword spotting. Proceedings of ICANN (2), pp. 220–229.

^ a b Sak, Haşim; Senior, Andrew; Rao, Kanishka; Beaufays, Françoise; Schalkwyk, Johan (September 2015). "Google voice search: faster and more accurate".

^ Hinton, Geoffrey E. (2007-10-01). "Learning multiple layers of representation". Trends in Cognitive Sciences. 11 (10): 428–434. doi:10.1016/j.tics.2007.09.004. ISSN 1364-6613. PMID 17921042.

^ Hinton, G. E.; Osindero, S.; Teh, Y. W. (2006). "A Fast Learning Algorithm for Deep Belief Nets" (PDF). Neural Computation. 18 (7): 1527–1554. doi:10.1162/neco.2006.18.7.1527. PMID 16764513.

^ Bengio, Yoshua (2012). "Practical recommendations for gradient-based training of deep architectures". arXiv:1206.5533 [cs.LG].

^ G. E. Hinton., "Learning multiple layers of representation," Trends in Cognitive Sciences, 11, pp. 428–434, 2007.

^ a b c Hinton, G.; Deng, L.; Yu, D.; Dahl, G.; Mohamed, A.; Jaitly, N.; Senior, A.; Vanhoucke, V.; Nguyen, P.; Sainath, T.; Kingsbury, B. (2012). "Deep Neural Networks for Acoustic Modeling in Speech Recognition --- The shared views of four research groups". IEEE Signal Processing Magazine. 29 (6): 82–97. doi:10.1109/msp.2012.2205597.

^ Deng, Li; Hinton, Geoffrey; Kingsbury, Brian (1 May 2013). "New types of deep neural network learning for speech recognition and related applications: An overview"  – via research.microsoft.com.

^ Deng, L.; Li, J.; Huang, J. T.; Yao, K.; Yu, D.; Seide, F.; Seltzer, M.; Zweig, G.; He, X. (May 2013). "Recent advances in deep learning for speech research at Microsoft". 2013 IEEE International Conference on Acoustics, Speech and Signal Processing: 8604–8608. doi:10.1109/icassp.2013.6639345. ISBN 978-1-4799-0356-6.

^ Sak, Hasim; Senior, Andrew; Beaufays, Francoise (2014). "Long Short-Term Memory recurrent neural network architectures for large scale acoustic modeling" (PDF).

^ Li, Xiangang; Wu, Xihong (2014). "Constructing Long Short-Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition". arXiv:1410.4281 [cs.CL].

^ Zen, Heiga; Sak, Hasim (2015). "Unidirectional Long Short-Term Memory Recurrent Neural Network with Recurrent Output Layer for Low-Latency Speech Synthesis" (PDF). Google.com. ICASSP. pp. 4470–4474.

^ Deng, L.; Abdel-Hamid, O.; Yu, D. (2013). "A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion" (PDF). Google.com. ICASSP.

^ a b Sainath, T. N.; Mohamed, A. r; Kingsbury, B.; Ramabhadran, B. (May 2013). "Deep convolutional neural networks for LVCSR". 2013 IEEE International Conference on Acoustics, Speech and Signal Processing: 8614–8618. doi:10.1109/icassp.2013.6639347. ISBN 978-1-4799-0356-6.

^ Yann LeCun (2016). Slides on Deep Learning Online

^ a b c NIPS Workshop: Deep Learning for Speech Recognition and Related Applications, Whistler, BC, Canada, Dec. 2009 (Organizers: Li Deng, Geoff Hinton, D. Yu).

^ a b Keynote talk: Recent Developments in Deep Neural Networks. ICASSP, 2013 (by Geoff Hinton).

^ D. Yu, L. Deng, G. Li, and F. Seide (2011). "Discriminative pretraining of deep neural networks," U.S. Patent Filing.

^ a b c Deng, L.; Hinton, G.; Kingsbury, B. (2013). "New types of deep neural network learning for speech recognition and related applications: An overview (ICASSP)" (PDF).

^ a b c Yu, D.; Deng, L. (2014). Automatic Speech Recognition: A Deep Learning Approach (Publisher: Springer). ISBN 978-1-4471-5779-3.

^ "Deng receives prestigious IEEE Technical Achievement Award - Microsoft Research". Microsoft Research. 3 December 2015.

^ a b Li, Deng (September 2014). "Keynote talk: 'Achievements and Challenges of Deep Learning - From Speech Analysis and Recognition To Language and Multimodal Processing'". Interspeech.

^ Yu, D.; Deng, L. (2010). "Roles of Pre-Training and Fine-Tuning in Context-Dependent DBN-HMMs for Real-World Speech Recognition". NIPS Workshop on Deep Learning and Unsupervised Feature Learning.

^ Seide, F.; Li, G.; Yu, D. (2011). "Conversational speech transcription using context-dependent deep neural networks". Interspeech.

^ Deng, Li; Li, Jinyu; Huang, Jui-Ting; Yao, Kaisheng; Yu, Dong; Seide, Frank; Seltzer, Mike; Zweig, Geoff; He, Xiaodong (2013-05-01). "Recent Advances in Deep Learning for Speech Research at Microsoft". Microsoft Research.

^ "Nvidia CEO bets big on deep learning and VR". Venture Beat. April 5, 2016.

^ "From not working to neural networking". The Economist.

^ a b Oh, K.-S.; Jung, K. (2004). "GPU implementation of neural networks". Pattern Recognition. 37 (6): 1311–1314. doi:10.1016/j.patcog.2004.01.013.

^ a b Chellapilla, K., Puri, S., and Simard, P. (2006). High performance convolutional neural networks for document processing. International Workshop on Frontiers in Handwriting Recognition.

^ Cireşan, Dan Claudiu; Meier, Ueli; Gambardella, Luca Maria; Schmidhuber, Jürgen (2010-09-21). "Deep, Big, Simple Neural Nets for Handwritten Digit Recognition". Neural Computation. 22 (12): 3207–3220. arXiv:1003.0358. doi:10.1162/neco_a_00052. ISSN 0899-7667. PMID 20858131.

^ Raina, Rajat; Madhavan, Anand; Ng, Andrew Y. (2009). "Large-scale Deep Unsupervised Learning Using Graphics Processors". Proceedings of the 26th Annual International Conference on Machine Learning. ICML '09. New York, NY, USA: ACM: 873–880. CiteSeerX 10.1.1.154.372. doi:10.1145/1553374.1553486. ISBN 9781605585161.

^ Sze, Vivienne; Chen, Yu-Hsin; Yang, Tien-Ju; Emer, Joel (2017). "Efficient Processing of Deep Neural Networks: A Tutorial and Survey". arXiv:1703.09039 [cs.CV].

^ a b "Announcement of the winners of the Merck Molecular Activity Challenge".

^ a b "Multi-task Neural Networks for QSAR Predictions | Data Science Association". www.datascienceassn.org. Retrieved 2017-06-14.

^ a b "Toxicology in the 21st century Data Challenge"

^ a b "NCATS Announces Tox21 Data Challenge Winners".

^ a b "Archived copy". Archived from the original on 2015-02-28. Retrieved 2015-03-05.CS1 maint: Archived copy as title (link)

^ Ciresan, D. C.; Meier, U.; Masci, J.; Gambardella, L. M.; Schmidhuber, J. (2011). "Flexible, High Performance Convolutional Neural Networks for Image Classification" (PDF). International Joint Conference on Artificial Intelligence. doi:10.5591/978-1-57735-516-8/ijcai11-210.

^ Ciresan, Dan; Giusti, Alessandro; Gambardella, Luca M.; Schmidhuber, Juergen (2012).  Pereira, F.; Burges, C. J. C.; Bottou, L.; Weinberger, K. Q. (eds.). Advances in Neural Information Processing Systems 25 (PDF). Curran Associates, Inc. pp. 2843–2851.

^ Ciresan, D.; Giusti, A.; Gambardella, L.M.; Schmidhuber, J. (2013). "Mitosis Detection in Breast Cancer Histology Images using Deep Neural Networks". Proceedings MICCAI. Lecture Notes in Computer Science. 7908: 411–418. doi:10.1007/978-3-642-40763-5_51. ISBN 978-3-642-38708-1.

^ "The Wolfram Language Image Identification Project". www.imageidentify.com. Retrieved 2017-03-22.

^ Vinyals, Oriol; Toshev, Alexander; Bengio, Samy; Erhan, Dumitru (2014). "Show and Tell: A Neural Image Caption Generator". arXiv:1411.4555 [cs.CV]..

^ Fang, Hao; Gupta, Saurabh; Iandola, Forrest; Srivastava, Rupesh; Deng, Li; Dollár, Piotr; Gao, Jianfeng; He, Xiaodong; Mitchell, Margaret; Platt, John C; Lawrence Zitnick, C; Zweig, Geoffrey (2014). "From Captions to Visual Concepts and Back". arXiv:1411.4952 [cs.CV]..

^ Kiros, Ryan; Salakhutdinov, Ruslan; Zemel, Richard S (2014). "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models". arXiv:1411.2539 [cs.LG]..

^ Zhong, Sheng-hua; Liu, Yan; Liu, Yang (2011). "Bilinear Deep Learning for Image Classification". Proceedings of the 19th ACM International Conference on Multimedia. MM '11. New York, NY, USA: ACM: 343–352. doi:10.1145/2072298.2072344. ISBN 9781450306164.

^ "Why Deep Learning Is Suddenly Changing Your Life". Fortune. 2016. Retrieved 13 April 2018.

^ Silver, David; Huang, Aja; Maddison, Chris J.; Guez, Arthur; Sifre, Laurent; Driessche, George van den; Schrittwieser, Julian; Antonoglou, Ioannis; Panneershelvam, Veda (January 2016). "Mastering the game of Go with deep neural networks and tree search". Nature. 529 (7587): 484–489. Bibcode:2016Natur.529..484S. doi:10.1038/nature16961. ISSN 1476-4687. PMID 26819042.

^ Szegedy, Christian; Toshev, Alexander; Erhan, Dumitru (2013). "Deep neural networks for object detection". Advances in Neural Information Processing Systems.

^ Hof, Robert D. "Is Artificial Intelligence Finally Coming into Its Own?". MIT Technology Review. Retrieved 2018-07-10.

^ a b Gers, Felix A.; Schmidhuber, Jürgen (2001). "LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages". IEEE Trans. Neural Netw. 12 (6): 1333–1340. doi:10.1109/72.963769. PMID 18249962.

^ a b c Sutskever, L.; Vinyals, O.; Le, Q. (2014). "Sequence to Sequence Learning with Neural Networks" (PDF). Proc. NIPS.

^ a b Jozefowicz, Rafal; Vinyals, Oriol; Schuster, Mike; Shazeer, Noam; Wu, Yonghui (2016). "Exploring the Limits of Language Modeling". arXiv:1602.02410 [cs.CL].

^ a b Gillick, Dan; Brunk, Cliff; Vinyals, Oriol; Subramanya, Amarnag (2015). "Multilingual Language Processing from Bytes". arXiv:1512.00103 [cs.CL].

^ Mikolov, T.;  et al. (2010). "Recurrent neural network based language model" (PDF). Interspeech.

^ a b "Learning Precise Timing with LSTM Recurrent Networks (PDF Download Available)". ResearchGate. Retrieved 2017-06-13.

^ LeCun, Y.;  et al. (1998). "Gradient-based learning applied to document recognition". Proceedings of the IEEE. 86 (11): 2278–2324. doi:10.1109/5.726791.

^ Bengio, Y.; Boulanger-Lewandowski, N.; Pascanu, R. (May 2013). "Advances in optimizing recurrent networks". 2013 IEEE International Conference on Acoustics, Speech and Signal Processing: 8624–8628. arXiv:1212.0901. CiteSeerX 10.1.1.752.9151. doi:10.1109/icassp.2013.6639349. ISBN 978-1-4799-0356-6.

^ Dahl, G.;  et al. (2013). "Improving DNNs for LVCSR using rectified linear units and dropout" (PDF). ICASSP.

^ "Data Augmentation - deeplearning.ai | Coursera". Coursera. Retrieved 2017-11-30.

^ Hinton, G. E. (2010). "A Practical Guide to Training Restricted Boltzmann Machines". Tech. Rep. UTML TR 2010-003.

^ You, Yang; Buluç, Aydın; Demmel, James (November 2017). "Scaling deep learning on GPU and knights landing clusters". SC '17, ACM. Retrieved 5 March 2018.

^ Viebke, André; Memeti, Suejb; Pllana, Sabri; Abraham, Ajith (March 2017). "CHAOS: a parallelization scheme for training convolutional neural networks on Intel Xeon Phi". The Journal of Supercomputing. 75: 197–227. doi:10.1007/s11227-017-1994-x.

^ Ting Qin, et al. "A learning algorithm of CMAC based on RLS." Neural Processing Letters 19.1 (2004): 49-61.

^ Ting Qin, et al. "Continuous CMAC-QRLS and its systolic array." Neural Processing Letters 22.1 (2005): 1-16.

^ TIMIT Acoustic-Phonetic Continuous Speech Corpus Linguistic Data Consortium, Philadelphia.

^ Robinson, Tony (30 September 1991). "Several Improvements to a Recurrent Error Propagation Network Phone Recognition System". Cambridge University Engineering Department Technical Report. CUED/F-INFENG/TR82. doi:10.13140/RG.2.2.15418.90567.

^ Abdel-Hamid, O.;  et al. (2014). "Convolutional Neural Networks for Speech Recognition". IEEE/ACM Transactions on Audio, Speech, and Language Processing. 22 (10): 1533–1545. doi:10.1109/taslp.2014.2339736.

^ Deng, L.; Platt, J. (2014). "Ensemble Deep Learning for Speech Recognition" (PDF). Proc. Interspeech.

^ Tóth, Laszló (2015). "Phone Recognition with Hierarchical Convolutional Deep Maxout Networks" (PDF). EURASIP Journal on Audio, Speech, and Music Processing. 2015. doi:10.1186/s13636-015-0068-3.

^ "How Skype Used AI to Build Its Amazing New Language Translator | WIRED". www.wired.com. Retrieved 2017-06-14.

^ Hannun, Awni; Case, Carl; Casper, Jared; Catanzaro, Bryan; Diamos, Greg; Elsen, Erich; Prenger, Ryan; Satheesh, Sanjeev; Sengupta, Shubho; Coates, Adam; Ng, Andrew Y (2014). "Deep Speech: Scaling up end-to-end speech recognition". arXiv:1412.5567 [cs.CL].

^ "Plenary presentation at ICASSP-2016" (PDF).

^ "MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges". yann.lecun.com.

^ Cireşan, Dan; Meier, Ueli; Masci, Jonathan; Schmidhuber, Jürgen (August 2012). "Multi-column deep neural network for traffic sign classification". Neural Networks. Selected Papers from IJCNN 2011. 32: 333–338. CiteSeerX 10.1.1.226.8219. doi:10.1016/j.neunet.2012.02.023. PMID 22386783.

^ Nvidia Demos a Car Computer Trained with "Deep Learning" (2015-01-06), David Talbot, MIT Technology Review

^ G. W. Smith; Frederic Fol Leymarie (10 April 2017). "The Machine as Artist: An Introduction". Arts. Retrieved 4 October 2017.

^ Blaise Agüera y Arcas (29 September 2017). "Art in the Age of Machine Intelligence". Arts. Retrieved 4 October 2017.

^ Bengio, Yoshua; Ducharme, Réjean; Vincent, Pascal; Janvin, Christian (March 2003). "A Neural Probabilistic Language Model". J. Mach. Learn. Res. 3: 1137–1155. ISSN 1532-4435.

^ Goldberg, Yoav; Levy, Omar (2014). "word2vec Explained: Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method". arXiv:1402.3722 [cs.CL].

^ a b Socher, Richard; Manning, Christopher. "Deep Learning for NLP" (PDF). Retrieved 26 October 2014.

^ Socher, Richard; Bauer, John; Manning, Christopher; Ng, Andrew (2013). "Parsing With Compositional Vector Grammars" (PDF). Proceedings of the ACL 2013 Conference.

^ Socher, Richard (2013). "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank" (PDF).

^ Shen, Yelong; He, Xiaodong; Gao, Jianfeng; Deng, Li; Mesnil, Gregoire (2014-11-01). "A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval". Microsoft Research.

^ Huang, Po-Sen; He, Xiaodong; Gao, Jianfeng; Deng, Li; Acero, Alex; Heck, Larry (2013-10-01). "Learning Deep Structured Semantic Models for Web Search using Clickthrough Data". Microsoft Research.

^ Mesnil, G.; Dauphin, Y.; Yao, K.; Bengio, Y.; Deng, L.; Hakkani-Tur, D.; He, X.; Heck, L.; Tur, G.; Yu, D.; Zweig, G. (2015). "Using recurrent neural networks for slot filling in spoken language understanding". IEEE Transactions on Audio, Speech, and Language Processing. 23 (3): 530–539. doi:10.1109/taslp.2014.2383614.

^ a b Gao, Jianfeng; He, Xiaodong; Yih, Scott Wen-tau; Deng, Li (2014-06-01). "Learning Continuous Phrase Representations for Translation Modeling". Microsoft Research.

^ Brocardo, Marcelo Luiz; Traore, Issa; Woungang, Isaac; Obaidat, Mohammad S. (2017). "Authorship verification using deep belief network systems". International Journal of Communication Systems. 30 (12): e3259. doi:10.1002/dac.3259.

^ "Deep Learning for Natural Language Processing: Theory and Practice (CIKM2014 Tutorial) - Microsoft Research". Microsoft Research. Retrieved 2017-06-14.

^ Turovsky, Barak (November 15, 2016). "Found in translation: More accurate, fluent sentences in Google Translate". The Keyword Google Blog. Retrieved March 23, 2017.

^ a b c d Schuster, Mike; Johnson, Melvin; Thorat, Nikhil (November 22, 2016). "Zero-Shot Translation with Google's Multilingual Neural Machine Translation System". Google Research Blog. Retrieved March 23, 2017.

^ Sepp Hochreiter; Jürgen Schmidhuber (1997). "Long short-term memory". Neural Computation. 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735. PMID 9377276.

^ Felix A. Gers; Jürgen Schmidhuber; Fred Cummins (2000). "Learning to Forget: Continual Prediction with LSTM". Neural Computation. 12 (10): 2451–2471. CiteSeerX 10.1.1.55.5709. doi:10.1162/089976600300015015.

^ Wu, Yonghui; Schuster, Mike; Chen, Zhifeng; Le, Quoc V; Norouzi, Mohammad; Macherey, Wolfgang; Krikun, Maxim; Cao, Yuan; Gao, Qin; Macherey, Klaus; Klingner, Jeff; Shah, Apurva; Johnson, Melvin; Liu, Xiaobing; Kaiser, Łukasz; Gouws, Stephan; Kato, Yoshikiyo; Kudo, Taku; Kazawa, Hideto; Stevens, Keith; Kurian, George; Patil, Nishant; Wang, Wei; Young, Cliff; Smith, Jason; Riesa, Jason; Rudnick, Alex; Vinyals, Oriol; Corrado, Greg;  et al. (2016). "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation". arXiv:1609.08144 [cs.CL].

^ "An Infusion of AI Makes Google Translate More Powerful Than Ever." Cade Metz, WIRED, Date of Publication: 09.27.16. https://www.wired.com/2016/09/google-claims-ai-breakthrough-machine-translation/

^ a b Boitet, Christian; Blanchon, Hervé; Seligman, Mark; Bellynck, Valérie (2010). "MT on and for the Web" (PDF). Retrieved December 1, 2016.

^ Arrowsmith, J; Miller, P (2013). "Trial watch: Phase II and phase III attrition rates 2011-2012". Nature Reviews Drug Discovery. 12 (8): 569. doi:10.1038/nrd4090. PMID 23903212.

^ Verbist, B; Klambauer, G; Vervoort, L; Talloen, W; The Qstar, Consortium; Shkedy, Z; Thas, O; Bender, A; Göhlmann, H. W.; Hochreiter, S (2015). "Using transcriptomics to guide lead optimization in drug discovery projects: Lessons learned from the QSTAR project". Drug Discovery Today. 20 (5): 505–513. doi:10.1016/j.drudis.2014.12.014. PMID 25582842.

^ Wallach, Izhar; Dzamba, Michael; Heifets, Abraham (2015-10-09). "AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug Discovery". arXiv:1510.02855 [cs.LG].

^ "Toronto startup has a faster way to discover effective medicines". The Globe and Mail. Retrieved 2015-11-09.

^ "Startup Harnesses Supercomputers to Seek Cures". KQED Future of You. Retrieved 2015-11-09.

^ "Toronto startup has a faster way to discover effective medicines".

^ Tkachenko, Yegor (April 8, 2015). "Autonomous CRM Control via CLV Approximation with Deep Reinforcement Learning in Discrete and Continuous Action Space". arXiv:1504.01840 [cs.LG].

^ van den Oord, Aaron; Dieleman, Sander; Schrauwen, Benjamin (2013).  Burges, C. J. C.; Bottou, L.; Welling, M.; Ghahramani, Z.; Weinberger, K. Q. (eds.). Advances in Neural Information Processing Systems 26 (PDF). Curran Associates, Inc. pp. 2643–2651.

^ Elkahky, Ali Mamdouh; Song, Yang; He, Xiaodong (2015-05-01). "A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems". Microsoft Research.

^ Chicco, Davide; Sadowski, Peter; Baldi, Pierre (1 January 2014). Deep Autoencoder Neural Networks for Gene Ontology Annotation Predictions. Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics - BCB '14. ACM. pp. 533–540. doi:10.1145/2649387.2649442. hdl:11311/964622. ISBN 9781450328944.

^ Sathyanarayana, Aarti (2016-01-01). "Sleep Quality Prediction From Wearable Data Using Deep Learning". JMIR mHealth and uHealth. 4 (4): e125. doi:10.2196/mhealth.6562. PMC 5116102. PMID 27815231.

^ Choi, Edward; Schuetz, Andy; Stewart, Walter F.; Sun, Jimeng (2016-08-13). "Using recurrent neural network models for early detection of heart failure onset". Journal of the American Medical Informatics Association. 24 (2): 361–370. doi:10.1093/jamia/ocw112. ISSN 1067-5027. PMC 5391725. PMID 27521897.

^ "Deep Learning in Healthcare: Challenges and Opportunities". Medium. 2016-08-12. Retrieved 2018-04-10.

^ Litjens, Geert; Kooi, Thijs; Bejnordi, Babak Ehteshami; Setio, Arnaud Arindra Adiyoso; Ciompi, Francesco; Ghafoorian, Mohsen; van der Laak, Jeroen A.W.M.; van Ginneken, Bram; Sánchez, Clara I. (December 2017). "A survey on deep learning in medical image analysis". Medical Image Analysis. 42: 60–88. doi:10.1016/j.media.2017.07.005.

^ Forslid, Gustav; Wieslander, Hakan; Bengtsson, Ewert; Wahlby, Carolina; Hirsch, Jan-Michael; Stark, Christina Runow; Sadanandan, Sajith Kecheril (October 2017). "Deep Convolutional Neural Networks for Detecting Cellular Changes Due to Malignancy". 2017 IEEE International Conference on Computer Vision Workshops (ICCVW). Venice: IEEE: 82–89. doi:10.1109/ICCVW.2017.18. ISBN 9781538610343.

^ De, Shaunak; Maity, Abhishek; Goel, Vritti; Shitole, Sanjay; Bhattacharya, Avik (2017). "Predicting the popularity of instagram posts for a lifestyle magazine using deep learning". 2nd IEEE Conference on Communication Systems, Computing and IT Applications: 174–177. doi:10.1109/CSCITA.2017.8066548. ISBN 978-1-5090-4381-1.

^ Schmidt, Uwe; Roth, Stefan. Shrinkage Fields for Effective Image Restoration (PDF). Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on.

^ Czech, Tomasz. "Deep learning: the next frontier for money laundering detection". Global Banking and Finance Review.

^ a b c "Army researchers develop new algorithms to train robots". EurekAlert!. Retrieved 2018-08-29.

^ Utgoff, P. E.; Stracuzzi, D. J. (2002). "Many-layered learning". Neural Computation. 14 (10): 2497–2529. doi:10.1162/08997660260293319. PMID 12396572.

^ Elman, Jeffrey L. (1998). Rethinking Innateness: A Connectionist Perspective on Development. MIT Press. ISBN 978-0-262-55030-7.

^ Shrager, J.; Johnson, MH (1996). "Dynamic plasticity influences the emergence of function in a simple cortical array". Neural Networks. 9 (7): 1119–1129. doi:10.1016/0893-6080(96)00033-0. PMID 12662587.

^ Quartz, SR; Sejnowski, TJ (1997). "The neural basis of cognitive development: A constructivist manifesto". Behavioral and Brain Sciences. 20 (4): 537–556. CiteSeerX 10.1.1.41.7854. doi:10.1017/s0140525x97001581.

^ S. Blakeslee., "In brain's early growth, timetable may be critical," The New York Times, Science Section, pp. B5–B6, 1995.

^ Mazzoni, P.; Andersen, R. A.; Jordan, M. I. (1991-05-15). "A more biologically plausible learning rule for neural networks". Proceedings of the National Academy of Sciences. 88 (10): 4433–4437. Bibcode:1991PNAS...88.4433M. doi:10.1073/pnas.88.10.4433. ISSN 0027-8424. PMC 51674. PMID 1903542.

^ O'Reilly, Randall C. (1996-07-01). "Biologically Plausible Error-Driven Learning Using Local Activation Differences: The Generalized Recirculation Algorithm". Neural Computation. 8 (5): 895–938. doi:10.1162/neco.1996.8.5.895. ISSN 0899-7667.

^ Testolin, Alberto; Zorzi, Marco (2016). "Probabilistic Models and Generative Neural Networks: Towards an Unified Framework for Modeling Normal and Impaired Neurocognitive Functions". Frontiers in Computational Neuroscience. 10: 73. doi:10.3389/fncom.2016.00073. ISSN 1662-5188. PMC 4943066. PMID 27468262.

^ Testolin, Alberto; Stoianov, Ivilin; Zorzi, Marco (September 2017). "Letter perception emerges from unsupervised deep learning and recycling of natural image features". Nature Human Behaviour. 1 (9): 657–664. doi:10.1038/s41562-017-0186-2. ISSN 2397-3374.

^ Buesing, Lars; Bill, Johannes; Nessler, Bernhard; Maass, Wolfgang (2011-11-03). "Neural Dynamics as Sampling: A Model for Stochastic Computation in Recurrent Networks of Spiking Neurons". PLOS Computational Biology. 7 (11): e1002211. Bibcode:2011PLSCB...7E2211B. doi:10.1371/journal.pcbi.1002211. ISSN 1553-7358. PMC 3207943. PMID 22096452.

^ Morel, Danielle; Singh, Chandan; Levy, William B. (2018-01-25). "Linearization of excitatory synaptic integration at no extra cost". Journal of Computational Neuroscience. 44 (2): 173–188. doi:10.1007/s10827-017-0673-5. ISSN 0929-5313. PMID 29372434.

^ Cash, S.; Yuste, R. (February 1999). "Linear summation of excitatory inputs by CA1 pyramidal neurons". Neuron. 22 (2): 383–394. doi:10.1016/s0896-6273(00)81098-3. ISSN 0896-6273. PMID 10069343.

^ Olshausen, B; Field, D (2004-08-01). "Sparse coding of sensory inputs". Current Opinion in Neurobiology. 14 (4): 481–487. doi:10.1016/j.conb.2004.07.007. ISSN 0959-4388.

^ Yamins, Daniel L K; DiCarlo, James J (March 2016). "Using goal-driven deep learning models to understand sensory cortex". Nature Neuroscience. 19 (3): 356–365. doi:10.1038/nn.4244. ISSN 1546-1726.

^ Zorzi, Marco; Testolin, Alberto (2018-02-19). "An emergentist perspective on the origin of number sense". Phil. Trans. R. Soc. B. 373 (1740): 20170043. doi:10.1098/rstb.2017.0043. ISSN 0962-8436. PMC 5784047. PMID 29292348.

^ Güçlü, Umut; van Gerven, Marcel A. J. (2015-07-08). "Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream". Journal of Neuroscience. 35 (27): 10005–10014. arXiv:1411.6422. doi:10.1523/jneurosci.5023-14.2015. PMID 26157000.

^ Metz, C. (12 December 2013). "Facebook's 'Deep Learning' Guru Reveals the Future of AI". Wired.

^ "Google AI algorithm masters ancient game of Go". Nature News & Comment. Retrieved 2016-01-30.

^ Silver, David; Huang, Aja; Maddison, Chris J.; Guez, Arthur; Sifre, Laurent; Driessche, George van den; Schrittwieser, Julian; Antonoglou, Ioannis; Panneershelvam, Veda; Lanctot, Marc; Dieleman, Sander; Grewe, Dominik; Nham, John; Kalchbrenner, Nal; Sutskever, Ilya; Lillicrap, Timothy; Leach, Madeleine; Kavukcuoglu, Koray; Graepel, Thore; Hassabis, Demis (28 January 2016). "Mastering the game of Go with deep neural networks and tree search". Nature. 529 (7587): 484–489. Bibcode:2016Natur.529..484S. doi:10.1038/nature16961. ISSN 0028-0836. PMID 26819042.

^ "A Google DeepMind Algorithm Uses Deep Learning and More to Master the Game of Go | MIT Technology Review". MIT Technology Review. Retrieved 2016-01-30.

^ "Blippar Demonstrates New Real-Time Augmented Reality App". TechCrunch.

^ "TAMER: Training an Agent Manually via Evaluative Reinforcement - IEEE Conference Publication". ieeexplore.ieee.org. Retrieved 2018-08-29.

^ "Talk to the Algorithms: AI Becomes a Faster Learner". governmentciomedia.com. Retrieved 2018-08-29.

^ Marcus, Gary (2018-01-14). "In defense of skepticism about deep learning". Gary Marcus. Retrieved 2018-10-11.

^ Knight, Will (2017-03-14). "DARPA is funding projects that will try to open up AI's black boxes". MIT Technology Review. Retrieved 2017-11-02.

^ Marcus, Gary (November 25, 2012). "Is "Deep Learning" a Revolution in Artificial Intelligence?". The New Yorker. Retrieved 2017-06-14.

^ Smith, G. W. (March 27, 2015). "Art and Artificial Intelligence". ArtEnt. Archived from the original on June 25, 2017. Retrieved March 27, 2015.CS1 maint: BOT: original-url status unknown (link)

^ Mellars, Paul (February 1, 2005). "The Impossible Coincidence: A Single-Species Model for the Origins of Modern Human Behavior in Europe" (PDF). Evolutionary Anthropology: Issues, News, and Reviews. Retrieved April 5, 2017.

^ Alexander Mordvintsev; Christopher Olah; Mike Tyka (June 17, 2015). "Inceptionism: Going Deeper into Neural Networks". Google Research Blog. Retrieved June 20, 2015.

^ Alex Hern (June 18, 2015). "Yes, androids do dream of electric sheep". The Guardian. Retrieved June 20, 2015.

^ a b c Goertzel, Ben (2015). "Are there Deep Reasons Underlying the Pathologies of Today's Deep Learning Algorithms?" (PDF).

^ Nguyen, Anh; Yosinski, Jason; Clune, Jeff (2014). "Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images". arXiv:1412.1897 [cs.CV].

^ Szegedy, Christian; Zaremba, Wojciech; Sutskever, Ilya; Bruna, Joan; Erhan, Dumitru; Goodfellow, Ian; Fergus, Rob (2013). "Intriguing properties of neural networks". arXiv:1312.6199 [cs.CV].

^ Zhu, S.C.; Mumford, D. (2006). "A stochastic grammar of images". Found. Trends Comput. Graph. Vis. 2 (4): 259–362. CiteSeerX 10.1.1.681.2190. doi:10.1561/0600000018.

^ Miller, G. A., and N. Chomsky. "Pattern conception." Paper for Conference on pattern detection, University of Michigan. 1957.

^ Eisner, Jason. "Deep Learning of Recursive Structure: Grammar Induction".

^ a b c d e "AI Is Easy to Fool—Why That Needs to Change". Singularity Hub. 2017-10-10. Retrieved 2017-10-11.

^ Gibney, Elizabeth (2017). "The scientist who spots fake videos". Nature. doi:10.1038/nature.2017.22784.


Further reading[edit]
.mw-parser-output .refbegin{font-size:90%;margin-bottom:0.5em}.mw-parser-output .refbegin-hanging-indents>ul{list-style-type:none;margin-left:0}.mw-parser-output .refbegin-hanging-indents>ul>li,.mw-parser-output .refbegin-hanging-indents>dl>dd{margin-left:0;padding-left:3.2em;text-indent:-3.2em;list-style:none}.mw-parser-output .refbegin-100{font-size:100%}
Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016). Deep Learning. MIT Press. ISBN 978-0-26203561-3, introductory textbook.



This article may rely excessively on sources too closely associated with the subject, potentially preventing the article from being verifiable and neutral. Please help improve it by replacing them with more appropriate citations to reliable, independent, third-party sources. (May 2019) (Learn how and when to remove this template message)
In U.S. education, deeper learning is a set of student educational outcomes including acquisition of robust core academic content, higher-order thinking skills, and learning dispositions. Deeper learning is based on the premise that the nature of work, civic, and everyday life is changing and therefore increasingly requires that formal education provides young people with mastery of skills like analytic reasoning, complex problem solving, and teamwork. 
Deeper learning is associated with a growing movement in U.S. education that places special emphasis on the ability to apply knowledge to real-world circumstances and to solve novel problems.[1]
A number of U.S. schools and school districts serving a broad socio-economic spectrum apply deeper learning as an integral component of their instructional approach.[2]

Contents

1 History
2 Skills and competencies
3 Instructional reforms
4 Network of schools
5 Assessment
6 See also
7 References
8 External links


History[edit]
While the term "deeper learning" is relatively new, the notion of enabling students to develop skills that empower them to apply learning and to adapt to and thrive in post-secondary education as well as career and life is not. A number of significant antecedents to deeper learning exist.
For example, American philosopher, psychologist and educational reformer John Dewey (1859–1952) made a strong case for the importance of education not only as a place to gain content knowledge, but also as a place to learn how to live. Like modern proponents of deeper learning, Dewey believed that students thrive in an environment where they are allowed to experience and interact with the curriculum, and all students should have the opportunity to take part in their own learning. Dewey's arguments undergirded the movements of progressive education and constructivist education, which called for teaching and learning beyond rote content knowledge.
In the 1990s, skills-based education saw a resurgence with the advent of the "21st Century Skills" movements and the "Partnership for 21st Century skills".[3] In 2012 the National Research Council of the National Academies issued Education for Life and Work: Developing Transferrable Knowledge and Skill in the 21st Century, a report on deeper learning re-elevating the issue and summarizing research evidence on its outcomes to date.[4]

Skills and competencies[edit]
According to labor economists Frank Levy of MIT and Richard Murnane of Harvard’s Graduate School of Education, since 1970, with the economic changes brought about by technology and globalization, employers’ demands for workers with routine, repetitive skills—whether manual or cognitive—have dropped steeply, while demand for those with deeper learning competencies like complex thinking and communications skills has soared.[5]
Research by Cassel and Kolstad found that by the year 2000 the top skills demanded by U.S. Fortune 500 companies had shifted from traditional reading, writing and arithmetic to teamwork, problem solving, and interpersonal skills.[6]
A 2006 Conference Board survey of some 400 employers revealed that key deeper learning competencies were the most important for new entrants into the workforce. Essential capabilities included oral and written communications and critical thinking/problem solving. The Conference Board findings indicate that "applied skills on all educational levels trump basic knowledge and skills, such as Reading Comprehension and Mathematics ... while the ‘three Rs’ are still fundamental to any new workforce entrant’s ability to do the job, employers emphasize that applied skills like Teamwork/Collaboration and Critical Thinking are ‘very important’ to success at work."[7]
In 2002 a coalition of national business community, education leaders, and policymakers founded the Partnership for 21st Century Skills (now the Partnership for 21st Century Learning, or P21), a non-profit organization.  P21's goal is to foster a national conversation on "the importance of 21st century skills for all students" and "position 21st century readiness at the center of US K-12 education".  The organization has released reports exploring how to integrate the Four Cs approach into learning environments.[8]  Their research and publications included an identification of deeper learning competencies and skills they called the Four Cs of 21st century learning (collaboration, communication, critical thinking, creativity).  In a 2012 survey conducted by the American Management Association (AMA), executives found a need for highly skilled employees to keep up with the fast pace of change in business and to compete on a global level.  The survey identified three of the "Four Cs" (critical thinking, communication and collaboration) as the top three skills necessary for these employees.[9]
"Deeper learning" was described by the William and Flora Hewlett Foundation in 2010[10] specifying a set of educational outcomes:[11]

Mastery of rigorous academic content
Development of critical thinking and problem-solving skills
The ability to work collaboratively
Effective oral and written communication
Learning how to learn
Developing and maintaining an academic mindset.
Instructional reforms[edit]
Deeper learning practitioners have developed a number instructional reform methods and built a variety of classroom, school, and district models. While stressing robust content mastery, instructors ask students to "move beyond basic comprehension and algorithmic procedures and engage in skills that lie at the top of traditional learning taxonomies—analysis, synthesis, and creation," according to Harvard education scholars Jal Mehta and Sarah Fine.[12] "Students are treated as active meaning makers with the capacity to do interesting and valuable work now ... the purpose of school is not so much to prepare students for a hypothetical future as to support them in engaging with the complex challenges that professional work at its best entails."

In its 2012 report Education for Life and Work, the National Research Council identified the following research-based methods for developing deeper learning:[4]

Use multiple and varied representations of concepts and tasks
Encourage elaboration, questioning, and self-explanation
Engage learners in challenging tasks, with supportive guidance and feedback
Teach with examples and cases
Prime student motivation
Use formative assessment
Deeper examination of what "best practices" evidence shows connect teaching methods to the development of the Partnership for 21st Century Learning's 4C framework[13] and the competencies identified in the Hewlett model for deeper learning,[14] give a sharper picture of "what works" in terms of instructional strategies and tools. For instance, the Marzano Lab has identified the high effects of cooperative learning to develop collaboration, graphic organizers to advance critical thinking, feedback to sharpen communication, advance organizers to enrich entry activities in PBLs, etc.[15] John Hattie's meta-analysis of visible learning is even more specific. Strategies that promote metacognition, reflection, student feedback, creativity, inquiry and more support the type of teaching that most enriches mindful, deeper learning. In addition, his studies detail how surface teaching strategies such as lectures, worksheets, overly frequent testing and others do little for achievement or deeper learning.[16] For young learners, the Center for Childhood Creativity has identified the powerful role of creative thinking in the classroom.[17]
While evidence supporting the direct impact of education organized around deeper learning outcomes in driving academic achievement is not robust to date, it continues to build. P21 is leading an effort at the University of Connecticut to remedy this.  As early as 2008 a study of seven hundred California students demonstrated that students exposed to math instruction designed to develop deeper learning competencies significantly outperformed peers taught through more traditional methods.[1]

Network of schools[edit]
A number of educational reform school networks across the country focus on developing deeper learning competencies.[18] While committed to deeper learning educational outcomes, these networks, however, vary in their instructional models and approaches to school design. Notable networks include Asia Society International Studies Schools Network, EdVisions Schools, and Envision Education.
Because of limits imposed by state and federal laws, public school districts face the largest challenges to bring deeper learning back into their schools. The Partnership for 21st Century Learning ([19] initiated the identification of exemplar schools which were relying on inclusion of 21st Century Skills as a base component for bringing deeper learning experiences to all children. Some of these exemplar schools come from the reform networks, but many are schools and districts that targeted deeper learning instruction and outcomes as their mission but without the benefits in money, public relations and compliance given to charter schools.
To further advance the notion, P21 created a Blogazine to "connect the dots between 21st Century skills and deeper learning outcomes".[20] The blog articles are written pro bono by major educational writers who advocate for the paradigm shift to Deeper Learning as well as by a balance of school leaders, teachers, professional learning specialists and others who are incorporating deeper learning practices into their curricula, instruction, assessment and system change plans. In its second year, the no-fee online P21 Blogazine expanded into a three times weekly online journal.
As more schools, especially public schools, began to plan to integrate deeper learning, a group of Illinois advocates, aligned with P21, searched for assistance to scale best 21st Century teaching practices into classrooms. Already successful exemplars in the US and abroad were relying on versions of project based learning (problem-based, inquiry-based, product-making, project- based);[21] there was great variation in effectiveness. After reviewing models from multiple sources, the Illinois Consortium for 21st Century Schools determined none were adequate for systemic integration into schools or systems. The consortium team, made up of volunteer, long experienced professional developers, classroom teachers, administrators and school change specialists, all with experience in public school reform, adapted and redesigned the most effective PBL models and designed a new school-wide approach of PBL that included explicit instruction and assessment of the 4CS as advocated by the Partnership, technology, reflection and a 5th C, cultural responsiveness. These elements were integrated into a PBL design cycle, called MindQuest21. Creative making was balanced with critical thinking to allow for teachers to challenge the narrow framework of the standards which ignored the creative C.[22]
The MindQuest21 approach was not an isolated example. As the P21 Exemplar identification program showed, more and more schools, often acting alone, sometimes in concert with other schools in a district, were shifting the learning paradigm from surface learning pushed by NCLB recall tests to deeper learning stimulated by entrepreneurial administrators and teachers. In a like manner, creative teachers who were able to defy the punishment threats of NCLB, did the same.[23]

Assessment[edit]
The majority of tests used in the current U.S. school system focus mainly on achievement of content knowledge and rely heavily on multiple-choice items, measuring primarily low-level knowledge and some basic skills.[24] A study by the RAND Corporation found that, in the 17 states studied, fewer than 2% of mathematics items and only about 20% of English language arts (ELA) items on state tests ask students to analyze, synthesize, compare, critique, investigate, prove, or explain their ideas.[24]
However, two federally funded multi-state assessment consortia—the Partnership for Assessment of Readiness for College and Careers (PARCC) and the Smarter Balanced Assessment Consortium (SBAC)[25] —were formed to develop next-generation assessment tools, to be launched in 2014–15. Research conducted by UCLA's CRESST show marked increases in the amount of higher-order skills to be assessed as measured by the Depth of Knowledge scale.[26] The Innovation Lab Network (ILN) of states,[27] coordinated by the Council of Chief State School Officers, convenes a smaller, informal consortium of ten states to develop strategies to create and deploy even more intellectually ambitious assessments. The performance assessments under development by participating states includes tasks that require students to analyze, critique, evaluate, and apply knowledge. The new tests also intend to encourage instruction aimed at helping students acquire and use knowledge in more complex ways.[24]
In September 2014, a report was released by the American Institutes for Research on a three-year, quasi-experimental comparison of traditional and Deeper Learning schools. The research findings demonstrated the following improved student outcomes: students attending deeper learning network schools benefited from greater opportunities to engage in deeper learning and reported higher levels of academic engagement, motivation to learn, self-efficacy, and collaboration skills; students had higher state standardized assessment scores regardless of student background; students scored higher on PISA-based Test for Schools[28] on measures of core content knowledge and complex problem-solving skills; students graduated on time at statistically significantly higher rates (9 percent); and after graduation students were more likely to attend four-year colleges and enroll in more selective institutions.[29]

See also[edit]
Design-based learning
Hands-on learning
Problem-based learning
Project-based learning
References[edit]


^ a b Martinez, Monica; McGrath, Dennis (2014). Deeper Learning: How Eight Innovative Public Schools Are Transforming Education in the Twenty-First Century. New York: The New Press. pp. 1–21. ISBN 978-1-59558-959-0..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ Mehta, Jal; Fine, Sarah (July–August 2014). "The Elusive Quest for Deeper Learning". Harvard Education Letter. 30 (4).

^ "About us". Partnership for 21st Century. Retrieved 28 September 2014.

^ a b Pellegrino, James W.; Hilton, Margaret L. (2012). Education for Life and Work: Developing Transferrable Knowledge and Skill in the 21st Century. Washington, D.C.: The National Academies Press.

^ Murnane, Richard J.; Levy, Frank (1996). Teaching the New Basic Skills: Principles for Educating Children to Thrive in a Changing Economy. New York: Free Press.

^ Cassel, R.N.; Kolstad, R. (1998). "The critical job-skills requirements for the 21st century: Living and working with people". Journal of Instructional Psychology. 25 (3): 176–180.

^ Are They Ready to Work? Employers' Perspectives on the Basic Knowledge and Applied Skills of New Entrants to the 21st Century U.S. Workforce (PDF). Washington, D.C.: Partnership for 21st Century Skills. 2006.

^ P21 Our History. Retrieved 2016-03-05

^ Critical Skills Survey (PDF). New York: American Management Association. 2012.

^ "Deeper Learning Strategic Plan Summary Education Program December 2012 Update" (PDF). Hewlett Foundation. Hewlett Foundation. December 2012. Retrieved 5 May 2019.

^ "Deeper Learning Defined" (PDF). Hewlett Foundation. Hewlett Foundation. April 2013. Retrieved 13 March 2019.

^ Mehta, Jal; Fine, Sarah (2012). "Teaching differently ... Learning deeply" (PDF). Kappan Magazine. 94 (2): 31–35.

^ http://www.p21.org/framework

^ http://www.hewlett.org/education/deeper learning

^ http://www.marzanoresearch.com/instructional strategies

^ http://www.treasury/gov.nz/publications//hattie

^ http://centerforchildhoodcreativity.org

^ Rothman, Robert (March–April 2013). "Diving into Deeper Learning: Schools gear up to promote thinking skills". Harvard Education Letter. 29 (2).

^ http://www.p21.org/exemplars

^ http://www.p21.org/our-work/p21blog

^ Bellanca, James A. (2010). Enriched Learning Projects (1st ed.). Bloomington, In: Solution Tree Press. p. 223. ISBN 978-1-934009-74-1.

^ http://www.ilc21.org/MindQuest21

^ http://www.p21.org/blogazine

^ a b c Darling-Hammond, Linda; Adamson, Frank (2013). Developing Assessments of Deeper Learning: The Costs and Benefits of Using Tests that Help Students Learn (PDF). Stanford, CA: Stanford Center for Opportunity Policy in Education. p. i.

^ "About". Smarter Balanced Assessment Consortium (SBAC). Retrieved 28 September 2014.

^ "Depth of Knowledge scale" (PDF). Archived from the original (PDF) on 12 June 2014.

^ "Innovation Lab Network". Council of Chief State School Officers. Retrieved 28 September 2014.

^ "PISA-based Test for Schools". Organisation for Economic Co-operation and Development (OECD). Retrieved 13 March 2019.

^ "Evidence of Deeper Learning Outcomes". American Institutes for Research. 24 September 2014. Retrieved 28 September 2014.


External links[edit]
Need a Job? Invent It. Friedman, Thomas L. New York Times, 3/31/2013
'The Banality of Deeper Learning', Loveless, Tom. The Brown Center Chalkboard Blog, Brookings, 5/29/13
8-Part Blog Series on Deeper Learning, Edutopia, George Lucas Education Foundation
Spotlight on Deeper Learning, Education Week
Teachers Embrace "Deep Learning," Translating Lessons into Practical Skills, PBS NewsHour
Kentucky School Aims for "Deeper Learning", PBS NewsHour
Can "Deeper Learning" Close the Achievement Gap? PBS NewsHour
OpEd: The Quest for Deeper Learning, Chow, Barbara. Education Week



The following table compares notable software frameworks, libraries and computer programs for deep learning.


Contents

1 Deep-learning software by name
2 Related software
3 See also
4 References


Deep-learning software by name[edit]


Software

Creator

Initial Release

Software license[a]

Open source

Platform

Written in

Interface

OpenMP support

OpenCL support

CUDA support

Automatic differentiation[1]

Has pretrained models

Recurrent nets

Convolutional nets

RBM/DBNs

Parallel execution (multi node)

Actively Developed


BigDL

Jason Dai (Intel)

2016

Apache 2.0

Yes

Apache Spark

Scala

Scala, Python





No



Yes

Yes

Yes








Caffe

Berkeley Vision and Learning Center

2013

BSD

Yes

Linux, macOS, Windows[2]

C++

Python, MATLAB, C++

Yes

Under development[3]

Yes

Yes

Yes[4]

Yes

Yes

No

?




Chainer

Preferred Networks

2015

BSD

Yes

Linux, macOS

Python

Python

No

No

Yes

Yes

Yes

Yes

Yes

No

Yes

Yes


Deeplearning4j

Skymind engineering team; Deeplearning4j community; originally Adam Gibson

2014

Apache 2.0

Yes

Linux, macOS, Windows, Android (Cross-platform)

C++, Java

Java, Scala, Clojure, Python (Keras), Kotlin

Yes

No[5]

Yes[6][7]

Computational Graph

Yes[8]

Yes

Yes

Yes

Yes[9]




Dlib

Davis King

2002

Boost Software License

Yes

Cross-Platform

C++

C++

Yes

No

Yes

Yes

Yes

No

Yes

Yes

Yes




Intel Data Analytics Acceleration Library

Intel

2015

Apache License 2.0

Yes

Linux, macOS, Windows on Intel CPU[10]

C++, Python, Java

C++, Python, Java[10]

Yes

No

No

Yes

No



Yes



Yes




Intel Math Kernel Library

Intel



Proprietary

No

Linux, macOS, Windows on Intel CPU[11]



C[12]

Yes[13]

No

No

Yes

No

Yes[14]

Yes[14]



No




Keras

François Chollet

2015

MIT license

Yes

Linux, macOS, Windows

Python

Python, R

Only if using Theano as backend

Can use Theano, Tensorflow or PlaidML as backends

Yes

Yes

Yes[15]

Yes

Yes

No[16]

Yes[17]

Yes


MATLAB + Deep Learning Toolbox

MathWorks



Proprietary

No

Linux, macOS, Windows

C, C++, Java, MATLAB

MATLAB

No

No

Train with Parallel Computing Toolbox and generate CUDA code with GPU Coder[18]

No

Yes[19][20]

Yes[19]

Yes[19]

No

With Parallel Computing Toolbox[21]

Yes


Microsoft Cognitive Toolkit (CNTK)

Microsoft Research

2016

MIT license[22]

Yes

Windows, Linux[23] (macOS via Docker on roadmap)

C++

Python (Keras), C++,  Command line,[24] BrainScript[25] (.NET on roadmap[26])

Yes[27]

No

Yes

Yes

Yes[28]

Yes[29]

Yes[29]

No[30]

Yes[31]

No[32]


Apache MXNet

Apache Software Foundation

2015

Apache 2.0

Yes

Linux, macOS, Windows,[33][34] AWS, Android,[35] iOS, JavaScript[36]

Small C++ core library

C++, Python, Julia, Matlab, JavaScript, Go, R, Scala, Perl

Yes

On roadmap[37]

Yes

Yes[38]

Yes[39]

Yes

Yes

Yes

Yes[40]

Yes


Neural Designer

Artelnics



Proprietary

No

Linux, macOS, Windows

C++

Graphical user interface

Yes

No

No

?

?

No

No

No

?




OpenNN

Artelnics

2003

GNU LGPL

Yes

Cross-platform

C++

C++

Yes

No

Yes

?

?

No

No

No

?




PyTorch

Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan (Facebook)

2016

BSD

Yes

Linux, macOS, Windows

Python, C, C++, CUDA

Python, C++

Yes

Via separately maintained package[41][42][43]

Yes

Yes

Yes

Yes

Yes



Yes

Yes


Apache SINGA

Apache Incubator

2015

Apache 2.0

Yes

Linux, macOS, Windows

C++

Python, C++, Java

No

Supported in V1.0

Yes

?

Yes

Yes

Yes

Yes

Yes




TensorFlow

Google Brain

2015

Apache 2.0

Yes

Linux, macOS, Windows,[44] Android

C++, Python, CUDA

Python (Keras), C/C++, Java, Go, JavaScript, R,[45] Julia, Swift

No

On roadmap[46] but already with SYCL[47] support

Yes

Yes[48]

Yes[49]

Yes

Yes

Yes

Yes

Yes


Theano

Université de Montréal

2007

BSD

Yes

Cross-platform

Python

Python (Keras)

Yes

Under development[50]

Yes

Yes[51][52]

Through Lasagne's model zoo[53]

Yes

Yes

Yes

Yes[54]

No


Torch

Ronan Collobert, Koray Kavukcuoglu, Clement Farabet

2002

BSD

Yes

Linux, macOS, Windows,[55] Android,[56] iOS

C, Lua

Lua, LuaJIT,[57] C, utility library for C++/OpenCL[58]

Yes

Third party implementations[59][60]

Yes[61][62]

Through Twitter's Autograd[63]

Yes[64]

Yes

Yes

Yes

Yes[65]

No


Wolfram Mathematica

Wolfram Research

1988

Proprietary

No

Windows, macOS, Linux, Cloud computing

C++, Wolfram Language, CUDA

Wolfram Language

Yes

No

Yes

Yes

Yes[66]

Yes

Yes

Yes

Under Development

Yes



^ Licenses here are a summary, and are not taken to be complete statements of the licenses. Some libraries may use other libraries internally under different licenses


Related software[edit]
Neural Engineering Object (NENGO) – A graphical and scripting software for simulating large-scale neural systems
Numenta Platform for Intelligent Computing – Numenta's open source implementation of their hierarchical temporal memory model
See also[edit]
Comparison of numerical-analysis software
Comparison of statistical packages
List of datasets for machine-learning research
List of numerical-analysis software
References[edit]


^ Atilim Gunes Baydin; Barak A. Pearlmutter; Alexey Andreyevich Radul; Jeffrey Mark Siskind (20 February 2015). "Automatic differentiation in machine learning: a survey". arXiv:1502.05767 [cs.LG]..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ "Microsoft/caffe". GitHub.

^ "OpenCL Caffe".

^ "Caffe Model Zoo".

^ "Support for Open CL · Issue #27 · deeplearning4j/nd4j". GitHub.

^ "N-Dimensional Scientific Computing for Java".

^ "Comparing Top Deep Learning Frameworks". Deeplearning4j.

^ Chris Nicholson; Adam Gibson. "Deeplearning4j Models".

^ Deeplearning4j. "Deeplearning4j on Spark". Deeplearning4j.

^ a b Intel® Data Analytics Acceleration Library (Intel® DAAL) | Intel® Software

^ Intel® Math Kernel Library (Intel® MKL) | Intel® Software

^ Deep Neural Network Functions

^ Using Intel® MKL with Threaded Applications | Intel® Software

^ a b Intel® Xeon Phi™ Delivers Competitive Performance For Deep Learning—And Getting Better Fast | Intel® Software

^ https://keras.io/applications/

^ https://github.com/keras-team/keras/issues/461

^ Does Keras support using multiple GPUs? · Issue #2436 · fchollet/keras

^ "GPU Coder - MATLAB & Simulink". MathWorks. Retrieved 13 November 2017.

^ a b c "Neural Network Toolbox - MATLAB". MathWorks. Retrieved 13 November 2017.

^ "Deep Learning Models - MATLAB & Simulink". MathWorks. Retrieved 13 November 2017.

^ "Parallel Computing Toolbox - MATLAB". MathWorks. Retrieved 13 November 2017.

^ "CNTK/LICENSE.md at master · Microsoft/CNTK · GitHub". GitHub.

^ "Setup CNTK on your machine". GitHub.

^ "CNTK usage overview". GitHub.

^ "BrainScript Network Builder". GitHub.

^ ".NET Support · Issue #960 · Microsoft/CNTK". GitHub.

^ "How to train a model using multiple machines? · Issue #59 · Microsoft/CNTK". GitHub.

^ https://github.com/Microsoft/CNTK/issues/140#issuecomment-186466820

^ a b "CNTK - Computational Network Toolkit". Microsoft Corporation.

^ url=https://github.com/Microsoft/CNTK/issues/534

^ "Multiple GPUs and machines". Microsoft Corporation.

^ "Disclaimer". CNTK TEAM.

^ "Releases · dmlc/mxnet". Github.

^ "Installation Guide — mxnet documentation". Readthdocs.

^ "MXNet Smart Device". ReadTheDocs.

^ "MXNet.js". Github.

^ "Support for other Device Types, OpenCL AMD GPU · Issue #621 · dmlc/mxnet". GitHub.

^ https://mxnet.readthedocs.io/

^ "Model Gallery". GitHub.

^ "Run MXNet on Multiple CPU/GPUs with Data Parallel". GitHub.

^ https://github.com/hughperkins/pytorch-coriander

^ https://github.com/pytorch/pytorch/issues/488

^ https://github.com/pytorch/pytorch/issues/488#issuecomment-273626736

^ https://developers.googleblog.com/2016/11/tensorflow-0-12-adds-support-for-windows.html

^ interface), JJ Allaire (R; RStudio; Eddelbuettel, Dirk; Golding, Nick; Tang, Yuan; Tutorials), Google Inc (Examples and (2017-05-26), tensorflow: R Interface to TensorFlow, retrieved 2017-06-14

^ "tensorflow/roadmap.md at master · tensorflow/tensorflow · GitHub". GitHub. January 23, 2017. Retrieved May 21, 2017.

^ "OpenCL support · Issue #22 · tensorflow/tensorflow". GitHub.

^ https://www.tensorflow.org/

^ https://github.com/tensorflow/models

^ "Using the GPU — Theano 0.8.2 documentation".

^ http://deeplearning.net/software/theano/library/gradient.html

^ https://groups.google.com/d/msg/theano-users/mln5g2IuBSU/gespG36Lf_QJ

^ "Recipes/modelzoo at master · Lasagne/Recipes · GitHub". GitHub.

^ Using multiple GPUs — Theano 0.8.2 documentation

^ https://github.com/torch/torch7/wiki/Windows

^ "GitHub - soumith/torch-android: Torch-7 for Android". GitHub.

^ "Torch7: A Matlab-like Environment for Machine Learning" (PDF).

^ "GitHub - jonathantompson/jtorch: An OpenCL Torch Utility Library". GitHub.

^ "Cheatsheet". GitHub.

^ "cltorch". GitHub.

^ "Torch CUDA backend". GitHub.

^ "Torch CUDA backend for nn". GitHub.

^ https://github.com/twitter/torch-autograd

^ "ModelZoo". GitHub.

^ https://github.com/torch/torch7/wiki/Cheatsheet#distributed-computing--parallel-processing

^ http://resources.wolframcloud.com/NeuralNetRepository





Deep reinforcement learning (DRL) uses deep learning and reinforcement learning principles in order to create efficient algorithms that can be applied on areas like robotics, video games, finance, healthcare.[1] Implementing deep learning architecture (deep neural networks or etc) with reinforcement learning algorithm (Q-learning, actor critic or etc), a powerful model (DRL) can be created that is capable to scale to problems that were previously unsolvable.[2]
That is because DRL usually uses raw sensor or image signals as input as can be seen in DQN for ATARI games[3], and can receive the benefit of end-to-end reinforcement learning as well as that of convolutional neural network.

References[edit]


^ Francois-Lavet, Vincent; Henderson, Peter; Islam, Riashat; Bellemare, Marc G.; Pineau, Joelle (2018). "An Introduction to Deep Reinforcement Learning". Foundations and Trends in Machine Learning. 11 (3–4): 219–354. doi:10.1561/2200000071. ISSN 1935-8237..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ Arulkumaran, K.; Deisenroth, M. P.; Brundage, M.; Bharath, A. A. (November 2017). "Deep Reinforcement Learning: A Brief Survey". IEEE Signal Processing Magazine. 34 (6): 26–38. doi:10.1109/MSP.2017.2743240. ISSN 1053-5888.

^ Mnih, Volodymyr;  et al. (December 2013). Playing Atari with Deep Reinforcement Learning (PDF). NIPS Deep Learning Workshop 2013.





Machine learning anddata mining
Problems
Classification
Clustering
Regression
Anomaly detection
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction


Supervised learning.mw-parser-output .nobold{font-weight:normal}(classification • regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)


Clustering
BIRCH
CURE
Hierarchical
k-means
Expectation–maximization (EM)
DBSCAN
OPTICS
Mean-shift


Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
t-SNE


Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov


Anomaly detection
k-NN
Local outlier factor


Artificial neural networks
Autoencoder
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net


Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)


Theory
Bias–variance dilemma
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory


Machine-learning venues
NIPS
ICML
ML
JMLR
ArXiv:cs.LG


Glossary of artificial intelligence
Glossary of artificial intelligence


Related articles
List of datasets for machine-learning research
Outline of machine learning


 Machine learning portalvte
Q-learning is a model-free reinforcement learning algorithm. The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. It does not require a model (hence the connotation "model-free") of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations.
For any finite Markov decision process (FMDP), Q-learning finds a policy that is optimal in the sense that it maximizes the expected value of the total reward over any and  all successive steps, starting from the current state.[1] Q-learning can identify an optimal action-selection policy for any given FMDP, given infinite exploration time and a partly-random policy.[1] "Q" names the function that returns the reward used to provide the reinforcement and can be said to stand for the "quality" of an action taken in a given state.[2]

Contents

1 Reinforcement learning
2 Algorithm
3 Influence of variables

3.1 Learning Rate
3.2 Discount factor
3.3 Initial conditions (Q0)


4 Implementation

4.1 Function approximation
4.2 Quantization


5 History
6 Variants

6.1 Deep Q-learning
6.2 Double Q-learning
6.3 Others


7 See also
8 References
9 External links


Reinforcement learning[edit]
Reinforcement learning involves an agent, a set of states 



S


{\displaystyle S}

, and a set 



A


{\displaystyle A}

 of actions per state. By performing an action 



a
∈
A


{\displaystyle a\in A}

, the agent transitions from state to state. Executing an action in a specific state provides the agent with a reward (a numerical score).  
The goal of the agent is to maximize its total (future) reward. It does this by adding the maximum reward attainable from future states to the reward for achieving its current state, effectively influencing the current action by the potential future reward. This potential reward is a weighted sum of the expected values of the rewards of all future steps starting from the current state. 
As an example, consider the process of boarding a train, in which the reward is measured by the negative of the total time spent boarding (alternatively, the cost of boarding the train is equal to the boarding time).  One strategy is to enter the train door as soon as they open, minimizing the initial wait time for yourself. If the train is crowded, however, then you will have a slow entry after the initial action of entering the door as people are fighting you to depart the train as you attempt to board.  The total boarding time, or cost, is then:

0 seconds wait time + 15 seconds fight time
On the next day, by random chance (exploration), you decide to wait and let other people depart first.  This initially results in a longer wait time.  However, time fighting other passengers is less. Overall, this path has a higher reward than that of the previous day, since the total boarding time is now: 

5 second wait time + 0 second fight time.
Through exploration, despite the initial (patient) action resulting in a larger cost (or negative reward) than in the forceful strategy, the overall cost is lower, thus revealing a more rewarding strategy.

Algorithm[edit]
 Q-Learning table of states by actions that is initialized to zero, then each cell is updated through training.
The weight for a step from a state 



Δ
t


{\displaystyle \Delta t}

 steps into the future is calculated as 




γ

Δ
t




{\displaystyle \gamma ^{\Delta t}}

. 



γ


{\displaystyle \gamma }

 (the discount factor) is a number between 0 and 1 (



0
≤
γ
≤
1


{\displaystyle 0\leq \gamma \leq 1}

) and has the effect of valuing rewards received earlier higher than those received later (reflecting the value of a "good start"). 



γ


{\displaystyle \gamma }

 may also be interpreted as the probability to succeed (or survive) at every step 



Δ
t


{\displaystyle \Delta t}

.
The algorithm, therefore, has a function that calculates the quality of a state-action combination:





Q
:
S
×
A
→

R



{\displaystyle Q:S\times A\to \mathbb {R} }

 .
Before learning begins, 



Q


{\displaystyle Q}

 is initialized to a possibly arbitrary fixed value (chosen by the programmer). Then, at each time 



t


{\displaystyle t}

 the agent selects an action 




a

t




{\displaystyle a_{t}}

, observes a reward 




r

t




{\displaystyle r_{t}}

, enters a new state 




s

t
+
1




{\displaystyle s_{t+1}}

 (that may depend on both the previous state 




s

t




{\displaystyle s_{t}}

 and the selected action), and 



Q


{\displaystyle Q}

 is updated. The core of the algorithm is a simple value iteration update, using the weighted average of the old value and the new information:






Q

n
e
w


(

s

t


,

a

t


)
←
(
1
−
α
)
⋅




Q
(

s

t


,

a

t


)

⏟



old value


+



α
⏟



learning rate


⋅






(






r

t


⏟



reward


+



γ
⏟



discount factor


⋅





max

a


Q
(

s

t
+
1


,
a
)

⏟



estimate of optimal future value




)



⏞



learned value




{\displaystyle Q^{new}(s_{t},a_{t})\leftarrow (1-\alpha )\cdot \underbrace {Q(s_{t},a_{t})} _{\text{old value}}+\underbrace {\alpha } _{\text{learning rate}}\cdot \overbrace {{\bigg (}\underbrace {r_{t}} _{\text{reward}}+\underbrace {\gamma } _{\text{discount factor}}\cdot \underbrace {\max _{a}Q(s_{t+1},a)} _{\text{estimate of optimal future value}}{\bigg )}} ^{\text{learned value}}}


where 




r

t




{\displaystyle r_{t}}

 is the reward received when moving from the state 




s

t




{\displaystyle s_{t}}

 to the state 




s

t
+
1




{\displaystyle s_{t+1}}

, and 



α


{\displaystyle \alpha }

 is the learning rate (



0
<
α
≤
1


{\displaystyle 0<\alpha \leq 1}

).
An episode of the algorithm ends when state 




s

t
+
1




{\displaystyle s_{t+1}}

 is a final or terminal state. However, Q-learning can also learn in non-episodic tasks.[citation needed] If the discount factor is lower than 1, the action values are finite even if the problem can contain infinite loops.
For all final states 




s

f




{\displaystyle s_{f}}

, 



Q
(

s

f


,
a
)


{\displaystyle Q(s_{f},a)}

 is never updated, but is set to the reward value 



r


{\displaystyle r}

 observed for state 




s

f




{\displaystyle s_{f}}

. In most cases, 



Q
(

s

f


,
a
)


{\displaystyle Q(s_{f},a)}

 can be taken to equal zero.

Influence of variables[edit]
Learning Rate[edit]
The learning rate or step size determines to what extent newly acquired information overrides old information. A factor of 0 makes the agent learn nothing (exclusively exploiting prior knowledge), while a factor of 1 makes the agent consider only the most recent information (ignoring prior knowledge to explore possibilities). In fully deterministic environments, a learning rate of 




α

t


=
1


{\displaystyle \alpha _{t}=1}

 is optimal. When the problem is stochastic, the algorithm converges under some technical conditions on the learning rate that require it to decrease to zero. In practice, often a constant learning rate is used, such as 




α

t


=
0.1


{\displaystyle \alpha _{t}=0.1}

 for all 



t


{\displaystyle t}

.[3]

Discount factor[edit]
The discount factor 



γ


{\displaystyle \gamma }

 determines the importance of future rewards. A factor of 0 will make the agent "myopic" (or short-sighted) by only considering current rewards, i.e. 




r

t




{\displaystyle r_{t}}

 (in the update rule above), while a factor approaching 1 will make it strive for a long-term high reward. If the discount factor meets or exceeds 1, the action values may diverge. For 



γ
=
1


{\displaystyle \gamma =1}

, without a terminal state, or if the agent never reaches one, all environment histories become infinitely long, and utilities with additive, undiscounted rewards generally become infinite.[4] Even with a discount factor only slightly lower than 1, Q-function learning leads to propagation of errors and instabilities when the value function is approximated with an artificial neural network.[5] In that case, starting with a lower discount factor and increasing it towards its final value accelerates learning.[6]

Initial conditions (Q0)[edit]
Since Q-learning is an iterative algorithm, it implicitly assumes an initial condition before the first update occurs. High initial values, also known as "optimistic initial conditions",[7] can encourage exploration: no matter what action is selected, the update rule will cause it to have lower values than the other alternative, thus increasing their choice probability. The first reward 



r


{\displaystyle r}

 can be used to reset the initial conditions.[8] According to this idea, the first time an action is taken the reward is used to set the value of 



Q


{\displaystyle Q}

. This allows immediate learning in case of fixed deterministic rewards. A model that incorporates reset of initial conditions (RIC) is expected to predict participants' behavior better than a model that assumes any arbitrary initial condition (AIC).[8] RIC seems to be consistent with human behaviour in repeated binary choice experiments.[8]

Implementation[edit]
Q-learning at its simplest stores data in tables. This approach falters with increasing numbers of states/actions.

Function approximation[edit]
Q-learning can be combined with function approximation.[9] This makes it possible to apply the algorithm to larger problems, even when the state space is continuous. 
One solution is to use an (adapted) artificial neural network as a function approximator.[10] Function approximation may speed up learning in finite problems, due to the fact that the algorithm can generalize earlier experiences to previously unseen states.

Quantization[edit]
Another technique to decrease the state/action space quantizes possible values. Consider the example of learning to balance a stick on a finger. To describe a state at a certain point in time involves the position of the finger in space, its velocity, the angle of the stick and the angular velocity of the stick. This yields a four-element vector that describes one state, i.e. a snapshot of one state encoded into four values. The problem is that infinitely many possible states are present. To shrink the possible space of valid actions multiple values can be assigned to a bucket. The exact distance of the finger from its starting position (-Infinity to Infinity) is not known, but rather whether it is far away or not (Near, Far).

History[edit]
Q-learning was introduced by Watkins[11] in 1989. A convergence proof was presented by Watkins and Dayan[12] in 1992. A more detailed mathematical proof was given by Tsitsiklis[13] in 1994, and by Bertsekas and Tsitsiklis in their 1996 Neuro-Dynamic Programming book.[14]
Watkins was addressing “Learning from delayed rewards”, the title of his PhD Thesis. Eight years earlier in 1981 the same problem, under the name of “Delayed reinforcement learning”, was solved by Bozinovski's Crossbar Adaptive Array (CAA).[15][16] The memory matrix W =||w(a,s)|| was the same as the eight years later Q-table of Q-learning. The architecture introduced the term “state evaluation” in reinforcement learning. The crossbar learning algorithm, written in mathematical pseudocode in the paper, in each iteration performs the following computation:

In state s perform action a;
Receive consequence state s’;
Compute state evaluation v(s’);
Update crossbar value w’(a,s) = w(a,s) + v(s’).
The term “secondary reinforcement” is borrowed from animal learning theory, to model state values via backpropagation: the state value v(s’) of the consequence situation is backpropagated to the previously encountered situations. CAA computes state values vertically and actions horizontally (the "crossbar"). Demonstration graphs showing delayed reinforcement learning contained states (desirable, undesirable, and neutral states), which were computed by the state evaluation function. This learning system was a forerunner of the Q-learning algorithm.[17]
In 2014 Google DeepMind patented[18] an application of Q-learning to deep learning, titled "deep reinforcement learning" or "deep Q-learning" that can play Atari 2600 games at expert human levels.

Variants[edit]
Deep Q-learning[edit]
The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q. This instability comes from the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and the data distribution, and the correlations between Q and the target values. 
The technique used experience replay, a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed.[2] This removes correlations in the observation sequence and smooths changes in the data distribution. Iterative update adjusts Q towards target values that are only periodically updated, further reducing correlations with the target.[19]

Double Q-learning[edit]
Because the future maximum approximated action value in Q-learning is evaluated using the same Q function as in current action selection policy, in noisy environments Q-learning can sometimes overestimate the action values, slowing the learning. A variant called Double Q-learning was proposed to correct this. Double Q-learning[20] is an off-policy reinforcement learning algorithm, where a different policy is used for value evaluation than what is used to select the next action.
In practice, two separate value functions are trained in a mutually symmetric fashion using separate experiences, 




Q

A




{\displaystyle Q^{A}}

 and 




Q

B




{\displaystyle Q^{B}}

. The double Q-learning update step is then as follows:






Q

t
+
1


A


(

s

t


,

a

t


)
=

Q

t


A


(

s

t


,

a

t


)
+

α

t


(

s

t


,

a

t


)

(


r

t


+
γ

Q

t


B



(


s

t
+
1


,



a
r
g
 
m
a
x



a


⁡

Q

t


A


(

s

t
+
1


,
a
)

)

−

Q

t


A


(

s

t


,

a

t


)

)



{\displaystyle Q_{t+1}^{A}(s_{t},a_{t})=Q_{t}^{A}(s_{t},a_{t})+\alpha _{t}(s_{t},a_{t})\left(r_{t}+\gamma Q_{t}^{B}\left(s_{t+1},\mathop {\operatorname {arg~max} } _{a}Q_{t}^{A}(s_{t+1},a)\right)-Q_{t}^{A}(s_{t},a_{t})\right)}

, and





Q

t
+
1


B


(

s

t


,

a

t


)
=

Q

t


B


(

s

t


,

a

t


)
+

α

t


(

s

t


,

a

t


)

(


r

t


+
γ

Q

t


A



(


s

t
+
1


,



a
r
g
 
m
a
x



a


⁡

Q

t


B


(

s

t
+
1


,
a
)

)

−

Q

t


B


(

s

t


,

a

t


)

)

.


{\displaystyle Q_{t+1}^{B}(s_{t},a_{t})=Q_{t}^{B}(s_{t},a_{t})+\alpha _{t}(s_{t},a_{t})\left(r_{t}+\gamma Q_{t}^{A}\left(s_{t+1},\mathop {\operatorname {arg~max} } _{a}Q_{t}^{B}(s_{t+1},a)\right)-Q_{t}^{B}(s_{t},a_{t})\right).}


Now the estimated value of the discounted future is evaluated using a different policy, which solves the overestimation issue.
This algorithm was later combined with deep learning, as in the DQN algorithm, resulting in Double DQN, which outperforms the original DQN algorithm.[21]

Others[edit]
Delayed Q-learning is an alternative implementation of the online Q-learning algorithm, with probably approximately correct (PAC) learning.[22]
Greedy GQ is a variant of Q-learning to use in combination with (linear) function approximation.[23] The advantage of Greedy GQ is that convergence is guaranteed even when function approximation is used to estimate the action values.

See also[edit]
Reinforcement learning
Temporal difference learning
SARSA
Iterated prisoner's dilemma
Game theory
References[edit]


^ a b Melo, Francisco S. "Convergence of Q-learning: a simple proof" (PDF)..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ a b Matiisen, Tambet (December 19, 2015). "Demystifying Deep Reinforcement Learning". neuro.cs.ut.ee. Computational Neuroscience Lab. Retrieved 2018-04-06.

^ Sutton, Richard; Barto, Andrew (1998). Reinforcement Learning: An Introduction. MIT Press.

^ Russell, Stuart J.; Norvig, Peter (2010). Artificial Intelligence: A Modern Approach (Third ed.). Prentice Hall. p. 649. ISBN 978-0136042594.

^ Baird, Leemon (1995). "Residual algorithms: Reinforcement learning with function approximation" (PDF). ICML: 30–37.

^ François-Lavet, Vincent; Fonteneau, Raphael; Ernst, Damien (2015-12-07). "How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies". arXiv:1512.02011 [cs.LG].

^ Sutton, Richard S.; Barto, Andrew G. "2.7 Optimistic Initial Values". Reinforcement Learning: An Introduction. Archived from the original on 2013-09-08. Retrieved 2013-07-18.

^ a b c Shteingart, Hanan; Neiman, Tal; Loewenstein, Yonatan (May 2013). "The role of first impression in operant learning" (PDF). Journal of Experimental Psychology: General. 142 (2): 476–488. doi:10.1037/a0029550. ISSN 1939-2222. PMID 22924882.

^ Hasselt, Hado van (5 March 2012). "Reinforcement Learning in Continuous State and Action Spaces".  In Wiering, Marco; Otterlo, Martijn van (eds.). Reinforcement Learning: State-of-the-Art. Springer Science & Business Media. pp. 207–251. ISBN 978-3-642-27645-3.

^ Tesauro, Gerald (March 1995). "Temporal Difference Learning and TD-Gammon". Communications of the ACM. 38 (3): 58–68. doi:10.1145/203330.203343. Retrieved 2010-02-08.

^ Watkins, C.J.C.H. (1989), Learning from Delayed Rewards (PDF) (Ph.D. thesis), Cambridge University

^ Watkins and Dayan, C.J.C.H., (1992), 'Q-learning.Machine Learning'

^  Tsitsiklis, J., (1994), 'Asynchronous Stochastic Approximation and Q-learning. Machine Learning'

^ Bertsekas and Tsitsiklis, (1996), 'Neuro-Dynamic Programming. Athena Scientific'

^ Bozinovski, S. (15 July 1999). "Crossbar Adaptive Array: The first connectionist network that solved the delayed reinforcement learning problem".  In Dobnikar, Andrej; Steele, Nigel C.; Pearson, David W.; Albrecht, Rudolf F. (eds.). Artificial Neural Nets and Genetic Algorithms: Proceedings of the International Conference in Portorož, Slovenia, 1999. Springer Science & Business Media. pp. 320–325. ISBN 978-3-211-83364-3.

^ Bozinovski, S. (1982). "A self learning system using secondary reinforcement".  In Trappl, Robert (ed.). Cybernetics and Systems Research: Proceedings of the Sixth European Meeting on Cybernetics and Systems Research. North Holland. pp. 397–402. ISBN 978-0-444-86488-8.

^ Barto, A. (24 February 1997). "Reinforcement learning".  In Omidvar, Omid; Elliott, David L. (eds.). Neural Systems for Control. Elsevier. ISBN 978-0-08-053739-9.

^ "Methods and Apparatus for Reinforcement Learning, US Patent #20150100530A1" (PDF). US Patent Office. 9 April 2015. Retrieved 28 July 2018.

^ Mnih, Volodymyr; Kavukcuoglu, Koray; Silver, David; Rusu, Andrei A.; Veness, Joel; Bellemare, Marc G.; Graves, Alex; Riedmiller, Martin; Fidjeland, Andreas K. (Feb 2015). "Human-level control through deep reinforcement learning". Nature. 518 (7540): 529–533. doi:10.1038/nature14236. ISSN 0028-0836. PMID 25719670.

^ van Hasselt, Hado (2011). "Double Q-learning" (PDF). Advances in Neural Information Processing Systems. 23: 2613–2622.

^ van Hasselt, Hado; Guez, Arthur; Silver, David (2015). "Deep reinforcement learning with double Q-learning" (PDF). AAAI Conference on Artificial Intelligence: 2094–2100.

^ Strehl, Alexander L.; Li, Lihong; Wiewiora, Eric; Langford, John; Littman, Michael L. (2006). "Pac model-free reinforcement learning" (PDF). Proc. 22nd ICML: 881–888.

^ Maei, Hamid; Szepesvári, Csaba; Bhatnagar, Shalabh; Sutton, Richard (2010). "Toward off-policy learning control with function approximation in Proceedings of the 27th International Conference on Machine Learning" (PDF). pp. 719–726.


External links[edit]
Watkins, C.J.C.H. (1989). Learning from Delayed Rewards. PhD thesis, Cambridge University, Cambridge, England.
Strehl, Li, Wiewiora, Langford, Littman (2006). PAC model-free reinforcement learning
Reinforcement Learning: An Introduction by Richard Sutton and Andrew S. Barto, an online textbook. See "6.5 Q-Learning: Off-Policy TD Control".
Piqle: a Generic Java Platform for Reinforcement Learning
Reinforcement Learning Maze, a demonstration of guiding an ant through a maze using Q-learning.
Q-learning work by Gerald Tesauro
JavaScript Example with Reward Driven RNN learning
A Brain Library
A Genetics Library used by the Brain



